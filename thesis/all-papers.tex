%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2006/03/15
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallextended]{svjour3}     % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}         % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{url} 
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{
Computer-assisted cantillation and chant research \\
using content-aware web visualization tools
}
\subtitle{}

%\titlerunning{Short form of title}        % if too long for running head

\author{Steven R. Ness \and
	D\'{a}niel P\'{e}ter Bir\'{o} \and
	 George Tzanetakis
}

%\author{First Author         \and
%        Second Author %etc.
%}

%\authorrunning{Short form of author list} % if too long for running head

\institute{
	Steven R. Ness \at
	Department of Computer Science\\ 
	University of Victoria, BC, Canada. \\
	\email{sness@sness.net}\\
	\and
\and
	D\'{a}niel P\'{e}ter Bir\'{o} \\
	School of Music\\
    University of Victoria, BC, Canada. \\
	\email{dpbiro@uvic.ca}\\
\and
	George Tzanetakis\\
	Department of Computer Science\\ 
	University of Victoria, BC, Canada. \\
	\email{gtzan@cs.uvic.ca}\\
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
  Chant and cantillation research is particularly interesting as it
  explores the transition from oral to written transmission of music.
  The goal of this work to create web-based computational tools that
  can assist the study of how diverse recitation traditions, having
  their origin in primarily non-notated melodies, later became
  codified. One of the authors is a musicologist and music theorist
  who has guided the system design and development by providing manual
  annotations and participating in the design process. We describe
  novel content-based visualization and analysis algorithms that can
  be used for problem-seeking exploration of audio recordings of chant
  and recitations.

\keywords{multimedia annotation\and multimedia analysis\and audio feature extraction\and semi-automatic annotation\and machine learning}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}


\section{Introduction}

In recent years there has been increasing research activity in the
areas of multimedia learning and information retrieval. Most of it has
been in traditional specific domains, such as sports video
\cite{hauptman97}, news video \cite{hauptman03} and natural
images. There is broad interest in these domains and in most cases
there are clearly defined objectives such as identifying highlights in
sports videos, explosions in news video or sunsets in natural
images. Our focus in this paper is a niche domain that shares
the challenge of effectively accessing large amounts of data but has
specific characteristics that preclude the use of
existing multimedia tools.

Although there is much related work little of it is directly relevant
to our particular application domain. There is a lot of work on
melodic similarity but most of it is based on symbolic representations
such as music notation or MIDI files \cite{Hanna2007} and therefore
not applicable in our case. Even in the cases where audio recordings
are used \cite{Duggan2008} there are no interactive visualizations of
the results making their use by expert musicologists limited. An
earlier version of the web-based system described in this paper that
did not have support for content-based similarity retrieval of pitch
contours was been described in Ness et. al \cite{sness2008}.

The goal of this project is to develop tools to study chants from
various traditions around the world including Hungarian \emph{siratok}
(laments)\cite{kodaly60}, Torah cantillation\cite{wigoder89}, tenth
century St. Gallen plainchant\cite{karp98} \cite{levy98}, and Koran
recitation\cite{nelson85}. These diverse traditions share the common
theme of having an origin in primarily non-notated melodies which then
later became codified. The evolution and spread of differences in the
oral traditions of these different chants are a current topic of
research in Ethnomusicology \cite{ce2008}.

It has proved difficult to study these changes using traditional
methods and it was decided that a combined approach, using field
recordings marked up by experts, mathematical models for analyzing the
fundamental frequency content of the audio, automatic alignment for
pitch contour similarity and a flexible graphic user interface, would
help figure out what questions needed to be asked. Unlike traditional
multimedia data where most users, including the developers of tools,
can be used as annotators, in our case any type of annotation requires
highly trained experts. In addition it is a problem seeking domain
where there are no clearly defined objectives and formulating problems
is as important as solving them. We believe that despite these
challenges it is possible to develop semi-automatic tools that can
assist researchers in formulating questions regarding how symbols are
used in chant and recitation. 

The number and variety of chants that are available to be studied is
vast, in our current work, we concentrate on four different chant
traditions.  Our collaborators have collected a large database of
recordings and are actively engaged in field research where they are
collecting more recordings.  In addition, the analytical approach we
describe in this paper could also be applied to many other chant
traditions from around the world, including chants of rapidly
disappearing indigenous cultures.  The recordings are currently mostly
tape based, however some newer recordings are directly recorded in
digital form. For the experiments described in section 3 we used 19
different recordings with a total duration of 26.8 minutes.

Web-based software has been helping connect communities of researchers
since its inception.  Recently, advances in software and in computer
power have dramatically widened its possible applications to include a
wide variety of multimedia content.  These advances have been
primarily in the business community, and the tools developed are just
starting to be used by academics. We have been working on applying
these technologies to ongoing collaborative projects that we are
involved in \cite{sness2008}. By leveraging several new technologies
including \emph{Flash}, \emph{haXe}, \emph{AJAX} and \emph{Ruby on
  Rails}, we have been able to rapidly develop web-based tools. Rapid
prototyping and iterative development have been key elements of our
collaborative strategy. Although our number of users is limited
compared to other areas of multimedia analysis and retrieval, this is
to some degree compensated by their passion and willingness to work
closely with us in developing these tools.

Classical tools to analyze chant recordings primarily consist of
listening to recordings on tape and on simple digital audio players
and annotating them by hand.  The primary method of investigation of
chant traditions involves listening to chants, and qualitatively
grouping similar chants together.  There are existing computational
tools that could be used for this purposes. For example, conceivably
ethnomusicologists could use a combination of the Praat software
\cite{boersma2001} for speech analysis and custom written Matlab
scripts to do this analysis. This would require programming expertise
which most of them do not have.

In the current work, we developed a computer based tool to help with
the study of chant traditions using a process of participatory design,
where the ethnomusicology domain expert interacts regularly and
frequently with the developers of the software tools.  This is a niche
application, designed to be used by experts in ethnomusicology, which
makes traditional human computer interaction user studies and
evaluations challenging because of the rarity of users with the
required expertise in ethnomusicology.  To compensate for this, our
primary mode of evaluation of the interface was through feedback with
the domain expert through an iterative process of design and
development.  In addition we provide experimental results that show
the effectiveness of the analysis tools used. These include a
comparison with traditional methods of pitch contour representation
(interval-based contour abstraction) used in the different but related
field of query-by-humming (QBH)\cite{ghias1995} \cite{dannenberg2007}.
More details of this process can be found in section 3.9.

In this paper, we examine of the effects of using different
quantization levels on the mean average precision recall of different
trope signs.  We then compare the results when we use a data-driven
approach of examining the most common notes that the singers use
versus notes derived from the western equal tempered scale.  This
data-driven approach is the one used in our interactive chant research
software, Cantillion.

\section{Chant research}

Our work in developing tools to assist with chant research is a
collaboration with Dr. Daniel Biro, a professor in the School of Music
at the University of Victoria. He has been collecting and studying
recordings of chant with specific focus on how music transmission
based on oral transmission and ritual was gradually changed to one
based on writing and music notation. The examples studied come from
improvised, partially notated, and gesture-based \cite{krumhansl90} notational chant
traditions: Hungarian siratok (laments)  \footnote{Archived Examples from Hungarian Academy of Science 
(1968-1973)}, Torah cantillation  \cite{zimmerman00} \footnote{Archived Examples from Hungary and Morocco from the Feher Music Center at the Bet Hatfatsut, Tel Aviv, Israel}, tenth
century St. Gallen plainchant \cite{treitler82} \footnote{Godehard Joppich and Singphoniker: Gregorian Chant from St. Gallen
(Gorgmarienh√ºtte: CPO 999267-2, 1994)}, and Koran recitation
\footnote{Examples from Indonesia and Egypt: in Approaching the Koran
  (Ashland: White Cloud, 1999)}. This work falls under the more
general area of Computational Ethnomusicology \cite{ce2008}. 


Although Dr. Biro has been studying these recordings for some time and
has considerable computer expertise for a professor in music, the
design and development of our tools has been challenging. This is
partly due to difficulties in communication and terminology as well as
the fact that the work is exploratory in nature and there are no
easily defined objectives. The tool has been developed through
extensive interactions with Dr. Biro with frequent frustration on both
sides. At the same time, a wonderful thing about expert users like
Dr. Biro is that they are willing to spend considerable time preparing
and annotating data as well as testing the system and user interface
which is not the case in more traditional broad application domains.

We used a number of different datasets in the current work, all of
which were recorded onto tape and then subsequently digitized at a
sample rate of 44100Hz and with 16 bit precision.  The recordings used
in this paper included five recordings of Torah chant from two
different reciters, four recordings of Koran recitation from three
singers, two recordings of Gregorian chant from a choir, and one
recording of a Hungarian lament.

\section{Melodic Contour Analysis} 

Our tool takes in a (digitized) monophonic (audio in which only one
voice is present) or heterophonic recording
(audio in which two or more voices elaborate the same melody
simultaneously) and produces a series of successively more refined and
abstract representations of the segments it contains as well as the
corresponding melodic contours. More specifically the following
analysis stages are performed:

\begin{itemize} 
\item{Hand Labeling of Audio Segments}
\item{First Order Markov Model of Sign Sequences}
\item{F0 Estimation}
\item{F0 Pruning}
\item{Scale Derivation: Kernel Density Estimation}
\item{Quantization in Pitch}
\item{Scale-Degree Histogram}
\item{Histogram-Based Contour Abstraction}
\item{Dynamic Time Warping for Contour Similarity} 
\item{Plotting and Recombining the Segments}
\end{itemize} 


\subsection{Hand Labeling of Audio Segments}

The recordings are manually segmented and annotated by the
expert. Even though we considered the possibility of creating an
automatic segmentation tool, it was decided that the task was too
subjective and critical to automate. Each segment is annotated with a
word/symbol that is related to the corresponding text or performance
symbols (for example cantillation marks) used during the recitation.


\begin{figure}[htb]
\includegraphics[width=120mm]{transition_matrix}
\caption{
Syntagmatic analysis with a first-order Markov model of the
sequence of Torah trope signs for the text Shir Ha Shirim (``Song of
Songs'').} 
\label{fig:transitions} 
\end{figure} 

\subsection{First Order Markov Model of Sign Sequences}

In order to study the transitions between signs/symbols we calculate a
first order Markov model of the sign sequence for each recording. We
were asked to perform this type of syntagmatic analysis by
Dr. Biro. Although it is completely straightforward to perform
automatically using the annotation, it would be difficult and time
consuming to calculate manually. Figure ~\ref{fig:transitions} shows
an example transition matrix. For a given trope sign (a row) it shows
how many total times does it appear in the example (numeral after row
label), and in what fraction of those appearances is it followed by
each of the other trope signs. The darkness of each cell corresponds
to the fraction of times that the trope sign in the given row is
followed by the trope sign in the given column.  (NB: Cell shading is
relative to the total number of occurrences of the trope sign in the
row, so, e.g., the black square saying that ``darga'' always precedes
``revia'' represents 1/1, while the black square saying that ``zakef''
always precedes ``katon'' represents 9/9.) This type of analysis can
help identify the syntactic role that different signs have.

The symbols above are the cantillation symbols from Torah chant, these
symbols are well established and form a stable and describable
grammar.  Different traditions, for example Koran recitation, would
have different sets of symbols, which may or may not be of such a
proscribed vocabulary, but generally have definite regularities in
their usage.

The above Markov Model pre-processing analysis shows that there are
certain well defined rules about which signs follow other signs, which
is of general interest to ethnomusicologists.  This analysis is meant
to be a qualitative measure of the regularity of the grammar of the
chant, and can give clues to ethnomusicologists about which sign
relationships will be most profitable to investigate.


\subsection{F0 Estimation}


After the segments have been hand-labeled and identified, the
fundamental frequency (``F0'' in this case equivalent to pitch) and
signal energy (related to loudness) are calculated for each segment as
functions of time. We use the SWIPEP fundamental frequency estimator
\cite{camachophd} with all default parameters except for upper and
lower frequency bounds that are hand-tuned for each example. For
signal energy we simply take the sum of squares of signal values in
each non-overlapping 10-ms rectangular window.

The SWIPEP algorithm \cite{camachophd} uses an algorithm that is
related to autocorrelation, and using a cosine as the kernel, performs
an integral transform of the spectrum.  Unlike autocorrelation, which
uses the square of the magnitude of the spectrum, SWIPEP uses the
square root of the magnitude of the spectrum.  SWIPEP also modifies
the cosine kernel in order to avoid some of the problems associated
with autocorrelation.  These involve first zeroing the first quarter
of the first cycle of the cosine, this allows it to avoid the maximum
value at zero lag that occurs when using autocorrelation.  It then
avoids the periodicity that autocorrelation experiences when analyzing
periodic signals by multiplying the kernel by a 1/f envelope.  To
force the width of the main spectral lobes to match the width of the
positive cosine lobes, it also normalizes the cosine kernel and
applies a pitch-dependant window size.

The SWIPEP algorithm is a powerful algorithm for estimating the
fundamental frequency of audio signals, and in a comparison against
twelve other leading F0 estimation algorithms it outperformed all of
them \cite{camachophd}.  It performs especially well when compared
against the traditional autocorrelation approach in that it makes less
errors, including less octave errors, a common problem encountered
when using traditional autocorrelation.



\begin{figure}[t]
\includegraphics[width=120mm]{f0contour}
\caption{F0 Contour : The top half of the graph shows the F0 contour
  as estimated by the SWIPEP algorithm.  The x-axis shows time in
  seconds from the start of the audio file, and the y-axis shows the
  pitch of the contour in MIDI note numbers.  The bottom half of the
  graph shows the signal energy of the audio, with the x-axis
  describes time in seconds, and the y-axis shows the energy of the
  audio in decibels.  }
\label{fig:contour}
\end{figure} 

\subsection{F0 Pruning}

The next step is to identify pauses between phrases, so as to
eliminate the meaningless and wildly varying F0 estimates during these
noisy regions. We define an energy threshold, generally 40 decibels
below each recording's maximum. If the signal energy stays below this
threshold for at least 100 ms then the quiet region is treated as
silence and its F0 estimates are ignored. Figure~\ref{fig:contour}
shows an excerpt of the F0 and energy curves for an excerpt from the
Koran sura (``section'') Al-Qadr (``destiny'') recited by the renowned
Sheikh Mahmud Khalil al-Husari from Egypt.



\begin{figure}[t] 
\includegraphics[width=120mm]{scalehistogram}
\caption{Recording-specific scale derivation} 
\label{fig:scale}
\end{figure} 

\subsection{Quantization in Pitch}

Following the pitch contour extraction is pitch quantization, which is
the discretization of the continuous pitch contour into discrete notes
of a scale. Rather than externally imposing a particular set of
pitches, such as an equal-tempered chromatic (the piano keys) or
diatonic scale, we have developed a novel method for extracting a
scale from an F0 envelope that is continuous (or at least very densely
sampled) in both time and pitch. Our method is inspired by Krumhansl's
time-on-pitch histograms adding up the total amount of time spent on
each pitch \cite{krumhansl90}. We demand a pitch resolution of one
cent \footnote{One cent is 1/100 of a semitone, corresponding to a
  frequency difference of about 0.06\%}, so we cannot use a simple
histogram. Instead we use a statistical technique known as
non-parametric kernel density estimation, with a Gaussian kernel
\footnote{Thinking statistically, our scale is related to a
distribution given the relative probability of each possible
pitch. We can think of each F0 estimate (i.e each sampled value of
the F0 envelope) as a sample drawn from this unknown distribution so
our problem becomes one of estimation the unknown distribution given
the samples}. More specifically a Gaussian (with standard deviation
of 33 cents) is centered on each sample of the frequency estimate and
the Gaussians of all the samples are added to form the kernel density
estimate. The resulting curve is our density estimate; like a
histogram, it can be interpreted as the relative probability of each
pitch appearing at any given point in time. Figure ~\ref{fig:scale} shows this
method's density estimate given the F0 curve from Figure ~\ref{fig:contour}.

In quantizing the pitch, the size of the excerpt chosen can influence
both the number of peaks and the location of these peaks.  In the
current work, we have chosen to use the entire file as the dataset for
doing pitch quantization.  In the user interface presented below, we
allow the user to choose subsets of signs which can then be viewed at
different levels of quantization granularity.  This process is
necessary because of the two step nature of our analysis process,
where first the audio file is analyzed, and then this analysis is
presented to the user, who can then perform further analysis on the
audio.  We are in the process of developing a new interface that will
overcome this drawback, and will allow the user to directly interact
with the first analysis procedure.


\subsection{Scale-Degree Histogram}

We interpret each peak in the density estimate as a note of the
scale. We restrict the minimum interval between scale pitches
(currently 80 cents by default) by choosing only the higher peak when
there are two or more very close peaks. This method's free parameter
is the standard deviation of the Gaussian kernel, which provides an
adjustable level of smoothness to our density estimate; we have
obtained good results with a standard deviation of 33 cents. Note that
this method has no knowledge of octaves.


Once we have determined the scale, pitch quantization is the trivial
task of converting each F0 estimate to the nearest note of the scale.
In our opinion these derived scales are more true to the actual nature
of pitch-contour relationships within oral/aural and semi-notated
musical traditions. Instead of viewing these pitches to be deviations
of pre-existing ``normalized'' scales our method defines a more
differentiated scale from the outset. With our approach the scale
tones do not require ``normalization'' and thereby exist in an
autonomous microtonal environment defined solely on statistical
occurrence of pitch within a temporal unfolding of the given melodic
context. Once the pitch contour is quantized into the
recording-specific scale calculated using Kernel density estimation,
we can calculate how many times a particular scale degree appears
during an excerpt. The resulting data is a scale-degree histogram
which is used create simplified abstract visual contour
representations.




\begin{figure}[htb]
\centering
\includegraphics[width=60mm]{cantillion-30pashta-histogramlevels}
\caption{Melodic contours at different levels of abstraction (top:
  original, middle: quantized, bottom: simplified using 3 most
  prominent scale degrees}
\label{fig:contours_histogram} 
\end{figure} 


\subsection{Histogram-Based Contour Abstraction}

The basic idea of histogram-based contour abstraction is to only use
the most salient discrete scale degrees (the histogram bins with the
highest magnitude) as significant points to simplify the
representation of the contour. By adjusting the number of prominent
scale degrees used to represent the simplified representation the
researchers can view/listen to the melodic contour at different levels
of abstraction and detail. Figure ~\ref{fig:contours_histogram} shows
an original continuous contour, the quantized representation using the
recording-specific derived scale and the abstracted representation
using only the 3 most prominent scale degrees.



\begin{figure*}[t]
\centering
\subfigure[F0 Contour of 11 Pashta]
{
    \label{fig:sub:contour-11pashta}
    \includegraphics[width=2cm,height=2cm]{contour-11pashta.ps}
}
\hspace{1cm}
\subfigure[F0 Contour of 42 Pashta]
{
    \label{fig:sub:contour-42pashta}
    \includegraphics[width=2cm,height=2cm]{contour-42pashta.ps}
}
\hspace{1cm}
\subfigure[F0 Contour of 18 Sof Pasuq] 
{
    \label{fig:sub:contour-18sofpasuq}
    \includegraphics[width=2cm,height=2cm]{contour-18sofpasuq.ps}
}
\hspace{1cm}
\subfigure[F0 Contour of 11 Pashta Doubled]
{
    \label{fig:sub:contour-11pashta-doubled}
    \includegraphics[width=4cm,height=2cm]{contour-11pashta-doubled.ps}
}
\caption{
F0 contours of 4 different gestures from a Torah recitation recorded
in Hungary.  The first two show different versions of the pashta
gesture (11 pashta and 42 pashta) and the third shows the gesture for
sof pasuq (18 sof pasuq).  The last is a version of the first pashta
gesture (11 pashta) with each audio sample doubled, which effectively
stretches the contour by a factor of two.
}
\end{figure*}

\begin{figure*}[t]
\centering
\subfigure[DTW of 11 Pashta vs 11 Pashta]
{
    \label{fig:sub:dtw-11pashta-11pashta}
    \includegraphics[width=2cm,height=2cm]{dtw-11pashta-11pashta.ps}
}
\hspace{1cm}
\subfigure[DTW of 11 Pashta vs 42 Pashta]
{
    \label{fig:sub:dtw-11pashta-42pashta}
    \includegraphics[width=2cm,height=2cm]{dtw-11pashta-42pashta.ps}
}
\hspace{1cm}
\subfigure[DTW of 11 Pashta vs 18 Sof Pasuq] 
{
    \label{fig:sub:dtw-11pashta-18sofpasuq}
    \includegraphics[width=2cm,height=2cm]{dtw-11pashta-18sofpasuq.ps}
}
\hspace{1cm}
\subfigure[DTW of 11 Pashta vs 11 Pashta Doubled]
{
    \label{fig:sub:dtw-11pashta-11pashta-doubled}
    \includegraphics[width=4cm,height=2cm]{dtw-11pashta-11pashta-doubled.ps}
}
\caption{
Shown above are Similarity Matrices of the above four gestures
compared with the first pashta gesture.  Superimposed on the figures
is the Dynamic Time Warping curve showing the optimally matching path
between the two songs.
}
\end{figure*}

In the next section we show that these simplified abstract contour
representations result in better retrieval performance than the
original ``continuous'' pitch contours.


One of the main aspects in the studying of signs in the context of
chant and recitation is to what extent they convey gesture information
that is invariant with respect to the underlying text. To study this
question it was necessary to develop a method to compare the pitch
contours of different realizations from different parts of the audio
recording of the same sign.

To our knowledge, our use of the Histogram-Based Contour abstraction
is novel, the classical approach is that of an interval-based contour
abstraction of the pitch contour.  This interval-based method is a
commonly used method in Query-by-Humming experiments
(QBH)\cite{dannenberg2007}.  The interval-based contour abstraction
representation simplifies a contour by describing the relation of each
tone to the next in terms of how many scale degrees exist between one
note and the next.  For example, using the note sequence AED, there
would be a step of +4 between the A and E, and a step of -1 between
the E and D.  This representation can then be simplified by quantizing
it to a smaller number of step.  The simplest interval abstraction has
three levels, ``goes up'' (+1), ``goes down''(-1), and ``remains the
same'' (0).  One can then successively subdivide the upper and lower
ranges, giving 5 levels, 9 levels, 11 levels, and so on.  The result
of interval-based quantization is a string of the quantized interval
differences between one pitch value and then next.  The resulting
strings of values for each trope are then compared to one another
using the technique of Dynamic Time Warping, as described in the next
section.

\subsection{Dynamic Time Warping for Contour Similarity Calculation} 

Dynamic Time Warping (DTW) is a technique by which the similarity
between two different time sequences can be measured. It allows a
computer to find an optimal match between two sequences by performing
a non-linear warping of one sequence to the other. The technique of
dynamic programming is used for efficient implementation. An example
of DTW in Music Information Retrieval is to compare the tempo
variations between two different performances of a classical
symphony. The DTW algorithm would identify the parts of the two
symphonies that were played at the same tempo as a diagonal line, with
the line varying above and below the diagonal when the tempo was
different between the two pieces.

First the similarity matrix between the two pitch contours we are
comparing is calculated.  Based on the calculated similarity matrix
the DTW algorithm finds the optimal alignment path of the two sequences
and calculates the cost of that alignment.  When the contours are
similar the alignment cost will be small compared to when the contours
are dissimilar. The matching process is pitch shift invariant and
allows variations and tempo stretching. That way for any particular
sign (pitch contour) we can sort the sign (pitch contours) by
similarity.

\subsection{Plotting and Recombining the Segments}

To illustrate the technique we use the gestures of two separate
annotated recordings of a section of the Torah. One of these was
recorded in Morocco, and the other was recorded in Hungary. Figures
~\ref{fig:sub:contour-11pashta}, ~\ref{fig:sub:contour-42pashta},
~\ref{fig:sub:contour-18sofpasuq} and
~\ref{fig:sub:contour-11pashta-doubled} show the F0 contour of the
sections of the audio file from a Torah recording from Hungary.
Figure ~\ref{fig:sub:contour-11pashta} shows a pashta sign, Figure
~\ref{fig:sub:contour-42pashta} shows another pashta sign from
further along in the audio file.  Figure
~\ref{fig:sub:contour-18sofpasuq} shows a sof pasuq gesture and Figure
~\ref{fig:sub:contour-11pashta-doubled} shows the first pashta
gesture, but with the sample stretched by a factor of two.

The figures
\ref{fig:sub:dtw-11pashta-11pashta},\ref{fig:sub:dtw-11pashta-42pashta}
, \ref{fig:sub:dtw-11pashta-18sofpasuq} and
\ref{fig:sub:dtw-11pashta-11pashta-doubled} show Similarity Matricies
and the alignment paths computed using DTW for these four gestures
compared to the first pashta gesture. White areas are highly similar
and black areas have low similarity. In Figure
\ref{fig:sub:dtw-11pashta-11pashta} the first pashta gesture is
compared to itself.  The DTW curve is overlaid in black and is
basically a straight diagonal line from one corner to the opposite
corner, showing that the optimal path between the start and the end of
the file is a direct alignment of one file to the other.  Figure
\ref{fig:sub:dtw-11pashta-11pashta-doubled} shows a similar behavior,
except that the slope of the line is shallower.  Figure
\ref{fig:sub:dtw-11pashta-42pashta} shows the comparison of one pashta
gesture to another.  This path had a DTW cost of 23.8442.  Figure
\ref{fig:sub:dtw-11pashta-18sofpasuq} shows an alignment between the
pashta gesture and a sof pasuq gesture.  One can see that the line is
not only not diagonal, but that the line is often on dark areas which
denote high alignment cost.



\begin{table} 
\begin{center}
\begin{tabular}{|lr|lr|}
\hline
 Gesture   &  Average  &  Gesture   &   Average   \\
 (Hungary)   &  Precision   & (Morocco)   &   Precision       \\
    &   (Hungary)  &    &   (Morocco)       \\
\hline
 tipha     &    0.662  &  katon     &  0.453  \\
 pashta    &    0.647  &  mapah     &  0.347  \\
 mapah     &    0.641  &  tipha     &  0.303  \\
 katon     &    0.604  &  sofpasuq  &  0.285  \\
 etnachta  &    0.601  &  pashta    &  0.242  \\
 sofpasuq  &    0.591  &  merha     &  0.251  \\
 merha     &    0.537  &  etnachta  &  0.150  \\
 revia     &    0.372  &  zakef     &  0.125  \\
 zakef     &    0.201  &  revia     &  0.091  \\
 kadma     &    0.200  &  kadma     &  0.043  \\
\hline
\end{tabular}
\caption{Average precision for different signs}
\label{table:precisions}
\end{center}
\end{table}


Table ~\ref{table:precisions} shows the average precision for
particular signs for two recordings of the same excerpt from the Torah
- one from Hungary and one from Morocco. Each recordings contains
approximately 130 realizations of each sign with a total of 12 unique
signs. Two pitch contours are considered relevant to each other if
they are annotated by the same sign. For each ``query'' contour we
return a list of results which are the pitch contours sorted by the
alignment cost of the DTW. Average precision emphasizes returning more
relevant contours earlier. It is the average of precisions computed
after truncating the list of returned results after each of the
relevant documents in turn. Unlike traditional retrieval systems where
the mean average precision can be used to characterize the overall
system performance in our cases we are more interested in the
individual difference in precision among different signs. These
differences show which signs have well-defined gestural
characteristics and which signs are not interpreted
consistently. Ultimately the numbers are only meaningful after careful
interpretation by an expert. For example based on Table
~\ref{table:precisions} one can infer that the performer in the
Hungarian version had more consistent interpretations of the signs
than the performer in the Moroccan version.

We have also investigated the retrieval effectiveness of quantized
contour representations at different levels of abstraction using the
approach described above. In this case it makes sense to use Mean
Average Precision across queries to explore what is the best level of
abstraction for this task.

This first DTW analysis was conducted using the continuous pitch
values determined by the SWIPEP algorithm.  We then extended this
analysis by quantizing the pitch contours, calculating the pairwise
score between each contour and then calculating the mean average
precision recall.  We did this for all possible number of histogram
bins, from the maximum number of scale degrees of 13, down to only the
most popular histogram bin.  We then repeated this analysis with notes
from a western equal-tempered scale.  The total range of notes was
from the A2\# (the A\# two octaves below middle C) to C4 (middle C).
This gave a total of 16 semitones, of which we used the most common 13
scale degrees.  For all of these possible histogram bin numbers, we
converted all notes to these quantized values and did a pairwise DTW
comparison between all of them.  We then calculated the mean average
precision recall for each histogram bin quantization level.  These
results are presented in Figure \ref{fig:histogram-equal-continuous}.

From this graph and Table ~\ref{table:simplify}, we can see that the
optimal number of histogram bins is 2 when notes are quantized to our
derived scale.  The mean average precision recall at this level is
0.493.  After this, the curve quickly drops, and then remains at a
steady state level of approximately 0.41. This is significantly better
than using the ``continuous'' contour mean average precision of
0.2951.  The term continuous contour refers to when the original
contour of the song is quantized to the closest equivalent
equal-tempered, or MIDI, note.  When we quantize the notes to the
equal-tempered scale, the maximum value of 0.443 is also obtained with
2 histogram bins.  It is important to note that the value of 0.493
that is derived when the data-driven approach of using the notes that
are actual chanted is higher than the value derived from using the
equal-tempered scale, and this can be easily understood by realizing
that the singers do not tune themselves to a western scale.  This
shows the fundamental utility of our method of deriving the quantized
scale from the notes that are actually sung.

These results are shown in a more intuitive way in Figure
\ref{fig:simplify-cont}.  In this figure three ``sof pasuq'' and three
``pashta'' contours were chosen, and were quantized to the derived,
data-driven scale using the optimal value of 2 histogram bins.  One
can see that the ``sof pasuq'' contours have quite a different shape.
This visualization shows the utility of our approach.

In order to compare these results with those of the classical method
of Interval-based Contour Abstraction (IBCA), we first quantized the
continuous contour using either the data-derived scale mentioned
previously or to an equal tempered scale.  We then generated the
interval-based contour abstraction for each trope using first 3, then
5, 7, 9, 11 and 13 different interval quantization levels.  These
strings of interval differences for each trope were then compared
against each other using the same Dynamic Time Warping (DTW) technique
used above.  Average precision-recall values were then generated for
each of the interval quantization levels.  The results of this are
shown in Table ~\ref{table:simplify} and are presented graphically in
Figure \ref{fig:histogram-equal-continuous}.

From these results, one can immediately see that our proposed method
of Histogram-based Contour Abstraction (HBCA) outperforms the
traditional method of Interval-based Contour Abstraction (IBCA) by a
large margin.  In addition, one can see a small improvement in the
IBCA approach when using a scale derived from the data, as opposed to
using an equal-tempered scale.  However, it must be stated that this
is a very small difference, and may not be statistically significant.
Further investigation in this area with larger sample sizes is
required.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm,angle=270]{results-histogram-equal-continuous}
\end{center}
\caption{ Mean average precision recall when quantizing the notes
  before DTW analysis.  Shown are the results for quantizing to a song
  specific scale (Histogram derived scale) versus an equal tempered
  scale (MIDI notes) for both Histogram-based Contour Abstraction
  (HBCA) and Interval-based Contour Abstraction (IBCA) approaches.  In
  addition, the mean average precision recall in the continuous case
  is also shown.}
\label{fig:histogram-equal-continuous} 
\end{figure} 


\begin{figure}[htb]
\begin{center}
\includegraphics[width=100mm]{simplify-cont.ps}
\end{center}
\caption{Comparison of contour quantized to the two most prevalent
  scale degrees in a data-driven approach to the original continuous
  contour.  Shown are three examples of the signs ``sof pasuq'' and
  ``pashta''}
\label{fig:simplify-cont} 
\end{figure} 

\begin{table} 
\begin{center}
%\begin{tabular}{|r|r|r|} \hline
%Number &  Data   & Equal \\
%of Bins & Driven & Temperment \\  \hline
%  1  &  0.1931  &  0.26581  \\
%  2  &  0.4932  &  0.44356  \\
%  3  &  0.4479  &  0.35044  \\
%  4  &  0.4057  &  0.37572  \\
%  5  &  0.4097  &  0.39797  \\
%  6  &  0.4061  &  0.41386  \\
%  7  &  0.4026  &  0.41350  \\
%  8  &  0.3941  &  0.41791  \\
%  9  &  0.3953  &  0.41655  \\
% 10  &  0.3947  &  0.41931  \\
% 11  &  0.3948  &  0.41584  \\
% 12  &  0.3948  &  0.41594  \\
% 13  &  0.3948  &  0.41617  \\ \hline
%\end{tabular}

\begin{tabular}{|r|r|r|r|r|}
\hline
Number &  HBCA    & HBCA Equal &  IBCA    & IBCA Equal \\
of Bins & Data Driven & Temperment &  Data Driven   & Temperment \\  \hline
     1  &  0.1931  &  0.2658  &    &    \\
     2  &  0.4932  &  0.4435  &    &    \\
     3  &  0.4479  &  0.3504  & 0.1684   &  0.1663  \\
     4  &  0.4057  &  0.3757  &    &    \\
     5  &  0.4097  &  0.3979  & 0.1575   &  0.1456  \\
     6  &  0.4061  &  0.4138  &    &    \\
     7  &  0.4026  &  0.4135  & 0.1605   & 0.1490   \\
     8  &  0.3941  &  0.4179  &    &   \\
     9  &  0.3953  &  0.4165  & 0.1629   & 0.1522   \\
    10  &  0.3947  &  0.4193  &    &    \\
    11  &  0.3948  &  0.4158  &  0.1571  &  0.1503  \\
    12  &  0.3948  &  0.4159  &    &    \\
    13  &  0.3948  &  0.4161  & 0.1606   &  0.1488  \\
\hline
\end{tabular}


\caption{Table of mean average precision values when quantizing the
  notes before DTW analysis.  Shown are the calculated values for the
  Data-driven and Equal-temperment approaches using both the
  Histogram-based Contour Abstraction (HBCA) and Interval-based
  Contour Abstraction (IBCA) approaches}
\label{table:simplify}
\end{center}
\end{table}






\subsection{Cantillion interface} 


\begin{figure}[htb]
\begin{center}
\includegraphics[width=120mm]{cbmi2009-cantillion}
\end{center}
\caption{
Web-based \emph{Flash} interface to allow users to listen to audio, and to
enable interactive querying of gesture contour diagrams.}
\label{fig:cantillion} 
\end{figure} 

We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways
(\url{http://cantillation.sness.net}). Each recording is manually
segmented into the appropriate units for each chant type (such as
trope sign, neumes, semantic units, or words). The pitch contours of
these segments can be viewed at different levels of detail and
smoothness using a histogram-based method. The segments can also be
rearranged in a variety of ways both manually and automatically. The
audio analysis (pitch extraction and dynamic time warping) are
performed using the Marsyas audio processing framework
\footnote{\url{http://marsyas.sourceforge.net}} \cite{Marsyas}. 

The interface (Figure 7) has four main sections: a sound
player, a main window to display the pitch contours, a control window,
and a histogram window.  The sound player window displays a
spectrogram representation of the sound file with shuttle controls to
let the user choose the current playback position in the sound
file. The main window shows all the pitch contours for the song as
icons that can be repositioned automatically based on a variety of
sorting criteria, or alternatively can be manually positioned by the
user. The name of each segment (from the initial segmentation step)
appears above its F0 contour. The shuttle control of the main sound
player is linked to the shuttle controls in each of these icons,
allowing the user to set the current playback state either way.

When an icon in the main F0 display window is clicked, the histogram
window shows a histogram of the distribution of quantized pitches in
the selected sign. Below this histogram is a slider to choose how many
of the largest histogram bins will be used to generate a simplified
contour representation of the F0 curve. In the limiting case of
selecting all histogram bins, the reduced curve is exactly the
quantized F0 curve. At lower values, only the histogram bins with the
most items are used to draw the reduced curve, which has the effect of
reducing the impact of outliers and providing a smoother
``abstract'' contour.  Shift-clicking selects multiple signs; in this
case the histogram window includes the data from all the selected
signs. We often select all segments with the same word, trope sign, or
neume; this causes the simplified contour representation to be
calculated using the sum of all the pitches found in that particular
sign, enhancing the quality of the simplified contour representation.
Figure 7 shows a screenshot of the browsing interface.

In the current work we implemented a mode that allows the researcher
to sort the samples based on the Dynamic Time Warping cost from one
sample to the other.  The interface allows the user to select an
arbitrary gesture from the interface, and then perform a sorting of
all other gestures to it.  In the example shown in Figure
~\ref{fig:cantillion} the user has chosen a ``revia'', and has sorted
all the other gestures based on their DTW-based alignment distance
from this first revia.  One can see that the gesture closest to this
revia is another revia gesture from a different section of the audio
file.

We are currently developing an addition to the Cantillion interface to
allow us to visualize subsets of signs at different quantization
levels, and to compare these to the original continuous contour.  This
interface uses a checkbox list to allow the user to select different
types of signs, and then displays these contours in the main interface
pane.  The user can select multiple quantization levels and can
compare them for many signs at once, which allows the user to quickly
perform an analysis similar to the full pair-wise comparison described
above, but interactively, and therefore using the knowledge and skills
of ethnomusicologists.


\section{Summary and discussion}

The identity of chant formulae in oral/aural chant traditions is to a
large extent determined by gesture/contour rather than by discrete
pitches. Computational approaches assist with the analysis of these
gestures/contours and enables the juxtaposition of multiple views at
different levels of detail in a variety of analytical (paradigmatic
and syntagmatic) contexts.  The possibilities for such complex
analysis methods would be difficult if not impossible without such
computer-assisted analysis. Employing these tools we hope to better
understand the role of and interchange between melodic formulae in
oral/aural and written chant cultures. While our present analysis
investigates melodic formulae primarily in terms of their gestural
content and semantic functionality, we hope that these methods might
allow scholars to reach a better understanding of the historical
development of melodic formulae within various chant traditions.

By combining the expert knowledge of our scientific collaborators with
new multimedia web-based tools in an agile development strategy, we
have been able to ask new questions that had previously been out of
reach. Chant research is a challenging domain where problem seeking is
important. Participatory design together with content-aware
visualizations and analysis tools can help researchers interact with
large collections of annotated audio recordings of chant in
interesting new ways. The integration of all the different components
in a single web-based interface is critical for an effective
system. Given the subjective interpretive nature of musicological
research each algorithm in isolation would be of little use. This
necessitates the development of the system as a whole and makes
evaluation harder. Ultimately we only have few experts users (one in
our case) and the only feedback we can receive is through them. By
including them in the design we having been able to create a system
that our expert finds useful and is willing to spend significant time
interacting with it. 

There are many directions for future work. We are planning to explore
the histogram-based contour simplification in conjunction with the
dynamic time warping alignment process to identify what is the
``optimal'' simplification of the pitch contours. More careful study
of the results by musicologists is also required. Making the system
available on the web can help collaborative approaches and reduce the
learning curve required for usage. We also hope to make the annotation
process part of the web interface and enable uploading of recordings
from researchers around the world.



\begin{acknowledgements}
We would like to thank Matt Wright for initial work on this project
and Emiru Tsunoo for the Marsyas implementation of dynamic time
warping and similarity matrix computation used in the paper.  We would
also like to thank the National Sciences and Engineering Research
Council (NSERC) and Social Sciences and Humanities Research Council
(SSHRC) of Canada for their financial support.


\end{acknowledgements}

%% % BibTeX users please use one of
%%\bibliographystyle{abbrv}      % basic style, author-year citations
%% %\bibliographystyle{spbasic}      % basic style, author-year citations
%% %\bibliographystyle{spmpsci}      % mathematics and physical sciences
%% %\bibliographystyle{spphys}       % APS-like style for physics
%%\bibliography{acm-cbmi2009gtzan}   % name your BibTeX data base

\begin{thebibliography}{10}

\bibitem{boersma2001}
P.~Boersma.
\newblock Praat, a system for doing phonetics by computer.
\newblock {\em Glot International}, 5(9/10):341--345, 2001.

\bibitem{camachophd}
A.~Camacho.
\newblock {\em A Sawtooth Waveform Inspired Pitch Estimator for Speech and
  Music}.
\newblock PhD thesis, University of Florida, 2007.

\bibitem{dannenberg2007}
R.~B. Dannenberg, W.~P. Birmingham, B.~Pardo, N.~Hu, C.~Meek, and
  G.~Tzanetakis.
\newblock A comparative evaluation of search techniques for query-by-humming
  using the musart testbed.
\newblock {\em J. Am. Soc. Inf. Sci. Technol.}, 58(5):687--701, 2007.

\bibitem{Duggan2008}
B.~Duggan, B.~O'~Shea, and P.~Cunningham.
\newblock A system for automatically annotating traditional irish music field
  recordings.
\newblock In {\em Int. Workshop on Content-Based Multimedia Indexing (CBMI)}.
  IEEE, 2008.

\bibitem{ghias1995}
A.~Ghias, J.~Logan, D.~Chamberlin, and B.~C. Smith.
\newblock Query by humming: musical information retrieval in an audio database.
\newblock In {\em MULTIMEDIA '95: Proceedings of the third ACM international
  conference on Multimedia}, pages 231--236. ACM, 1995.

\bibitem{Hanna2007}
P.~Hanna and P.~Ferraro.
\newblock Polyphonic music retrieval by local edition of quotiented sequences.
\newblock In {\em Int. Workshop on Content-Based Multimedia Indexing (CBMI)}.
  IEEE, 2007.

\bibitem{hauptman03}
A.~Hauptman and et~al.
\newblock Informedia at trec 2003 : Analyzing and searching broadcast news
  video.
\newblock In {\em Proc. of (VIDEO) TREC 2003}, Gaithersburg, MD, 2003.

\bibitem{hauptman97}
A.~Hauptman and M.~Witbrock.
\newblock {\em Informedia: News-on-demand Multimedia Information Acquisition
  and Retrieval}.
\newblock MIT Press, Cambridge, Mass, 1997.

\bibitem{karp98}
T.~Karp.
\newblock {\em Aspects of Orality and Formularity in Gregorian Chant}.
\newblock Northwestern University Press, Evanston, 1998.

\bibitem{kodaly60}
Z.~Kodaly.
\newblock {\em Folk Music of Hungary}.
\newblock Corvina Press, Budapest, 1960.

\bibitem{krumhansl90}
C.~L. Krumhansl.
\newblock {\em Cognitive Foundations of Musical Pitch}.
\newblock Oxford University Press, Oxford, 1990.

\bibitem{levy98}
K.~Levy.
\newblock {\em Gregorian Chant and the Carolingians}.
\newblock Princeton University Press, Princeton, 1998.

\bibitem{nelson85}
K.~Nelson.
\newblock {\em The Art of Reciting the Koran}.
\newblock University of Texas Press, Austin, 1985.

\bibitem{sness2008}
S.~Ness, M.~Wright, L.~Martins, and Tzanetakis.G.
\newblock {C}hants and {O}rcas: {S}emi-automatic {t}ools for {A}udio
  {A}nnotation and {A}nalysis in {N}iche {D}omains.
\newblock In {\em Proc. ACM Multimedia}, Vancouver, Canada, 2008.

\bibitem{treitler82}
L.~Treitler.
\newblock The early history of music writing in the west.
\newblock {\em Journal of the American Musicological Society}, 35, 1982.

\bibitem{Marsyas}
G.~Tzanetakis.
\newblock {\em Marsyas-0.2: A case study in implementing music information
  retrieval systems}, chapter~2, pages 31--49.
\newblock Intelligent Music Information Systems: Tools and Methodologies.
  Information Science Reference, 2008.
\newblock Shen, Shepherd, Cui, Liu (eds).

\bibitem{ce2008}
G.~Tzanetakis, K.~A, W.~Schloss, and M.~Wright.
\newblock Computational ethnomusicology.
\newblock {\em Journal of Interdisciplinary Music Studies}, 1(2), 2007.

\bibitem{wigoder89}
G.~Wigoder and et~al.
\newblock {\em Masora, The Encyclopedia of Judaism}.
\newblock MacMillan Publishing Company, New York, 1989.

\bibitem{zimmerman00}
H.~Zimmermann.
\newblock {\em Untersuchungen zur Musikauffassung des rabbinischen Judentums}.
\newblock Peter Lang, Bern, 2000.

\end{thebibliography}



\end{document}
%% % end of file template.tex


% LocalWords:  temperment
% This is "sig-alternate.tex" V1.8 June 2007
% This file should be compiled with V2.3 of "sig-alternate.cls" June 2007
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.3 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.3) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.8 - June 2007

\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{1st International Workshop on: Robust Multimedia Learning in Broad Domains}{October 31, Vancouver, BC, Canada
In conjunction with ACM Multimedia 2008}
%\CopyrightYear{2007} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Chants and Orcas, semi-automatic tools for audio annotation and analysis in niche domains}
%%% sness %\titlenote{(Produces the permission block, and copyright information). For use with SIG-ALTERNATE.CLS. Supported by ACM.}}
%%% sness %\subtitle{[Extended Abstract]
%%% sness %\titlenote{A full version of this paper is available as
%%% sness %\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%%% sness %\LaTeX$2_\epsilon$\ and BibTeX} at
%%% sness %\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven Ness\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{sness@sness.net}
% 2nd. author
\alignauthor
Matthew Wright\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{mattwrig@uvic.ca}
% 3rd. author
\alignauthor 
L. Gustavo Martins\\
       \affaddr{Telecommunications and Multimedia Unit}\\
       \affaddr{INESC Porto}\\
       \affaddr{Porto, Portugal}\\
       \email{lmartins@inescporto.pt}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
George Tzantakis\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{gtzan@cs.uvic.ca}
}
\date{05 July 2008}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
The recent explosion of web-based collaborative applications in
business and social media sites have demonstrated the power of
collaborative internet scale software.  These advantages in iclude the
ability to access to huge datasets, ability to quickly update
software, and the ability to let people around the world collaborate
seamlessly.  We present two different web-based collaborative projects
being developed in our lab, Cantillion, and the Orchive.  Cantillion
enables scholars to listen and view data relating to chants from a
variety of traditions, letting them view and interact with pitch
contour representations of the chant, to enable them to produce lower
dimensionality gesture representations of these chants.  The Orchive
is a project to digitize and present to the community a resource of
over 20,000 hours of Orcinus Orca (killer whale) song recorded at
Orcalab over a period of approximately 35 years.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Delphi theory}

\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
Web-based software has been helping connect communities of researchers
since its inception.  Recently, advances in software and in computer
power have dramatically widened its possible applications to include a
wide variety of multimedia content.  These advances have been
primarily in the business community, and the tools developed there are
starting to be used in the academic community.

We have been working on applying these technologies to ongoing
collaborative projects that we are involved in.  By leveraging several
new technologies including Flash, haXe, AJAX and Ruby on Rails, we
have been able to rapidly develop tools that have utility for our
scientific partners.  Rapid prototyping and iterative development have
been key elements of our collaborative strategy.  This agile
development strategy has proven its effectiveness in these projects.

The first of these collaborations is a project to develop tools to
study chants from various traditions around the world including
Hungarian sirat√≥k (laments), Torah cantillation, tenth century
St. Gallen plainchant, and Koran recitation.  These diverse traditions
share the common theme of having an origin in primarily non-notated
melodies which then later became codified.  The evolution and spread
of differences in the oral traditions of these different chants are a
current topic of research in Ethnomusicology.  

It has proved difficult to study these changes using traditional
methods, and it was decided that a combined approach, using field
recordings marked up by experts, advanced mathematical models for
analyzing the fundamental frequency content of the audio and a
flexible graphic user interface, would allow us to figure out what
questions needed to be asked.

The second project involved the analysis of a large archive of
recordings of Orcinus Orca (killer whale) vocalizations recorded at
OrcaLab on the west coast of Canada.

There are stable resident populations of Orcinus Orca in the northwest
Pacific Ocean, some of these populations are found near Hanson Island,
off the north tip of Vancouver Island in Canada.  Orcalab is a
research station that has been recording audio of these Orca
populations since 1972 [Reference].  They have amassed a huge archive
of more than 20,000 hours of audio recordings collected via a
permanent installation of underwater hydrophones.

The archive was recorded onto cassette and DAT tapes.  In a previous
work [Reference] we described a system whereby we digitized this audio
and stored it on a computer system.  We also presented results whereby
we could denoise orca vocalizations.

Although these recordings contain large amounts of Orca vocalizations,
the recordings also other sources of audio, including voice-overs
describing the current observing conditions, boat and cruise-ship
noise, and large sections of silence.  Finding the Orca vocalizations
on these tapes is a labor-intensive and time-consuming task.

In the current work, we present a web-based collaborative system to
assist with the task of identifying and annotating the sections of
these audio recordings that contain Orca vocalization.  This system
consists of a dynamic and researcher informed front end written in
XHTML/CSS and Flash which lets a researcher identify and label
sections of audio as Orca vocalization, voice-over or background noise.

\section{Technology:}

\subsection{Chants:}

xxxxx

We have developed algorithms to process a digitized monophonic or
heterophonic recording and produce a series of successively more
refined and abstract representations of the melodic contours.

xxxxx

It first estimates the fundamental frequency (‚ÄúF0,‚Äù in this case
equivalent to pitch) and signal energy (related to loudness) as
functions of time. We use the SWIPEP fundamental frequency estimator
(Camacho 2007) with all default parameters except for upper and lower
frequency bounds hand-tuned for each example. For signal energy we
simply take the sum of squares of signal values in each
non-overlapping 10-ms rectangular window.

\includegraphics[width=80mm]{f0contour.eps}

xxxxx

The next step is to identify pauses between phrases, so as to
eliminate the meaningless and wildly varying F0 estimates during these
noisy regions. We define an energy threshold, generally 40 decibels
below each recording‚Äôs maximum. If the signal energy stays below this
threshold for at least 100 ms then the quiet region is treated as
silence and its F0 estimates are ignored. Figure 1 shows an excerpt of
the F0 and energy curves for an excerpt from the Koran sura
(‚Äúsection‚Äù) Al-Qadr (‚Äúdestiny‚Äù) recited by the renowned Sheikh Mahm√ªd
Khal√Æl al-Husar√Æ from Egypt.

xxxxx

The next step is pitch quantization. Rather than externally imposing a
particular set of pitches such as an equal-tempered chromatic or
diatonic scale, we have developed a novel method for extracting a
scale from an F0 envelope that is continuous (or at least very densely
sampled) in both time and pitch. Our method is inspired by Krumhansl‚Äôs
time-on-pitch histograms adding up the total amount of time spent on
each pitch (Krumhanl 1990). We demand a pitch resolution of one cent9,
so we cannot use a simple histogram.10 Instead we use a statistical
technique known as nonparametric kernel density estimation, with a
Gaussian kernel.11 The resulting curve is our density estimate; like a
histogram, it can be interpreted as the relative probability of each
pitch appearing at any given point in time. Figure 2 shows this
method‚Äôs density estimate given the F0 curve from Figure 1.

xxxxx

We interpret each peak in the density estimate as a note of the
scale. We restrict the minimum interval between scale pitches
(currently 80 cents by default) by choosing only the higher peak when
there are two or more very close peaks. This method‚Äôs free parameter
is the standard deviation of the Gaussian kernel, which provides an
adjustable level of smoothness to our density estimate; we have
obtained good results with a standard deviation of 30 cents. Note that
this method has no knowledge of octaves.

\includegraphics[width=80mm]{scalehistogram.eps}

xxxxx

Once we have determined the scale, pitch quantization is the trivial
task of converting each F0 estimate to the nearest note of the scale.

xxxxx

In our opinion these derived scales are more true to the actual nature
of pitch-contour relationships within oral/aural and semi-notated
musical traditions. Instead of viewing these pitches to be deviations
of pre-existing ‚Äúnormalized‚Äù scales our method defines a more
differentiated scale from the outset. With our approach the scale
tones do not require ‚Äúnormalization‚Äù and thereby exist in an
autonomous microtonal environment defined solely on statistical
occurrence of pitch within a temporal unfolding of the given melodic
context.

xxxxx


We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways. The user
manually segments each recording into the appropriate units for each
chant type (such as trope sign, neumes, semantic units, or words). The
pitch contours of these segments can be viewed at different levels of
detail and smoothness using a histogram-based method. The segments can
also be rearranged in a variety of ways both manually and
automatically. That way one can compare the beginning and ending
pitches of any trope sign, neume or word.

xxxxx

The interface (shown in Figure 3) has four main
sections: a sound player, a main window to display the pitch contours,
a control window, and a histogram window.  The sound player window
displays a spectrogram representation of the sound file with shuttle
controls to let the user choose the current playback position in the
sound file. It also provides controls to start and pause playback of
the sound, to change the volume.  The main window shows all the pitch
contours for the song as icons that can be repositioned automatically
based on a variety of sorting criteria, or alternatively can be
manually positioned by the user. The name of each segment (from the
initial segmentation step) appears above its F0 contour. The shuttle
control of the main sound player is linked to the shuttle controls in
each of these icons, allowing the user to set the current playback
state either by clicking on the sound player window, or directly in
the icon of interest. When the user mouses over these icons, some
salient data about the sign is displayed at the bottom of the screen.
The control window has a variety of buttons that control the sorting
order of the icons in the main F0 display window. A user can sort the
icons in playback order, alphabetical order, length order, and also by
the beginning, ending, highest and lowest F0. The user can also
display the sounds in an X-Y graph, with the x-axis representing
highest F0 minus lowest F0, and the y-axis showing the ending F0 pitch
minus the beginning F0 pitch. Also in this section are controls to
toggle a mode to hear individual sounds when they are clicking on, and
controls to hide the pitch contour window leaving just the
label. 

xxxxx

There are also buttons allowing the user to choose to hear the
original sound file, the F0 curve applied to a sine wave, or the
quantized F0 curve applied to a sine wave.12 When an icon in the main
F0 display window is clicked, the histogram window shows a histogram
of the distribution of quantized pitches in the selected sign. Below
this histogram is a slider to choose how many of the largest histogram
bins will be used to generate a simplified contour representation of
the F0 curve. In the limiting case of selecting all histogram bins,
the reduced curve is exactly the quantized F0 curve. At lower values,
only the histogram bins with the most items are used to draw the
reduced curve, which has the effect of reducing the impact of outlier
values and providing a smoother ‚Äúabstract‚Äù contour.  Shift-clicking
selects multiple signs; in this case the histogram window includes the
data from all the selected signs. We often select all segments with
the same word, trope sign, or neume; this causes the simplified
contour representation to be calculated using the sum of all the
pitches found in that particular sign, enhancing the quality of the
simplified contour representation.  

\includegraphics[width=80mm]{cantillioninterface.eps}

xxxxx

Below the histogram window is a window that shows a zoomed-in graph of
the selected F0 contours. When more than one F0 contour is selected,
the lines in the graph are color coded to make it possible to easily
distinguish the different selected signs.  Discussion and Future Work
The identity of chant formulae in oral/aural chant traditions is to a
large extent determined by gesture/contour rather than by discrete
pitches. Computational approaches assist with the analysis of these
gestures/contours and enables the juxtaposition of multiple views at
different levels of detail in a variety of analytical (paradigmatic
and syntagmatic) contexts.  The possibilities for such complex
analysis methods would be difficult if not impossible without such
computer-assisted analysis. Employing these tools we hope to better
understand the role of and interchange between melodic formulae in
oral/aural and written chant cultures. While our present analysis
investigates melodic formulae primarily in terms of their gestural
content and semantic functionality, we hope that these methods might
allow scholars to reach a better understanding of the historical
development of melodic formulae within various chant traditions.

xxxxx

\subsection{Orchive}

In the current work, we present a web-based collaborative system to
assist with the task of identifying and annotating the sections of
these audio recordings that contain Orca vocalization.  This system
consists of a dynamic and researcher informed front end written in
XHTML/CSS and Flash which lets a researcher identify and label
sections of audio as Orca vocalization, voice-over or noise.

This front end then communicates with a Ruby on Rails [Reference]
based web server which stores the data in a MySQL database.  This web
server then runs machine-learning classifiers built with the Marsyas
[Reference] Music Information Retrieval system.

Marsyas builds a classifier model given the data input from the user,
and then identifies other regions in the audio archive that fit the
classifier profile and sends these new predictions back to the user.
The user then edits the label markers predicted by Marsyas, marking
sections as correctly or incorrectly labeled, and correcting end
points of classified sections.  Marsyas is then run again on the
corrected data, and submits a new prediction back to the user.

This boot-strapping procedure allows a human to rapidly annotate large
sections of the audio archive, thus making the archive more valuable
to the research community.

OrcaAnnotator is a full Model-View-Controller system containing
well-defined and well-separated sections, each of which presents a
uniform interface to the other sections of the system.  Each part is
made to be a simple and well-defined section, making them easier to
test and maintain.

The primary mode of communication with the user is via a XHTML/CSS and
Flash based interface.  The user is presented with a simple and
attractive XHTML/CSS web page that has been designed in a standards
compliant way to facilitate accessibility by the research community.

The Flash based interface is written in the haXe [Reference]
programming language, which compiles the ECMAScript language haXe down
to Flash bytecodes.

The Flash interface presents a simple interface to the user with a
spectrogram of the sound file, shuttle and volume controls, a time
display, and an interface for labelling the audio file.  We used the
Audacity labelling functionality as a model for our user-interaction
paradigm.  To add a label, the user simply clicks and drags the mouse
on the label region.  This creates a label with left and right
extents, and a text region where the user can enter a text description
of the audio.  The label also contains a pull down menu with the
choices "Orca vocalization", "Voice Over" and "Silence/Noise" to
faciliate quick labelling of audio sections.

Labels are also marked with the user that created them and the time
that they were created.  This user can be an actual user on the
system, or can be labelled with Marsyas and the name and parameters
of the classifier that was used to classify that section of audio.

Marsyas contains a number of machine-learning classifiers (Tzanetakis
and Cook 1999), including Gaussian (MAP), Gaussian Mixture-Model
(GMM), and K-nearest-neighbour algorithms.  It also provides a
framework for the quick addition of new classifiers.  We used the
"bextract" program which is part of Marsyas with a new TimeLine class
that allowed us to input human-annotated sections of audio into
Marsyas as a start for a boot strapping approach.

We also used the Weka machine-learning environment to run a variety of
different classifiers, including the Naive Bayes classifier and the
J48 classifier.

To provide communications between the Flash user-interface and the
Marsyas classifier algorithms, we have employed the Ruby on Rails web
framework[Reference].  Ruby on Rails has the advatage of being fast
and easy to setup and add new features to, and it provides a tight
glue layer to an underlying MySQL database.

Ruby on Rails also has the advantage that it makes it easy to build
REST based applications[Reference].  REST is the model on which the
internet is built and minimizes latency and network communication
while simultaneously maximizes independence and scalability of network
services.  It provides multiple

Ruby on Rails queries the database for user data, label data,
locations of audio files.  It then generates all the XHTML/CSS files
displayed to the user and sends the required XML data to the Flash
application.  Once the user submits their annotated data back to the
web server, it first stores this data in the database and then queues
this data for Marsyas to run in a separate background process, perhaps
on another machine, or network of machines.

The queuing communciation is handled by Amazon SQS which is a very
reliable and scalable queue based system that stores messages for
futher processing.  The advantage of using Amazon SQS over other home
grown queuing systems is it's high reliability and availability.

Once the results from Marsyas are complete, the results are
automatically sent back to the web server using REST web services.


\section{Preliminary Results}

\subsection{Chants}
The Cantillion chant interface has proved to have utility to
researchers in the ethnomusicology community.  By providing them with
an interface that allows them to query different dimensionality
representations of the melodic contour, they are able to visualize
chant data in new ways.  The can compare chants from different
traditions in a unified interface, looking at different pitch contours
from traditions around the world.  We are currently working with these
scholars to develop new versions of the interface based on new
questions that have arisen as part of this work.  By developing our
software in an agile way, we are quickly refactoring code to be able
to ask and answer new questions.

\subsection{Orchive}


\begin{table}
\centering
\caption{Classification Performance}
\begin{tabular}{|c|c|c|} \hline

Dataset&Correctly Classified Instances&Percent Correct\\ \hline

446A & 30403 & 90.31\\ \hline
446B & 33701 & 81.97\\ \hline
447B & 23013 & 73.65\\ \hline
448A & 15307 & 58.04\\ \hline
448B & 20822 & 74.61\\ \hline
449B & 25239 & 81.17\\ \hline
450A & 31872 & 87.85\\ \hline
450B & 23916 & 93.51\\ \hline
451A & 54281 & 89.74\\ \hline
451B & 25528 & 69.91\\ \hline
\hline\end{tabular}
\end{table}


\section{Conclusions}

By combining the expert knowledge of our scientific collaborators with
new multimedia web-based tools in an agile development strategy, we
have been able to ask new questions that had previously been out of
reach.  The large and multi-dimensional datasets in both the chant
community and in orca vocalization research are challenging fields to
study, and new web-based technologies provide the flexibility to allow
true collaboration between scientific partners in widely disparate
fields of study.


%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}

xxxxx

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{oa}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%% %APPENDICES are optional
%% %\balancecolumns
%% \appendix
%% %Appendix A
%% \section{Headings in Appendices}
%% The rules about hierarchical headings discussed above for
%% the body of the article are different in the appendices.
%% In the \textbf{appendix} environment, the command
%% \textbf{section} is used to
%% indicate the start of each Appendix, with alphabetic order
%% designation (i.e. the first is A, the second B, etc.) and
%% a title (if you include one).  So, if you need
%% hierarchical structure
%% \textit{within} an Appendix, start with \textbf{subsection} as the
%% highest level. Here is an outline of the body of this
%% document in Appendix-appropriate form:
%% \subsection{Introduction}
%% \subsection{The Body of the Paper}
%% \subsubsection{Type Changes and  Special Characters}
%% \subsubsection{Math Equations}
%% \paragraph{Inline (In-text) Equations}
%% \paragraph{Display Equations}
%% \subsubsection{Citations}
%% \subsubsection{Tables}
%% \subsubsection{Figures}
%% \subsubsection{Theorem-like Constructs}
%% \subsubsection*{A Caveat for the \TeX\ Expert}
%% \subsection{Conclusions}
%% \subsection{Acknowledgments}
%% \subsection{Additional Authors}
%% This section is inserted by \LaTeX; you do not insert it.
%% You just add the names and information in the
%% \texttt{{\char'134}additionalauthors} command at the start
%% of the document.
%% \subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.
%% % This next section command marks the start of
%% % Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.
%% %\balancecolumns % GM June 2007
%% % That's all folks!
\end{document}
% This is "sig-alternate.tex" V1.3 OCTOBER 2002
% This file should be compiled with V1.6 of "sig-alternate.cls" OCTOBER 2002
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V1.6 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.3 - OCTOBER 2002

\documentclass[letterpaper]{sig-alternate}
\begin{document}

\conferenceinfo{W4A2008 - Technical,} {April 21-22, 2008, Beijing, China. Co-Located with the 17th International World Wide Web Conference.}
\CopyrightYear{2008}
\crdata{}
%

\title{Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
Format\titlenote{(Produces the permission block, and
copyright information). For use with
SIG-ALTERNATE.CLS. Supported by ACM.}}
\subtitle{[Extended Abstract]
\titlenote{A full version of this paper is available as
\textit{Author's Guide to Preparing ACM SIG Proceedings Using
\LaTeX$2_\epsilon$\ and BibTeX} at
\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{5}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Ben Trovato\titlenote{Dr.~Trovato insisted his name
be first.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{1932 Wallamaloo Lane}\\
       \affaddr{Wallamaloo, New Zealand}\\
       \email{trovato@corporation.com}
\alignauthor G.K.M. Tobin\titlenote{The secretary disavows
any knowledge of this author's actions.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{P.O. Box 1212}\\
       \affaddr{Dublin, Ohio 43017-6221}\\
       \email{webmaster@marysville-ohio.com}
\alignauthor Lars Th{\Large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld\titlenote{This author is the
one who did all the really hard work.}\\
       \affaddr{The Th{\large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld Group}\\
       \affaddr{1 Th{\large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld Circle}\\
       \affaddr{Hekla, Iceland}\\
       \email{larst@affiliation.org}
}
\additionalauthors{Additional authors: John Smith (The Th{\o}rv\"{a}ld Group,
email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
\maketitle
\begin{abstract}
This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings. It is an {\em alternate} style which produces
a {\em tighter-looking} paper and was designed in response to
concerns expressed, by authors, over page-budgets.
It complements the document \textit{Author's (Alternate) Guide to
Preparing ACM SIG Proceedings Using \LaTeX$2_\epsilon$\ and Bib\TeX}.
This source file has been written with the intention of being
compiled under \LaTeX$2_\epsilon$\ and BibTeX.

The developers have tried to include every imaginable sort
of ``bells and whistles", such as a subtitle, footnotes on
title, subtitle and authors, as well as in the text, and
every optional component (e.g. Acknowledgments, Additional
Authors, Appendices), not to mention examples of
equations, theorems, tables and figures.

To make best use of this sample document, run it through \LaTeX\
and BibTeX, and compare this source code with the printed
output produced by the dvi file. A compiled PDF version
is available on the web page to help you with the
`look and feel'.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Delphi theory}

\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
The \textit{proceedings} are the records of a conference.
ACM seeks to give these conference by-products a uniform,
high-quality appearance.  To do this, ACM has some rigid
requirements for the format of the proceedings documents: there
is a specified format (balanced  double columns), a specified
set of fonts (Arial or Helvetica and Times Roman) in
certain specified sizes (for instance, 9 point for body copy),
a specified live area (18 $\times$ 23.5 cm [7" $\times$ 9.25"]) centered on
the page, specified size of margins (2.54cm [1"] top and
bottom and 1.9cm [.75"] left and right; specified column width
(8.45cm [3.33"]) and gutter size (.083cm [.33"]).

The good news is, with only a handful of manual
settings\footnote{Two of these, the {\texttt{\char'134 numberofauthors}}
and {\texttt{\char'134 alignauthor}} commands, you have
already used; another, {\texttt{\char'134 balancecolumns}}, will
be used in your very last run of \LaTeX\ to ensure
balanced column heights on the last page.}, the \LaTeX\ document
class file handles all of this for you.

The remainder of this document is concerned with showing, in
the context of an ``actual'' document, the \LaTeX\ commands
specifically available for denoting the structure of a
proceedings paper, rather than with giving rigorous descriptions
or explanations of such commands.

\section{The {\secit Body} of The Paper}
Typically, the body of a paper is organized
into a hierarchical structure, with numbered or unnumbered
headings for sections, subsections, sub-subsections, and even
smaller sections.  The command \texttt{{\char'134}section} that
precedes this paragraph is part of such a
hierarchy.\footnote{This is the second footnote.  It
starts a series of three footnotes that add nothing
informational, but just give an idea of how footnotes work
and look. It is a wordy one, just so you see
how a longish one plays out.} \LaTeX\ handles the numbering
and placement of these headings for you, when you use
the appropriate heading commands around the titles
of the headings.  If you want a sub-subsection or
smaller part to be unnumbered in your output, simply append an
asterisk to the command name.  Examples of both
numbered and unnumbered headings will appear throughout the
balance of this sample document.

Because the entire article is contained in
the \textbf{document} environment, you can indicate the
start of a new paragraph with a blank line in your
input file; that is why this sentence forms a separate paragraph.

\subsection{Type Changes and {\subsecit Special} Characters}
We have already seen several typeface changes in this sample.  You
can indicate italicized words or phrases in your text with
the command \texttt{{\char'134}textit}; emboldening with the
command \texttt{{\char'134}textbf}
and typewriter-style (for instance, for computer code) with
\texttt{{\char'134}texttt}.  But remember, you do not
have to indicate typestyle changes when such changes are
part of the \textit{structural} elements of your
article; for instance, the heading of this subsection will
be in a sans serif\footnote{A third footnote, here.
Let's make this a rather short one to
see how it looks.} typeface, but that is handled by the
document class file. Take care with the use
of\footnote{A fourth, and last, footnote.}
the curly braces in typeface changes; they mark
the beginning and end of
the text that is to be in the different typeface.

You can use whatever symbols, accented characters, or
non-English characters you need anywhere in your document;
you can find a complete list of what is
available in the \textit{\LaTeX\
User's Guide}\cite{Lamport:LaTeX}.

\subsection{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of
the three are discussed in the next sections.

\subsubsection{Inline (In-text) Equations}
A formula that appears in the running text is called an
inline or in-text formula.  It is produced by the
\textbf{math} environment, which can be
invoked with the usual \texttt{{\char'134}begin. . .{\char'134}end}
construction or with the short form \texttt{\$. . .\$}. You
can use any of the symbols and structures,
from $\alpha$ to $\omega$, available in
\LaTeX\cite{Lamport:LaTeX}; this section will simply show a
few examples of in-text equations in context. Notice how
this equation: \begin{math}\lim_{n\rightarrow \infty}x=0\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsubsection{Display Equations}
A numbered display equation -- one set off by vertical space
from the text and centered horizontally -- is produced
by the \textbf{equation} environment. An unnumbered display
equation is produced by the \textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols
and structures available in \LaTeX; this section will just
give a couple of examples of display equations in context.
First, consider the equation, shown as an inline equation above:
\begin{equation}\lim_{n\rightarrow \infty}x=0\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}\sum_{i=0}^{\infty} x + 1\end{displaymath}
and follow it with another numbered equation:
\begin{equation}\sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\subsection{Citations}
Citations to articles \cite{bowman:reasoning,
clark:pct, braams:babel, herlihy:methodology},
conference proceedings \cite{clark:pct} or
books \cite{salas:calculus, Lamport:LaTeX} listed
in the Bibliography section of your
article will occur throughout the text of your article.
You should use BibTeX to automatically produce this bibliography;
you simply need to insert one of several citation commands with
a key of the item cited in the proper location in
the \texttt{.tex} file \cite{Lamport:LaTeX}.
The key is a short reference you invent to uniquely
identify each work; in this sample document, the key is
the first author's surname and a
word from the title.  This identifying key is included
with each item in the \texttt{.bib} file for your article.

The details of the construction of the \texttt{.bib} file
are beyond the scope of this sample document, but more
information can be found in the \textit{Author's Guide},
and exhaustive details in the \textit{\LaTeX\ User's
Guide}\cite{Lamport:LaTeX}.

This article shows only the plainest form
of the citation command, using \texttt{{\char'134}cite}.
This is what is stipulated in the SIGS style specifications.
No other citation format is endorsed or supported.

\subsection{Tables}
Because tables cannot be split across pages, the best
placement for them is typically the top of the page
nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and
the table caption.  The contents of the table itself must go
in the \textbf{tabular} environment, to
be aligned properly in rows and columns, with the desired
horizontal and vertical rules.  Again, detailed instructions
on \textbf{tabular} material
is found in the \textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table 1 is included in the input file; compare the
placement of the table here with the table in the printed
dvi output of this document.

\begin{table}
\centering
\caption{Frequency of Special Characters}
\begin{tabular}{|c|c|l|} \hline
Non-English or Math&Frequency&Comments\\ \hline
\O & 1 in 1,000& For Swedish names\\ \hline
$\pi$ & 1 in 5& Common in math\\ \hline
\$ & 4 in 5 & Used in business\\ \hline
$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
\hline\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of
the page's live area, use the environment
\textbf{table*} to enclose the table's contents and
the table caption.  As with a single-column table, this wide
table will ``float" to a location deemed more desirable.
Immediately following this sentence is the point at which
Table 2 is included in the input file; again, it is
instructive to compare the placement of the
table here with the table in the printed dvi
output of this document.


\begin{table*}
\centering
\caption{Some Typical Commands}
\begin{tabular}{|c|c|l|} \hline
Command&A Number&Comments\\ \hline
\texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
\texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
\texttt{{\char'134}table}& 300 & For tables\\ \hline
\texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
\end{table*}
% end the environment with {table*}, NOTE not {table}!

\subsection{Figures}
Like tables, figures cannot be split across pages; the
best placement for them
is typically the top or the bottom of the page nearest
their initial cite.  To ensure this proper ``floating'' placement
of figures, use the environment
\textbf{figure} to enclose the figure and its caption.

This sample document contains examples of \textbf{.eps}
and \textbf{.ps} files to be displayable with \LaTeX.  More
details on each of these is found in the \textit{Author's Guide}.

\begin{figure}
\centering
\epsfig{file=fly.eps}
\caption{A sample black and white graphic (.eps format).}
\end{figure}

\begin{figure}
\centering
\epsfig{file=fly.eps, height=1in, width=1in}
\caption{A sample black and white graphic (.eps format)
that has been resized with the \texttt{epsfig} command.}
\end{figure}


As was the case with tables, you may want a figure
that spans two columns.  To do this, and still to
ensure proper ``floating'' placement of tables, use the environment
\textbf{figure*} to enclose the figure and its caption.
\begin{figure*}
\centering
\epsfig{file=flies.eps}
\caption{A sample black and white graphic (.eps format)
that needs to span two columns of text.}
\end{figure*}
and don't forget to end the environment with
{figure*}, not {figure}!

Note that either {\textbf{.ps}} or {\textbf{.eps}} formats are
used; use
the \texttt{{\char'134}epsfig} or \texttt{{\char'134}psfig}
commands as appropriate for the different file types.

\begin{figure}
\centering
\psfig{file=rosette.ps, height=1in, width=1in,}
\caption{A sample black and white graphic (.ps format) that has
been resized with the \texttt{psfig} command.}
\end{figure}

\subsection{Theorem-like Constructs}
Other common constructs that may occur in your article are
the forms for logical constructs like theorems, axioms,
corollaries and proofs.  There are
two forms, one produced by the
command \texttt{{\char'134}newtheorem} and the
other by the command \texttt{{\char'134}newdef}; perhaps
the clearest and easiest way to distinguish them is
to compare the two in the output of this sample document:

This uses the \textbf{theorem} environment, created by
the \texttt{{\char'134}newtheorem} command:
\newtheorem{theorem}{Theorem}
\begin{theorem}
Let $f$ be continuous on $[a,b]$.  If $G$ is
an antiderivative for $f$ on $[a,b]$, then
\begin{displaymath}\int^b_af(t)dt = G(b) - G(a).\end{displaymath}
\end{theorem}

The other uses the \textbf{definition} environment, created
by the \texttt{{\char'134}newdef} command:
\newdef{definition}{Definition}
\begin{definition}
If $z$ is irrational, then by $e^z$ we mean the
unique number which has
logarithm $z$: \begin{displaymath}{\log e^z = z}\end{displaymath}
\end{definition}

Two lists of constructs that use one of these
forms is given in the
\textit{Author's  Guidelines}.
 
There is one other similar construct environment, which is
already set up
for you; i.e. you must \textit{not} use
a \texttt{{\char'134}newdef} command to
create it: the \textbf{proof} environment.  Here
is a example of its use:
\begin{proof}
Suppose on the contrary there exists a real number $L$ such that
\begin{displaymath}
\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
\end{displaymath}
Then
\begin{displaymath}
l=\lim_{x\rightarrow c} f(x)
= \lim_{x\rightarrow c}
\left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
= \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
\frac{f(x)}{g(x)} = 0\cdot L = 0,
\end{displaymath}
which contradicts our assumption that $l\neq 0$.
\end{proof}

Complete rules about using these environments and using the
two different creation commands are in the
\textit{Author's Guide}; please consult it for more
detailed instructions.  If you need to use another construct,
not listed therein, which you want to have the same
formatting as the Theorem
or the Definition\cite{salas:calculus} shown above,
use the \texttt{{\char'134}newtheorem} or the
\texttt{{\char'134}newdef} command,
respectively, to create it.

\subsection*{A {\secit Caveat} for the \TeX\ Expert}
Because you have just been given permission to
use the \texttt{{\char'134}newdef} command to create a
new form, you might think you can
use \TeX's \texttt{{\char'134}def} to create a
new command: \textit{Please refrain from doing this!}
Remember that your \LaTeX\ source code is primarily intended
to create camera-ready copy, but may be converted
to other forms -- e.g. HTML. If you inadvertently omit
some or all of the \texttt{{\char'134}def}s recompilation will
be, to say the least, problematic.

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This section is optional; it is a location for you
to acknowledge grants, funding, editing assistance and
what have you.  In the present case, for example, the
authors would like to thank Gerald Murray of ACM for
his help in codifying this \textit{Author's Guide}
and the \textbf{.cls} and \textbf{.tex} files that it describes.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\appendix
%Appendix A
\section{Headings in Appendices}
The rules about hierarchical headings discussed above for
the body of the article are different in the appendices.
In the \textbf{appendix} environment, the command
\textbf{section} is used to
indicate the start of each Appendix, with alphabetic order
designation (i.e. the first is A, the second B, etc.) and
a title (if you include one).  So, if you need
hierarchical structure
\textit{within} an Appendix, start with \textbf{subsection} as the
highest level. Here is an outline of the body of this
document in Appendix-appropriate form:
\subsection{Introduction}
\subsection{The Body of the Paper}
\subsubsection{Type Changes and  Special Characters}
\subsubsection{Math Equations}
\paragraph{Inline (In-text) Equations}
\paragraph{Display Equations}
\subsubsection{Citations}
\subsubsection{Tables}
\subsubsection{Figures}
\subsubsection{Theorem-like Constructs}
\subsubsection*{A Caveat for the \TeX\ Expert}
\subsection{Conclusions}
\subsection{Acknowledgments}
\subsection{Additional Authors}
This section is inserted by \LaTeX; you do not insert it.
You just add the names and information in the
\texttt{{\char'134}additionalauthors} command at the start
of the document.
\subsection{References}
Generated by bibtex from your ~.bib file.  Run latex,
then bibtex, then latex twice (to resolve references)
to create the ~.bbl file.  Insert that ~.bbl file into
the .tex source file and comment out
the command \texttt{{\char'134}thebibliography}.
% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
\section{More Help for the Hardy}
The sig-alternate.cls file itself is chock-full of succinct
and helpful comments.  If you consider yourself a moderately
experienced to expert user of \LaTeX, you may find reading
it useful but please remember not to change it.
\balancecolumns % GM July 2000
% That's all folks!
\end{document}
% sig-alternate.tex
% Alternate ACM SIG Proceedings document using LaTeX2e
% Author: G.K.M. Tobin / Gerry Murray
% based upon LaTeX2.09 Guidelines, 9 June 1996
% Revisions: 1 September 1999
% 21 October 1999
% 1 July 2000
\documentclass{sig-alternate}
\begin{document}
Hello World!
\end{document}
% This is "sig-alternate.tex" V1.8 June 2007
% This file should be compiled with V2.3 of "sig-alternate.cls" June 2007
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.3 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.3) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.8 - June 2007
\documentclass{sig-alternate}
\usepackage{url} 

\begin{document}
%
% --- Author Metadata here ---
%\conferenceinfo{1st International Workshop on: Robust Multimedia Learning in Broad Domains}{October 31, Vancouver, BC, Canada In conjunction with ACM Multimedia 2008}

\conferenceinfo{MS'08,} {October 31, 2008, Vancouver, British Columbia, Canada.} 	
\CopyrightYear{2008}
\crdata{978-1-60558-316-7/08/10}

% --- End of Author Metadata ---

\title{Chants and Orcas: Semi-automatic Tools for Audio Annotation and Analysis in Niche Domains}
%%% sness %\titlenote{(Produces the permission block, and copyright information). For use with SIG-ALTERNATE.CLS. Supported by ACM.}}
%%% sness %\subtitle{[Extended Abstract]
%%% sness %\titlenote{A full version of this paper is available as
%%% sness %\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%%% sness %\LaTeX$2_\epsilon$\ and BibTeX} at
%%% sness %\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven R. Ness \\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{sness@sness.net}
% 2nd. author
\alignauthor
Matthew Wright\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{mattwrig@uvic.ca}
% 3rd. author
\alignauthor 
L. Gustavo Martins\\
       \affaddr{Telecommunications and Multimedia Unit}\\
       \affaddr{INESC Porto}\\
       \affaddr{Porto, Portugal}\\
       \email{lmartins@inescporto.pt}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
George Tzanetakis\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{gtzan@cs.uvic.ca}
}
\date{05 July 2008}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}

The recent explosion of web-based collaborative applications in
business and social media sites demonstrated the power of
collaborative internet scale software.  This includes the
ability to access huge datasets, the ability to quickly update
software, and the ability to let people around the world collaborate
seamlessly. Multimedia learning techniques have the potential to make 
unstructured multimedia data accessible, reusable, searchable, and
manageable.  We present two different web-based collaborative
projects: \emph{Cantillion}, and the \emph{Orchive}. \emph{Cantillion}
enables ethnomusicology scholars to listen and view data relating to 
chants from a variety of traditions, letting them view and interact 
with various pitch contour representations of the chant. The \emph{Orchive}
is a project to digitize over 20,000 hours of \emph{Orcinus orca} 
(killer whale) vocalizations, recorded over a period of approximately 35 years, 
and provide tools to assist their study. The developed tools utilize 
ideas and techniques that are similar to the ones used in general
multimedia domains such as sports video or news. However, 
their niche nature has presented us with special challenges 
as well as opportunities. Unlike more traditional domains where 
there are clearly defined objectives one of the biggest challenges has
been the desire to support researchers to formulate questions and 
problems related to the data even when there is no clearly defined objective. 


\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Algorithms, Human Factors}

\keywords{multimedia annotation, multimedia analysis, audio feature extraction, semi-automatic annotation, machine learning}

\section{Introduction}
Web-based software has been helping connect communities of researchers
since its inception.  Recently, advances in software and in computer
power have dramatically widened its possible applications to include a
wide variety of multimedia content.  These advances have been
primarily in the business community, and the tools developed are just
starting to be used by academics.

In recent years there has been increasing research activity in the
areas of multimedia learning and information retrieval. Most of it has
been in traditional specific domains, such as sports video
\cite{hauptman97}, news video \cite{hauptman03} and natural
images. There is broad interest in these domains and in most cases
there are clearly defined objectives such as highlights in sports
videos, explosions in news video or sunsets in natural images. Our
focus in this paper is two rather niche domains that share the
challenge of effectively accessing large amounts of data but have
their own specific characteristics and challenges
\cite{foote97}. Interest in these domains is much more focused and
specific. Unlike traditional multimedia data where most users,
including the developers of tools, can be used as annotators, in these
niche domains any type of annotation requires highly trained
experts. These are also problem seeking domains where there are no
clearly defined objectives and formulating problems is as important as
solving them. We believe that despite these challenges it is possible
to develop semi-automatic tools that can assist researchers in
significantly improving access and understanding of large data
collections.

We have been working on applying these technologies to ongoing
collaborative projects that we are involved in.  By leveraging several
new technologies including \emph{Flash}, \emph{haXe}, \emph{AJAX} and \emph{Ruby on Rails}, we
have been able to rapidly develop web-based tools that have utility
for our scientific partners.  Rapid prototyping and iterative
development have been key elements of our collaborative strategy.
This agile development strategy has proven its effectiveness in these
projects. Although our number of users is limited compared to other
areas of multimedia analysis and retrieval, this is to some degree
compensated by their passion and willingness to work closely with us 
in developing these tools. 

The first of these collaborations is a project to develop tools to
study chants from various traditions around the world including
Hungarian \emph{siratok} (laments)\cite{kodaly60}, Torah cantillation\cite{wigoder89}, tenth century
St. Gallen plainchant\cite{karp98} \cite{levy98}, and Koran recitation\cite{nelson85} .  These diverse traditions
share the common theme of having an origin in primarily non-notated
melodies which then later became codified.  The evolution and spread
of differences in the oral traditions of these different chants are a
current topic of research in Ethnomusicology. 

It has proved difficult to study these changes using traditional
methods and it was decided that a combined approach, using field
recordings marked up by experts, mathematical models for analyzing the
fundamental frequency content of the audio, and a flexible graphic user
interface, would help figure out what questions needed to be asked.

The second project involves the analysis of a large archive of
recordings of \emph{Orcinus orca} (killer whale) vocalizations \cite{ford87} recorded at
OrcaLab, a research station on the west coast of Canada. There are
stable resident populations \cite{ford89} of \emph{Orcinus orca} in the northwest Pacific 
Ocean, and some of these populations \cite{ford00} are found near Hanson Island, off the
north tip of Vancouver Island in Canada.  Orcalab is a research
station that has been recording audio of these Orca populations since
1972 \cite{deecke00, weiss06}.  They have amassed a huge archive of more than
20,000 hours of audio recordings collected via a permanent
installation of underwater hydrophones. The archive was recorded onto
cassette and DAT tapes. In a previous work \cite{tzanetakis07} a system for
digitizing the audio was presented as well as some preliminary results
in denoising orca vocalizations.

Although these recordings contain large amounts of Orca vocalizations,
the recordings also contain other sources of audio, including voice-overs
describing the current observing conditions, boat and cruise-ship
noise, and large sections of silence.  Finding the Orca vocalizations
on these tapes is a labor-intensive and time-consuming task. 

In the current work, we present a web-based collaborative system to
assist with the task of identifying and annotating the sections of
these audio recordings that contain Orca vocalizations.  This system
consists of a dynamic and user-informed front end written in
\emph{XHTML/CSS} and \emph{Flash} which lets a researcher identify and label
sections of audio as Orca vocalization, voice-over or background
noise. By using annotation boot-strapping \cite{tzanetakis04}, an approach inspired 
by semi-supervised learning, we show that it is possible to obtain 
good classification results while annotating only a small subset of
the data. This is critical as it would take several human years to fully 
annotate the entire archive. Once the data is annotated it is trivial
to focus on data of interest such as all the orca vocalizations for a
particular year without having to manually search through the audio
file to find the corresponding relevant sections. 

\newpage
\section{Domains:}

\subsection{Chants}

Our work in developing tools to assist with chant research is a
collaboration with Dr. Daniel Biro, a professor in the School of Music
at the University of Victoria. He has been collecting and studying
recordings of chant with specific focus on how music transmission
based on oral transmission and ritual was gradually changed to one
based on writing and music notation. The examples studied come from
improvised, partially notated, and gesture-based \cite{krumhansl90} notational chant
traditions: Hungarian siratok (laments)  \footnote{Archived Examples from Hungarian Academy of Science 
(1968-1973)}, Torah cantillation  \cite{zimmerman00} \footnote{Archived Examples from Hungary and Morocco from the Feher Music Center at the Bet Hatfatsut, Tel Aviv, Israel}, tenth
century St. Gallen plainchant \cite{treitler82} \footnote{Godehard Joppich and Singphoniker: Gregorian Chant from St. Gallen
(Gorgmarienh√ºtte: CPO 999267-2, 1994)}, and Koran recitation \footnote{Examples from Indonesia and Egypt: in Approaching the Koran (Ashland: White Cloud, 1999)}. 


Although Dr. Biro has been studying these recordings for some time and
has considerable computer expertise for a professor in music, the
design and development of our tools has been challenging. This is partly 
due to difficulties in communication and terminology as well as 
the fact that the work is exploratory in nature and there are no
easily defined objectives. The tool has been developed through 
extensive interactions with Dr. Biro with frequent frustration on both
sides. At the same time, a wonderful thing about expert users like Dr. Biro 
is that they are willing to spend considerable time preparing and
annotating data as well as testing the system and user interface which
is not the case in more traditional broad application domains.  


\subsection{Orca vocalizations} 

The goal of the Orchive project is to digitize acoustic data that have
been collected over a period of 36 years using a variety of analog
media at the research station OrcaLab (\url{http://www.orcalab.org}) on
Hanson Island on the west coast of Vancouver Island in
Canada. Currently we have approximately 20000 hours of analog
recordings, mostly in high quality audio cassettes. In addition to the
digitization effort which is underway, we are developing algorithms and
software tools to facilitate access and retrieval for this large audio
collection.  The size of this collection makes access and retrieval
especially challenging (for example it would take approximately 2.2
years of continuous listening to cover the entire archive).  Therefore
the developed algorithms and tools are essential for effective long
term studies employing acoustic techniques. Currently such studies
require enormous effort as the relevant acoustic tapes need to be
recovered and the relevant segments need to be tediously digitized
for analysis.

The majority of the audio recordings consist of three broad classes of
audio signals: background noise caused mainly by the hydrophones,
boats, background noise containing orca vocalizations and voice
over sections where the observer that started the recording is talking
about the details of the particular recording. In some cases there is
also significant overlap between multiple orca vocalizations. The orca
vocalizations frequently can be categorized into discrete calls that
allow expert researchers to identify their social group (matriline and
pod) and in some cases even allow identification of individuals.

Even when the data is digitized, locating a particular segment of
interest in a long monolithic audio recording can be very tedious as
users have to listen to many irrelevant sections until they can
locate what they are looking for. Even though visualizations such as
spectrograms can provide some assistance this is still a task that requires much
manual effort. In this paper we describe experiments for the automatic
classification and segmentation of the orca recordings for the
purposes of locating segments of interest and facilitating interaction
with this large audio archive.


\section{Analysis and Browsing}

\subsection{Melodic Contour Analysis} 

Our tool takes in a (digitized) monophonic or heterophonic recording
and produces a series of succesively more refined and abstract
representations of the segments it contains as well as the
corresponding melodic contours . More specifically the following
analysis stages are performed:

\begin{itemize} 
\item{Hand Labeling of Audio Segments}
\item{First Order Markov Model of Sign Sequences}
\item{F0 Estimation}
\item{F0 Pruning}
\item{Scale Derivation: Kernel Density Estimation}
\item{Quantization in Pitch}
\item{Scale-Degree Histogram}
\item{Histogram-Based Contour Abstraction}
\item{Plotting and Recombining the Segments}
\end{itemize} 


The recordings are manually segmented and annotated by the expert. 
Even though we considered the possibility of creating an automatic
segmentation tool, it was decided that the task was too subjective and 
critical to automate. Each segment is annotated with a word/symbol
that is related to the corresponding text or performance symbols used 
during the recitation. 

In order to study the transitions between signs/symbols we calculate a
first order Markov model of the sign sequence for each recording.  We
were asked to perform this type of syntagmatic analysis by
Dr. Biro. Although it is completely straightforward to perform
automatically using the annotation, it would be hard, if not impossible,
to calculate manually. Figure ~\ref{fig:transitions} shows an example
transition matrix. For a given trope sign (a row), how many total
times does it appear in the example (numeral after row label), and in
what fraction of those appearances is it followed by each of the other
trope signs?  The darkness of each cell corresponds to the fraction of
times that the trope sign in the given row is followed by the trope
sign in the given column.  (NB: Cell shading is relative to the total
number of occurrences of the trope sign in the row, so, e.g., the
black square saying that ``darga'' always precedes ``revia''
represents 1/1, while the black square saying that ``zakef'' always
precedes ``katon'' represents 9/9.)

\begin{figure}[htb]
\includegraphics[width=80mm]{transition_matrix}
\label{fig:transitions} 
\caption{
Syntagmatic analysis with a first-order Markov model of the
sequence of Torah trope signs for the text Shir Ha Shirim (``Song of
Songs'').} 
\end{figure} 


After the segments have been identified, the fundamental frequency
(``F0'' in this case equivalent to pitch) and signal energy (related to
loudness) are calculated for each segment as functions of time. We use
the SWIPEP fundamental frequency estimator \cite{camachophd} with all
default parameters except for upper and lower frequency bounds that are
hand-tuned for each example. For signal energy we simply take the sum
of squares of signal values in each non-overlapping 10-ms rectangular
window.


\begin{figure}[htb]
\includegraphics[width=80mm]{f0contour}
\label{fig:contour}
\caption{F0 contour} 
\end{figure} 

The next step is to identify pauses between phrases, so as to
eliminate the meaningless and wildly varying F0 estimates during these
noisy regions. We define an energy threshold, generally 40 decibels
below each recording's maximum. If the signal energy stays below this
threshold for at least 100 ms then the quiet region is treated as
silence and its F0 estimates are ignored. Figure~\ref{fig:contour}
shows an excerpt of the F0 and energy curves for an excerpt from the
Koran sura (``section'') Al-Qadr (``destiny'') recited by the renowned
Sheikh Mahmud Khalil al-Husari from Egypt.

Following the pitch contour extraction is pitch quantization, which is
the discretization of the continuous pitch contour into discrete notes
of a scale. Rather than externally imposing a particular set of
pitches, such as an equal-tempered chromatic (the piano keys) or
diatonic scale, we have developed a novel method for extracting a
scale from an F0 envelope that is continuous (or at least very densely
sampled) in both time and pitch. Our method is inspired by Krumhansl's
time-on-pitch histograms adding up the total amount of time spent on
each pitch \cite{krumhansl90}. We demand a pitch resolution of one
cent \footnote{One cent is 1/100 of a semitone, corresponding to a
  frequency difference of about 0.06\%}, so we cannot use a simple
histogram. Instead we use a statistical technique known as
nonparametric kernel density estimation, with a Gaussian kernel
\footnote{Thinking statistically, our scale is related to a
distribution given the relative probability of each possible
pitch. We can think of each F0 estimate (i.e each sampled value of
the F0 envelope) as a sample drawn from this unknown distribution so
our problem becomes one of estimation the unknown distribution given
the samples}. More specifically a Gaussian (with standard deviation
of 33 cents) is centered on each sample of the frequency estimate and
the Gaussians of all the samples are added to form the kernel density
estimate. The resulting curve is our density estimate; like a
histogram, it can be interpreted as the relative probability of each
pitch appearing at any given point in time. Figure 2 shows this
method's density estimate given the F0 curve from Figure 1.


We interpret each peak in the density estimate as a note of the
scale. We restrict the minimum interval between scale pitches
(currently 80 cents by default) by choosing only the higher peak when
there are two or more very close peaks. This method's free parameter
is the standard deviation of the Gaussian kernel, which provides an
adjustable level of smoothness to our density estimate; we have
obtained good results with a standard deviation of 30 cents. Note that
this method has no knowledge of octaves.

\begin{figure} 
\includegraphics[width=80mm]{scalehistogram}
\label{fig:scale}
\caption{Recording-specific scale derivation} 
\end{figure} 

Once we have determined the scale, pitch quantization is the trivial
task of converting each F0 estimate to the nearest note of the scale.
In our opinion these derived scales are more true to the actual nature
of pitch-contour relationships within oral/aural and semi-notated
musical traditions. Instead of viewing these pitches to be deviations
of pre-existing ``normalized'' scales our method defines a more
differentiated scale from the outset. With our approach the scale
tones do not require ``normalization'' and thereby exist in an
autonomous microtonal environment defined solely on statistical
occurrence of pitch within a temporal unfolding of the given melodic
context.

Once the pitch contour is quantized into the recording-specific scale
calculated using Kernel density estimation, we can calculate how many 
times a particular scale degree appears during an excerpt. The
resulting data is a scale-degree histogram which is used create
simplify abstract visual representations of the melodic contours. 



\begin{figure}[htb]
\includegraphics[width=80mm]{cantillion-30pashta-histogramlevels}
\label{fig:contours_histogram} 
\caption{Melodic contours at different levels of abstraction (top:
  original, middle: quantized, bottom: simplified using 3 most
  prominent scale degrees}
\end{figure} 

The basic idea is to only use the most salient discrete scale degrees
(the histogram bins with the highest magnitude) as significant points
to simplify the representation of the contour. By adjusting the number
of prominent scale degrees used to represent the simplified
representation the researchers can view/listen to the melodic contour
at different levels of abstraction and detail. Figure
~\ref{fig:contours_histogram} shows an original continuous contour, 
the quantized representation using the recording-specific derived
scale and the abstracted representation using only the 3 most prominent 
scale degrees. 






\subsection{Cantillion interface} 

We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways
(\url{http://cantillation.sness.net}). Each recording is manually
segmented into the appropriate units for each chant type (such as
trope sign, neumes, semantic units, or words). The pitch contours of
these segments can be viewed at different levels of detail and
smoothness using a histogram-based method. The segments can also be
rearranged in a variety of ways both manually and automatically. That
way one can compare the beginning and ending pitches of any trope
sign, neume or word.

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=160mm]{cantillioninterface}
\end{center}
\label{fig:cantillion} 
\caption{
Web-based \emph{Flash} interface to allow users to listen to audio, and to
enable interactive querying of gesture contour diagrams.}
\end{figure*} 


The interface ~\ref{fig:cantillion} has four main sections: a sound
player, a main window to display the pitch contours, a control window,
and a histogram window.  The sound player window displays a
spectrogram representation of the sound file with shuttle controls to
let the user choose the current playback position in the sound
file. The main window shows all the pitch contours for the song as
icons that can be repositioned automatically based on a variety of
sorting criteria, or alternatively can be manually positioned by the
user. The name of each segment (from the initial segmentation step)
appears above its F0 contour. The shuttle control of the main sound
player is linked to the shuttle controls in each of these icons,
allowing the user to set the current playback state either by clicking
on the sound player window, or directly in the icon of interest. 

When an icon in the main F0 display window is clicked, the histogram
window shows a histogram of the distribution of quantized pitches in
the selected sign. Below this histogram is a slider to choose how many
of the largest histogram bins will be used to generate a simplified
contour representation of the F0 curve. In the limiting case of
selecting all histogram bins, the reduced curve is exactly the
quantized F0 curve. At lower values, only the histogram bins with the
most items are used to draw the reduced curve, which has the effect of
reducing the impact of outlier values and providing a smoother
``abstract'' contour.  Shift-clicking selects multiple signs; in this
case the histogram window includes the data from all the selected
signs. We often select all segments with the same word, trope sign, or
neume; this causes the simplified contour representation to be
calculated using the sum of all the pitches found in that particular
sign, enhancing the quality of the simplified contour representation.
Figure ~\ref{fig:cantillion} shows a screenshot of the browsing
interface. 




The identity of chant formulae in oral/aural chant traditions is to a
large extent determined by gesture/contour rather than by discrete
pitches. Computational approaches assist with the analysis of these
gestures/contours and enables the juxtaposition of multiple views at
different levels of detail in a variety of analytical (paradigmatic
and syntagmatic) contexts.  The possibilities for such complex
analysis methods would be difficult if not impossible without such
computer-assisted analysis. Employing these tools we hope to better
understand the role of and interchange between melodic formulae in
oral/aural and written chant cultures. While our present analysis
investigates melodic formulae primarily in terms of their gestural
content and semantic functionality, we hope that these methods might
allow scholars to reach a better understanding of the historical
development of melodic formulae within various chant traditions.




\subsection{Orchive}

The \emph{Orchive} (\url{http://orchive.cs.uvic.ca}) is a web-based collaborative system designed to assist
with the task of identifying and annotating sections of audio
recordings that contain orca vocalizations.  This system consists of a
dynamic front end written in \emph{XHTML/CSS} and \emph{Flash}. The interface
allows the user to annotate regions of the recording as  ``orca'' and ``voiceover'', 
and automatically assigns the ``background'' label to
unlabeled regions of the audio.  In voiceover sections the observer
that started the tape recording talks about the details of the
particular recording such as the geographic location of the Orcas, the
time of the day, the weather conditions and other items of note. A sample section of audio
with voiceover, orca vocalizations and background is shown in Figure
~\ref{fig:orcascreenshot}. Although we eventually want to provide more
detailed classification, such as the type of orca calls, in practical
terms this basic classification to three categories is very
important to the researchers involved.

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=160mm]{orca-waveform-spectrogram-screenshot}
\end{center}
\label{fig:orcascreenshot} 
\caption{
An annotated region of audio from the \emph{Orchive}, with regions of voice
and orca vocalization shown.  Unlabeled regions are automatically
assigned a label of background noise.}
\end{figure*} 


This web
server then runs audio feature extraction and performs supervised and
semi-supervised learning using the \emph{Marsyas} \cite{tzanetakis00}
(\url{http://marsyas.sness.net})  open
source software framework for audio analysis. 

\newpage
OrcaAnnotator is a Model-View-Controller system containing
well-defined and well-separated sections, each of which presents a
uniform interface to the other sections of the system.  Each part is
made to be a simple and well-defined unit, making them easier to
test and maintain.

The primary mode of communication with the user is via an
\emph{XHTML/CSS} and \emph{Flash} based interface. The user is
presented with a simple and attractive \emph{XHTML/CSS} web page that
has been designed to be standards compliant which will facilitate
accessibility by the research community on a wide variety of different
web browsers and computer platforms.  The \emph{Flash} based interface
is written in the haXe \cite{mccoll08} programming language, which
compiles the ECMAScript language \emph{haXe} down to \emph{Flash}
bytecodes. The \emph{Flash} interface presents a simple interface to
the user with a spectrogram of the sound file, shuttle and volume
controls, a time display, and an interface for labeling the audio
file.  We used the labeling functionality in \emph{Audacity} as a
model for our user-interaction paradigm.  To add a label, the user
simply clicks and drags the mouse on the label region.  This creates a
label with left and right extents, and a text region where the user
can enter a text description of the audio. In addition, a pull-down menu 
with labels can be used for quick annotation. 


Labels are saved to the database with the user that created them and
the time that they were created.  This user can be an actual user on
the system, or can be labeled with \emph{Marsyas} and the name and
parameters of the classifier that was used for
labeling. \emph{Marsyas} contains a number of machine-learning
classifiers, including Gaussian (MAP), Gaussian Mixture-Model (GMM),
and Support Vector Machines (SVM).  We used the ``bextract'' program
which is part of \emph{Marsyas}, which now includes a new Timeline
module that allows the import of human-annotated sections of audio
into \emph{Marsyas} as a start for a bootstrapping approach. A variety
of standard audio feature extraction algorithms such as Mel-Frequency
Cepstral Coefficients (MFCC) as well as various types of spectral
features are also provided. The integration of machine learning and
audio signal processing is essential in creating a semi-automatic
annotation interface.

To provide communication between the \emph{Flash} user-interface and the
\emph{Marsyas} classifier algorithms, we have employed the \emph{Ruby on Rails} web
framework\cite{thomas06}.  \emph{Ruby on Rails} allows for quick and easy development and
deployment of websites, and it provides a tight
interface layer to an underlying database like \emph{MySQL}.

\emph{Ruby on Rails} also has the advantage that it makes it simple to
build REST based applications\cite{fieldingphd}.  REST is the model on
which the internet is built and has the ability to minimize latency
and network communication, while simultaneously maximizing the
independence and scalability of network services. \emph{Ruby on Rails}
queries the database for user data, label data and locations of audio
files.  It then generates all the \emph{XHTML/CSS} files displayed to
the user and sends the required XML data to the \emph{Flash}
application.  Once the user submits their annotated data back to the
web server, it first stores this data in the database and then queues
this data for \emph{Marsyas} to run in a separate background process,
perhaps on another machine, or network of machines. Once
\emph{Marsyas} completes processing the audio, the results are
automatically sent back to the web server using REST web services.

Being able to segment and label the audio recordings into the three
main categories (voiceover, orca vocalizations and background noise)
is immensly useful to researchers working with this vast amount of
data. For example background noise comprises approximately $64\% $ of
the recordings, and is much higher in some individual recordings.
Fully annotating the data even using a well-designed user interface is
out of the question given the size of the archive. To address this
problem we have designed a semi-supervised learning system that only
requires manual annotation of a small percentage of the data and
utilizes machine learning techniques to annotate the other part. This
recording-specific annotation bootstrapping can potentially be used
with other types of time-based multimedia data.
\newpage



\subsection{Annotation Bootstrapping} 

Annotation bootstrapping is inspired by semi-supervised learning
\cite{chapelle2006}.  It has been shown that unlabeled data, when used
in conjuction with a small amount of labeled data, can produce
considerable improvements in learning accuracy. The acquisition of
labeled data for a learning problem often requires manual annotation
which is a time consuming process so semi-supervised learning can
significantly reduce annotation time for large multimedia archives. 

We extend the idea of semi-supervised learning to take advantage 
of the strong correlation between feature vectors from the same 
audio recording.  In the \emph{Orchive} each audio recording has a duration 
of 45 minutes and corresponds to a particular date and time. 
There is considerable consistency within a recording as the same 
person is doing the voiceover sections, the mixing settings are the
same and the orcas that are vocalizing typically come from the same group. 
A recording-specific bootstrap classifier is trained as follows: 
a small percentage of the specific audio recording is manually
annotated and used to train a recording-specific classifier. 
This classifier is then used to label the remaining parts of the
recording.  Due to the consistency of the recording this classifier will 
be to some extent overfitted to the recording and will not generalize
well to other recordings.  However, that is not a problem in our case as we are
mainly interested in obtained labels for the entire recording. This
process is repeated for each recording.  Once all the recordings have 
been semi-automatically fully labeled then feature extraction is
performed for the entire archive and a generalizing classifier is
trained using the full dataset. 

\begin{table}
\centering
\caption{Recording-specific classification performance}
\begin{tabular}{|c|c|c|c|c|} \hline

&\multicolumn{2}{|c|}{Naive bayes}&\multicolumn{2}{|c|}{SMO}\\
&\multicolumn{2}{|c|}{\% correct}&\multicolumn{2}{|c|}{\% correct}\\ \hline
&self&train with&self&train with\\
&&remaining&&remaining\\ \hline

446A  &  89.42  &  93.10  &  95.00  &  73.39\\ \hline
446B  &  63.45  &  77.66  &  85.85  &  70.23\\ \hline
447B  &  75.46  &  57.32  &  82.02  &  68.17\\ \hline
448A  &  52.18  &  61.02  &  81.57  &  62.24\\ \hline
448B  &  84.63  &  67.62  &  83.64  &  67.87\\ \hline
449B  &  82.24  &  51.85  &  86.41  &  75.72\\ \hline
450A  &  94.66  &  90.91  &  96.12  &  91.58\\ \hline
450B  &  83.65  &  96.27  &  99.29  &  94.92\\ \hline
451A  &  70.92  &  89.58  &  97.04  &  78.72\\ \hline
451B  &  74.18  &  33.73  &  82.34  &  50.88\\ \hline
\end{tabular}
\label{table:classification}
\end{table}



In order to explore whether this idea would work for our data, we 
created a representative database consisting of 10 excerpts from 
our recordings with each excerpt lasting between 5 and 10 minutes. 
Table ~\ref{table:classification} shows classification results using 
10-fold cross-validation for each particular recording using a
recording specific classifier as well as using a classifier trained 
on the entire dataset. Two classifiers are used: a simple Naive Bayes
classifier (NBS), as well as a Support Vector Machine (SVM). The results shown 
are based on the use of the standard Mel-Frequency Cepstral
Coefficients (MFCC) as audio features. The ``self'' column shows the 
classification accuracy results of using a recording-specific
classifier, whereas the ``remaining'' columns 
shows the classification accuracy results using the remaining nine
recordings. As can be seen, recording-specific classifier can generate
significantly better results than generalized classifiers, which is not 
surprising as they adapt to the specific data of the recording. This
justifies the use of their annotation results to labeled the unlabeled 
parts of the audio recording. 



\begin{table}
\centering
\caption{Classification performance using annotation-bootstrapping
  (SVM classifier)}
\begin{tabular}{|c|c|c|} \hline

\% data &\%correct&F-measure\\
used    & &\\ \hline

100  &  82.38  &  0.876\\ \hline
10   &  81.98  &  0.874\\ \hline
5    &  82.04  &  0.874\\ \hline
1    &  79.95  &  0.864\\ \hline
0.1  &  78.08  &  0.857\\ \hline
0.01 &  71.42  &  0.800\\ \hline
\end{tabular}
\label{table:annotation_classification} 
\end{table}



The goal of annotation bootstrapping is to only label a small part of
each recording to train a recording-specific classifier which is then
used to annotate the remainder of the recording. Table
~\ref{table:annotation_classification} shows the results in terms of
classification accuracy and F-measure over the entire dataset for
different amounts of labeled data. As one can see the classification
accuracy remains quite good, even when only a small percentage of the
data is labeled and annotation bootstrapping is used to label the
rest. The first row shows the classification accuracy when all the
data is used for training.

\begin{figure}[htbp]

% For pdflatex
%\includegraphics[width=80mm]{smodata}

% For latex (postscript)
\includegraphics[height=80mm,angle=-90]{smodata}

\label{fig:graph} 
\caption{Graph of classification accuracy as percentage of labeling required.  
Data shown is for the performance of SMO classifier for different percentages of
data used to train the classifier.}
\end{figure} 



Figure ~\ref{fig:graph} shows graphically how the classification
accuracy increases with the amount of labeled data used for
training. In both the table and the figure, the classifier used is a
Support Vector Machine (SVM) and for evaluation a variation of 10-fold
cross-validation where each of the 10 recordings is held out for
testing, the remaining ones are used for training, and the process is
iterated 10 times. We also experimented with different choices of
window size for the feature calculation as well as different audio
feature parametrization but there was no significant difference in the
obtained results. 

To make the importance of annotation bootstrapping concrete, fully
annotating the archive would take approximately 2 and half years
(assuming 24/7 manual annotation) whereas using one percent annotation
bootstrapping would take 3 months (assuming 24/7 manual annotation)
without significantly affecting the ability of the system to
succesfully label all the recordings in the 3 classes of interest.




\section{Conclusions}

By combining the expert knowledge of our scientific collaborators with
new multimedia web-based tools in an agile development strategy, we
have been able to ask new questions that had previously been out of
reach.  The large and multi-dimensional datasets in both the chant
community and in orca vocalization research provide challenging fields for
study, and new web-based technologies provide the flexibility to allow
true collaboration between scientific partners in widely disparate
fields of study.  We described an automatic technique for simplifying
melodic contours based on kernel density estimation.  Annotation of the
archive of orca vocalizations is very time-consuming, we proposed
annotation bootstrapping and show that it is an effective technique 
for automatically annotating recordings. 


\section{Acknowledgments}

We would like to thank Paul Spong and Helena Symonds of Orcalab 
and well as Daniel Biro for providing the data and inspiration for
this project. We would also like to thank the National Sciences
and Engineering Research Council (NSERC) and Social
Sciences and Humanities Research Council (SSHRC) of
Canada for their financial support. 



%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{oa}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%% %APPENDICES are optional
%% %\balancecolumns
%% \appendix
%% %Appendix A
%% \section{Headings in Appendices}
%% The rules about hierarchical headings discussed above for
%% the body of the article are different in the appendices.
%% In the \textbf{appendix} environment, the command
%% \textbf{section} is used to
%% indicate the start of each Appendix, with alphabetic order
%% designation (i.e. the first is A, the second B, etc.) and
%% a title (if you include one).  So, if you need
%% hierarchical structure
%% \textit{within} an Appendix, start with \textbf{subsection} as the
%% highest level. Here is an outline of the body of this
%% document in Appendix-appropriate form:
%% \subsection{Introduction}
%% \subsection{The Body of the Paper}
%% \subsubsection{Type Changes and  Special Characters}
%% \subsubsection{Math Equations}
%% \paragraph{Inline (In-text) Equations}
%% \paragraph{Display Equations}
%% \subsubsection{Citations}
%% \subsubsection{Tables}
%% \subsubsection{Figures}
%% \subsubsection{Theorem-like Constructs}
%% \subsubsection*{A Caveat for the \TeX\ Expert}
%% \subsection{Conclusions}
%% \subsection{Acknowledgments}
%% \subsection{Additional Authors}
%% This section is inserted by \LaTeX; you do not insert it.
%% You just add the names and information in the
%% \texttt{{\char'134}additionalauthors} command at the start
%% of the document.
%% \subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.
%% % This next section command marks the start of
%% % Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.
%% %\balancecolumns % GM June 2007
%% % That's all folks!
\end{document}

\begin{thebibliography}{10}

\bibitem{camachophd}
A.~Camacho.
\newblock {\em A Sawtooth Waveform Inspired Pitch Estimator for Speech and
  Music}.
\newblock PhD thesis, University of Florida, 2007.

\bibitem{chapelle2006}
O.~Chappelle, B.~Scholkopf, and A.~Zien.
\newblock {\em Semi-Supervised Learning}.
\newblock MIT Press, Cambridge, MA, 2006.

\bibitem{deecke00}
V.~Deecke, J.~Ford, and P.~Spong.
\newblock Dialect change in resident killer whales (orcinus orca): implications
  for vocal learning and cultural transmission.
\newblock {\em Animal Behaviour}, 60(5):619--638, 2000.

\bibitem{fieldingphd}
R.~T. Fielding.
\newblock {\em Architectural Styles and the Design of Network-based Software
  Architectures}.
\newblock Phd dissertation, University Of California, Irvine.

\bibitem{foote97}
J.~Foote.
\newblock {\em Content-based Retrieval of Music and Audio}, pages 138--147.
\newblock 1997.

\bibitem{ford87}
J.~Ford.
\newblock A catalogue of underwater calls produced by killer whales (orcinus
  orca) in british columbia.
\newblock Technical Report 633, Canadian Data Report of Fisheries and Aquatic
  Science, 1987.

\bibitem{ford89}
J.~Ford.
\newblock Acoustic behaviour of resident killer whales (orcinus orca) off
  vancouver island, british columbia.
\newblock {\em Canadian Journal of Zoology}, 64:727--745, 1989.

\vfill \eject
\bibitem{ford00}
J.~Ford, E.~G.M., and B.~K.C.
\newblock {\em Killer Whales : The natural history and genealogy of Orcinus
  orca in British Columbia and Washington, 2nd ed}.
\newblock UBC, Vancouver, 2000.

\bibitem{hauptman03}
A.~Hauptman and et~al.
\newblock Informedia at trec 2003 : Analyzing and searching broadcast news
  video.
\newblock In {\em Proc. of (VIDEO) TREC 2003}, Gaithersburg, MD, 2003.

\bibitem{hauptman97}
A.~Hauptman and M.~Witbrock.
\newblock {\em Informedia: News-on-demand Multimedia Information Acquisition
  and Retrieval}.
\newblock MIT Press, Cambridge, Mass, 1997.

\bibitem{karp98}
T.~Karp.
\newblock {\em Aspects of Orality and Formularity in Gregorian Chant}.
\newblock Northwestern University Press, Evanston, 1998.

\bibitem{kodaly60}
Z.~Kodaly.
\newblock {\em Folk Music of Hungary}.
\newblock Corvina Press, Budapest, 1960.

\bibitem{krumhansl90}
C.~L. Krumhansl.
\newblock {\em Cognitive Foundations of Musical Pitch}.
\newblock Oxford University Press, Oxford, 1990.

\bibitem{levy98}
K.~Levy.
\newblock {\em Gregorian Chant and the Carolingians}.
\newblock Princeton University Press, Princeton, 1998.

\bibitem{mccoll08}
L.~McColl-Sylvester and F.~Ponticelli.
\newblock {\em Professional haXe and Neko}.
\newblock Wiley Publishing, Inc., Indianapolis, IN, 2008.

\bibitem{nelson85}
K.~Nelson.
\newblock {\em The Art of Reciting the Koran}.
\newblock University of Texas Press, Austin, 1985.

\bibitem{thomas06}
D.~Thomas, D.~Hansson, L.~Breedt, M.~Clark, J.~D. Davidson, J.~Gehtland, and
  A.~Schwarz.
\newblock {\em Agile Web Development with Rails, 2nd Edition}.
\newblock Pragmatic Bookshelf, Flower Mound, TX, 2006.

\bibitem{treitler82}
L.~Treitler.
\newblock The early history of music writing in the west.
\newblock {\em Journal of the American Musicological Society}, 35, 1982.

\bibitem{tzanetakis04}
G.~Tzanetakis.
\newblock Song specific bootstrapping of singing voice structure.
\newblock In {\em Proc. Int. Conf. on Multimedia and Exposition ICME}, TaiPei,
  Taiwan, 2004. IEEE.

\bibitem{tzanetakis00}
G.~Tzanetakis and P.~Cook.
\newblock Marsyas: A framework for audio analysis.
\newblock {\em Organized Sound}, 4(3), 2000.

\bibitem{tzanetakis07}
G.~Tzanetakis, M.~Lagrange, P.~Spong, and H.~Symonds.
\newblock Orchive: Digitizing and analyzing orca vocalizations.
\newblock In {\em Proc. of the RIAO, Large-Scale Semantic Access to Content
  Conference}. RIAO, 2007.

\bibitem{weiss06}
B.~M. Weiss, F.~Ladich, P.~Spong, and H.~Symonds.
\newblock Vocal behavior of resident killer whale matrilines with newborn
  calves: The role of family signatures.
\newblock {\em The Journal of the Acoustical Society of America},
  119(1):627--635, 2006.

\bibitem{wigoder89}
G.~Wigoder and et~al.
\newblock {\em Masora, The Encyclopedia of Judaism}.
\newblock MacMillan Publishing Company, New York, 1989.

\bibitem{zimmerman00}
H.~Zimmermann.
\newblock {\em Untersuchungen zur Musikauffassung des rabbinischen Judentums}.
\newblock Peter Lang, Bern, 2000.

\end{thebibliography}
% This is "sig-alternate.tex" V1.8 June 2007
% This file should be compiled with V2.3 of "sig-alternate.cls" June 2007
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.3 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.3) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.8 - June 2007

\documentclass{sig-alternate}
\usepackage{url} 

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{1st International Workshop on: Robust Multimedia Learning in Broad Domains}{October 31, Vancouver, BC, Canada
In conjunction with ACM Multimedia 2008}
%\CopyrightYear{2007} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Chants and Orcas: Semi-automatic Tools for Audio Annotation and Analysis in Niche Domains}
%%% sness %\titlenote{(Produces the permission block, and copyright information). For use with SIG-ALTERNATE.CLS. Supported by ACM.}}
%%% sness %\subtitle{[Extended Abstract]
%%% sness %\titlenote{A full version of this paper is available as
%%% sness %\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%%% sness %\LaTeX$2_\epsilon$\ and BibTeX} at
%%% sness %\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven Ness \\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{sness@sness.net}
% 2nd. author
\alignauthor
Matthew Wright\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{mattwrig@uvic.ca}
% 3rd. author
\alignauthor 
L. Gustavo Martins\\
       \affaddr{Telecommunications and Multimedia Unit}\\
       \affaddr{INESC Porto}\\
       \affaddr{Porto, Portugal}\\
       \email{lmartins@inescporto.pt}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
George Tzanetakis\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{gtzan@cs.uvic.ca}
}
\date{05 July 2008}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}

The recent explosion of web-based collaborative applications in
business and social media sites demonstrated the power of
collaborative internet scale software.  This includes the
ability to access huge datasets, the ability to quickly update
software, and the ability to let people around the world collaborate
seamlessly. Multimedia learning techniques have the potential to make 
unstructured multimedia data accessible, reusable, searchable, and
manageable.  We present two different web-based collaborative
projects: \emph{Cantillion}, and the \emph{Orchive}. \emph{Cantillion}
enables ethnomusicology scholars to listen and view data relating to 
chants from a variety of traditions, letting them view and interact 
with various pitch contour representations of the chant. The \emph{Orchive}
is a project to digitize over 20,000 hours of \emph{Orcinus orca} 
(killer whale) vocalizations, recorded over a period of approximately 35 years, 
and provide tools to assist their study. The developed tools utilize 
ideas and techniques that are similar to the ones used in general
multimedia domains such as sports video or news. However, 
their niche nature has presented us with special challenges 
as well as opportunities. Unlike more traditional domains where 
there are clearly defined objectives one of the biggest challenges has
been the desire to support researchers to formulate questions and 
problems related to the data even when there is no clearly defined objective. 


\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{multimedia annotation, multimedia analysis}

\keywords{audio feature extraction, semi-automatic annotation, machine
  learning}

\section{Introduction}
Web-based software has been helping connect communities of researchers
since its inception.  Recently, advances in software and in computer
power have dramatically widened its possible applications to include a
wide variety of multimedia content.  These advances have been
primarily in the business community, and the tools developed are just
starting to be used by academics.

In recent years there has been increasing research activity in the
areas of multimedia learning and information retrieval. Most of it has
been in traditional specific domains, such as sports video
\cite{hauptman97}, news video \cite{hauptman03} and natural
images. There is broad interest in these domains and in most cases
there are clearly defined objectives such as highlights in sports
videos, explosions in news video or sunsets in natural images. Our
focus in this paper is two rather niche domains that share the
challenge of effectively accessing large amounts of data but have
their own specific characteristics and challenges
\cite{foote97}. Interest in these domains is much more focused and
specific. Unlike traditional multimedia data where most users,
including the developers of tools, can be used as annotators, in these
niche domains any type of annotation requires highly trained
experts. These are also problem seeking domains where there are no
clearly defined objectives and formulating problems is as important as
solving them. We believe that despite these challenges it is possible
to develop semi-automatic tools that can assist researchers in
significantly improving access and understanding of large data
collections.

We have been working on applying these technologies to ongoing
collaborative projects that we are involved in.  By leveraging several
new technologies including \emph{Flash}, \emph{haXe}, \emph{AJAX} and \emph{Ruby on Rails}, we
have been able to rapidly develop web-based tools that have utility
for our scientific partners.  Rapid prototyping and iterative
development have been key elements of our collaborative strategy.
This agile development strategy has proven its effectiveness in these
projects. Although our number of users is limited compared to other
areas of multimedia analysis and retrieval, this is to some degree
compensated by their passion and willingness to work closely with us 
in developing these tools. 

The first of these collaborations is a project to develop tools to
study chants from various traditions around the world including
Hungarian \emph{siratok} (laments)\cite{kodaly60}, Torah cantillation\cite{wigoder89}, tenth century
St. Gallen plainchant\cite{karp98} \cite{levy98}, and Koran recitation\cite{nelson85} .  These diverse traditions
share the common theme of having an origin in primarily non-notated
melodies which then later became codified.  The evolution and spread
of differences in the oral traditions of these different chants are a
current topic of research in Ethnomusicology. 

It has proved difficult to study these changes using traditional
methods and it was decided that a combined approach, using field
recordings marked up by experts, mathematical models for analyzing the
fundamental frequency content of the audio, and a flexible graphic user
interface, would help figure out what questions needed to be asked.

The second project involves the analysis of a large archive of
recordings of \emph{Orcinus orca} (killer whale) vocalizations \cite{ford87} recorded at
OrcaLab, a research station on the west coast of Canada. There are
stable resident populations \cite{ford89} of \emph{Orcinus orca} in the northwest Pacific 
Ocean, and some of these populations \cite{ford00} are found near Hanson Island, off the
north tip of Vancouver Island in Canada.  Orcalab is a research
station that has been recording audio of these Orca populations since
1972 \cite{deecke00, weiss06}.  They have amassed a huge archive of more than
20,000 hours of audio recordings collected via a permanent
installation of underwater hydrophones. The archive was recorded onto
cassette and DAT tapes. In a previous work \cite{tzanetakis07} a system for
digitizing the audio was presented as well as some preliminary results
in denoising orca vocalizations.

Although these recordings contain large amounts of Orca vocalizations,
the recordings also contain other sources of audio, including voice-overs
describing the current observing conditions, boat and cruise-ship
noise, and large sections of silence.  Finding the Orca vocalizations
on these tapes is a labor-intensive and time-consuming task. 

In the current work, we present a web-based collaborative system to
assist with the task of identifying and annotating the sections of
these audio recordings that contain Orca vocalizations.  This system
consists of a dynamic and user-informed front end written in
\emph{XHTML/CSS} and \emph{Flash} which lets a researcher identify and label
sections of audio as Orca vocalization, voice-over or background
noise. By using annotation boot-strapping \cite{tzanetakis04}, an approach inspired 
by semi-supervised learning, we show that it is possible to obtain 
good classification results while annotating only a small subset of
the data. This is critical as it would take several human years to fully 
annotate the entire archive. Once the data is annotated it is trivial
to focus on data of interest such as all the orca vocalizations for a
particular year without having to manually search through the audio
file to find the corresponding relevant sections. 

\newpage
\section{Domains:}

\subsection{Chants}

Our work in developing tools to assist with chant research is a
collaboration with Dr. Daniel Biro, a professor in the School of Music
at the University of Victoria. He has been collecting and studying
recordings of chant with specific focus on how music transmission
based on oral transmission and ritual was gradually changed to one
based on writing and music notation. The examples studied come from
improvised, partially notated, and gesture-based \cite{krumhansl90} notational chant
traditions: Hungarian siratok (laments)  \footnote{Archived Examples from Hungarian Academy of Science 
(1968-1973)}, Torah cantillation  \cite{zimmerman00} \footnote{Archived Examples from Hungary and Morocco from the Feher Music Center at the Bet Hatfatsut, Tel Aviv, Israel}, tenth
century St. Gallen plainchant \cite{treitler82} \footnote{Godehard Joppich and Singphoniker: Gregorian Chant from St. Gallen
(Gorgmarienh√ºtte: CPO 999267-2, 1994)}, and Koran recitation \footnote{Examples from Indonesia and Egypt: in Approaching the Koran (Ashland: White Cloud, 1999)}. 


Although Dr. Biro has been studying these recordings for some time and
has considerable computer expertise for a professor in music, the
design and development of our tools has been challenging. This is partly 
due to difficulties in communication and terminology as well as 
the fact that the work is exploratory in nature and there are no
easily defined objectives. The tool has been developed through 
extensive interactions with Dr. Biro with frequent frustration on both
sides. At the same time, a wonderful thing about expert users like Dr. Biro 
is that they are willing to spend considerable time preparing and
annotating data as well as testing the system and user interface which
is not the case in more traditional broad application domains.  


\subsection{Orca vocalizations} 

The goal of the Orchive project is to digitize acoustic data that have
been collected over a period of 36 years using a variety of analog
media at the research station OrcaLab (http://www.orcalab.org) on
Hanson Island on the west coast of Vancouver Island in
Canada. Currently we have approximately 20000 hours of analog
recordings, mostly in high quality audio cassettes. In addition to the
digitization effort which is underway, we are developing algorithms and
software tools to facilitate access and retrieval for this large audio
collection.  The size of this collection makes access and retrieval
especially challenging (for example it would take approximately 2.2
years of continuous listening to cover the entire archive).  Therefore
the developed algorithms and tools are essential for effective long
term studies employing acoustic techniques. Currently such studies
require enormous effort as the relevant acoustic tapes need to be
recovered and the relevant segments need to be tediously digitized
for analysis.

The majority of the audio recordings consist of three broad classes of
audio signals: background noise caused mainly by the hydrophones,
boats, background noise containing orca vocalizations and voice
over sections where the observer that started the recording is talking
about the details of the particular recording. In some cases there is
also significant overlap between multiple orca vocalizations. The orca
vocalizations frequently can be categorized into discrete calls that
allow expert researchers to identify their social group (matriline and
pod) and in some cases even allow identification of individuals.

Even when the data is digitized, locating a particular segment of
interest in a long monolithic audio recording can be very tedious as
users have to listen to many irrelevant sections until they can
locate what they are looking for. Even though visualizations such as
spectrograms can provide some assistance this is still a task that requires much
manual effort. In this paper we describe experiments for the automatic
classification and segmentation of the orca recordings for the
purposes of locating segments of interest and facilitating interaction
with this large audio archive.


\section{Analysis and Browsing}

\subsection{Melodic Contour Analysis} 

Our tool takes in a (digitized) monophonic or heterophonic recording
and produces a series of succesively more refined and abstract
representations of the segments it contains as well as the
corresponding melodic contours . More specifically the following
analysis stages are performed:

\begin{itemize} 
\item{Hand Labeling of Audio Segments}
\item{First Order Markov Model of Sign Sequences}
\item{F0 Estimation}
\item{F0 Pruning}
\item{Scale Derivation: Kernel Density Estimation}
\item{Quantization in Pitch}
\item{Scale-Degree Histogram}
\item{Histogram-Based Contour Abstraction}
\item{Plotting and Recombining the Segments}
\end{itemize} 


The recordings are manually segmented and annotated by the expert. 
Even though we considered the possibility of creating an automatic
segmentation tool, it was decided that the task was too subjective and 
critical to automate. Each segment is annotated with a word/symbol
that is related to the corresponding text or performance symbols used 
during the recitation. 

In order to study the transitions between signs/symbols we calculate a
first order Markov model of the sign sequence for each recording.  We
were asked to perform this type of syntagmatic analysis by
Dr. Biro. Although it is completely straightforward to perform
automatically using the annotation, it would be hard, if not impossible,
to calculate manually. Figure ~\ref{fig:transitions} shows an example
transition matrix. For a given trope sign (a row), how many total
times does it appear in the example (numeral after row label), and in
what fraction of those appearances is it followed by each of the other
trope signs?  The darkness of each cell corresponds to the fraction of
times that the trope sign in the given row is followed by the trope
sign in the given column.  (NB: Cell shading is relative to the total
number of occurrences of the trope sign in the row, so, e.g., the
black square saying that ``darga'' always precedes ``revia''
represents 1/1, while the black square saying that ``zakef'' always
precedes ``katon'' represents 9/9.)

\begin{figure}[htb]
\includegraphics[width=80mm]{transition_matrix}
\label{fig:transitions} 
\caption{
Syntagmatic analysis with a first-order Markov model of the
sequence of Torah trope signs for the text Shir Ha Shirim (``Song of
Songs'').} 
\end{figure} 


After the segments have been identified, the fundamental frequency
(``F0'' in this case equivalent to pitch) and signal energy (related to
loudness) are calculated for each segment as functions of time. We use
the SWIPEP fundamental frequency estimator \cite{camachophd} with all
default parameters except for upper and lower frequency bounds that are
hand-tuned for each example. For signal energy we simply take the sum
of squares of signal values in each non-overlapping 10-ms rectangular
window.


\begin{figure}[htb]
\includegraphics[width=80mm]{f0contour}
\label{fig:contour}
\caption{F0 contour} 
\end{figure} 

The next step is to identify pauses between phrases, so as to
eliminate the meaningless and wildly varying F0 estimates during these
noisy regions. We define an energy threshold, generally 40 decibels
below each recording's maximum. If the signal energy stays below this
threshold for at least 100 ms then the quiet region is treated as
silence and its F0 estimates are ignored. Figure~\ref{fig:contour}
shows an excerpt of the F0 and energy curves for an excerpt from the
Koran sura (``section'') Al-Qadr (``destiny'') recited by the renowned
Sheikh Mahmud Khalil al-Husari from Egypt.

Following the pitch contour extraction is pitch quantization, which is
the discretization of the continuous pitch contour into discrete notes
of a scale. Rather than externally imposing a particular set of
pitches, such as an equal-tempered chromatic (the piano keys) or
diatonic scale, we have developed a novel method for extracting a
scale from an F0 envelope that is continuous (or at least very densely
sampled) in both time and pitch. Our method is inspired by Krumhansl's
time-on-pitch histograms adding up the total amount of time spent on
each pitch \cite{krumhansl90}. We demand a pitch resolution of one
cent \footnote{One cent is 1/100 of a semitone, corresponding to a
  frequency difference of about 0.06\%}, so we cannot use a simple
histogram. Instead we use a statistical technique known as
nonparametric kernel density estimation, with a Gaussian kernel
\footnote{Thinking statistically, our scale is related to a
distribution given the relative probability of each possible
pitch. We can think of each F0 estimate (i.e each sampled value of
the F0 envelope) as a sample drawn from this unknown distribution so
our problem becomes one of estimation the unknown distribution given
the samples}. More specifically a Gaussian (with standard deviation
of 33 cents) is centered on each sample of the frequency estimate and
the Gaussians of all the samples are added to form the kernel density
estimate. The resulting curve is our density estimate; like a
histogram, it can be interpreted as the relative probability of each
pitch appearing at any given point in time. Figure 2 shows this
method's density estimate given the F0 curve from Figure 1.


We interpret each peak in the density estimate as a note of the
scale. We restrict the minimum interval between scale pitches
(currently 80 cents by default) by choosing only the higher peak when
there are two or more very close peaks. This method's free parameter
is the standard deviation of the Gaussian kernel, which provides an
adjustable level of smoothness to our density estimate; we have
obtained good results with a standard deviation of 30 cents. Note that
this method has no knowledge of octaves.

\begin{figure} 
\includegraphics[width=80mm]{scalehistogram}
\label{fig:scale}
\caption{Recording-specific scale derivation} 
\end{figure} 

Once we have determined the scale, pitch quantization is the trivial
task of converting each F0 estimate to the nearest note of the scale.
In our opinion these derived scales are more true to the actual nature
of pitch-contour relationships within oral/aural and semi-notated
musical traditions. Instead of viewing these pitches to be deviations
of pre-existing ``normalized'' scales our method defines a more
differentiated scale from the outset. With our approach the scale
tones do not require ``normalization'' and thereby exist in an
autonomous microtonal environment defined solely on statistical
occurrence of pitch within a temporal unfolding of the given melodic
context.

Once the pitch contour is quantized into the recording-specific scale
calculated using Kernel density estimation, we can calculate how many 
times a particular scale degree appears during an excerpt. The
resulting data is a scale-degree histogram which is used create
simplify abstract visual representations of the melodic contours. 



\begin{figure}[htb]
\includegraphics[width=80mm]{cantillion-30pashta-histogramlevels}
\label{fig:contours_histogram} 
\caption{Melodic contours at different levels of abstraction (top:
  original, middle: quantized, bottom: simplified using 3 most
  prominent scale degrees}
\end{figure} 

The basic idea is to only use the most salient discrete scale degrees
(the histogram bins with the highest magnitude) as significant points
to simplify the representation of the contour. By adjusting the number
of prominent scale degrees used to represent the simplified
representation the researchers can view/listen to the melodic contour
at different levels of abstraction and detail. Figure
~\ref{fig:contours_histogram} shows an original continuous contour, 
the quantized representation using the recording-specific derived
scale and the abstracted representation using only the 3 most prominent 
scale degrees. 






\subsection{Cantillion interface} 

We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways
(\url{http://cantillation.sness.net}). Each recording is manually
segmented into the appropriate units for each chant type (such as
trope sign, neumes, semantic units, or words). The pitch contours of
these segments can be viewed at different levels of detail and
smoothness using a histogram-based method. The segments can also be
rearranged in a variety of ways both manually and automatically. That
way one can compare the beginning and ending pitches of any trope
sign, neume or word.

\begin{figure*}[ht]
\includegraphics[width=160mm]{cantillioninterface}
\label{fig:cantillion} 
\caption{
Web-based \emph{Flash} interface to allow users to listen to audio, and to
enable interactive querying of gesture contour diagrams.}
\end{figure*} 


The interface ~\ref{fig:cantillion} has four main sections: a sound
player, a main window to display the pitch contours, a control window,
and a histogram window.  The sound player window displays a
spectrogram representation of the sound file with shuttle controls to
let the user choose the current playback position in the sound
file. The main window shows all the pitch contours for the song as
icons that can be repositioned automatically based on a variety of
sorting criteria, or alternatively can be manually positioned by the
user. The name of each segment (from the initial segmentation step)
appears above its F0 contour. The shuttle control of the main sound
player is linked to the shuttle controls in each of these icons,
allowing the user to set the current playback state either by clicking
on the sound player window, or directly in the icon of interest. 

When an icon in the main F0 display window is clicked, the histogram
window shows a histogram of the distribution of quantized pitches in
the selected sign. Below this histogram is a slider to choose how many
of the largest histogram bins will be used to generate a simplified
contour representation of the F0 curve. In the limiting case of
selecting all histogram bins, the reduced curve is exactly the
quantized F0 curve. At lower values, only the histogram bins with the
most items are used to draw the reduced curve, which has the effect of
reducing the impact of outlier values and providing a smoother
``abstract'' contour.  Shift-clicking selects multiple signs; in this
case the histogram window includes the data from all the selected
signs. We often select all segments with the same word, trope sign, or
neume; this causes the simplified contour representation to be
calculated using the sum of all the pitches found in that particular
sign, enhancing the quality of the simplified contour representation.
Figure ~\ref{fig:cantillion} shows a screenshot of the browsing
interface. 




The identity of chant formulae in oral/aural chant traditions is to a
large extent determined by gesture/contour rather than by discrete
pitches. Computational approaches assist with the analysis of these
gestures/contours and enables the juxtaposition of multiple views at
different levels of detail in a variety of analytical (paradigmatic
and syntagmatic) contexts.  The possibilities for such complex
analysis methods would be difficult if not impossible without such
computer-assisted analysis. Employing these tools we hope to better
understand the role of and interchange between melodic formulae in
oral/aural and written chant cultures. While our present analysis
investigates melodic formulae primarily in terms of their gestural
content and semantic functionality, we hope that these methods might
allow scholars to reach a better understanding of the historical
development of melodic formulae within various chant traditions.




\subsection{Orchive}

The \emph{Orchive} is a web-based collaborative system designed to assist
with the task of identifying and annotating sections of audio
recordings that contain orca vocalizations.  This system consists of a
dynamic front end written in \emph{XHTML/CSS} and \emph{Flash}. The interface
allows the user to annotate regions of the recording as  ``orca'' and ``voiceover'', 
and automatically assigns the ``background'' label to
unlabeled regions of the audio.  In voiceover sections the observer
that started the tape recording talks about the details of the
particular recording such as the geographic location of the Orcas, the
time of the day, the weather conditions and other items of note. A sample section of audio
with voiceover, orca vocalizations and background is shown in Figure
~\ref{fig:orcascreenshot}. Although we eventually want to provide more
detailed classification, such as the type of orca calls, in practical
terms this basic classification to three categories is very
important to the researchers involved.

\begin{figure*}[htb]
\includegraphics[width=160mm]{orca-waveform-spectrogram-screenshot}
\label{fig:orcascreenshot} 
\caption{
An annotated region of audio from the \emph{Orchive}, with regions of voice
and orca vocalization shown.  Unlabeled regions are automatically
assigned a label of background noise.}
\end{figure*} 


This front end then communicates with a \emph{Ruby on Rails} \cite{thomas06}
based web server which stores the data in a \emph{MySQL} database.  This web
server then runs audio feature extraction and performs supervised and
semi-supervised learning using the \emph{Marsyas} \cite{tzanetakis00}
(\url{http://marsyas.sness.net})  open
source software framework for audio analysis. 

OrcaAnnotator is a Model-View-Controller system containing
well-defined and well-separated sections, each of which presents a
uniform interface to the other sections of the system.  Each part is
made to be a simple and well-defined unit, making them easier to
test and maintain.

The primary mode of communication with the user is via an
\emph{XHTML/CSS} and \emph{Flash} based interface. The user is
presented with a simple and attractive \emph{XHTML/CSS} web page that
has been designed to be standards compliant which will facilitate
accessibility by the research community on a wide variety of different
web browsers and computer platforms.  The \emph{Flash} based interface
is written in the haXe \cite{mccoll08} programming language, which
compiles the ECMAScript language \emph{haXe} down to \emph{Flash}
bytecodes. The \emph{Flash} interface presents a simple interface to
the user with a spectrogram of the sound file, shuttle and volume
controls, a time display, and an interface for labeling the audio
file.  We used the labeling functionality in \emph{Audacity} as a
model for our user-interaction paradigm.  To add a label, the user
simply clicks and drags the mouse on the label region.  This creates a
label with left and right extents, and a text region where the user
can enter a text description of the audio. In addition, a pull-down menu 
with labels can be used for quick annotation. 


Labels are saved to the database with the user that created them and
the time that they were created.  This user can be an actual user on
the system, or can be labeled with \emph{Marsyas} and the name and
parameters of the classifier that was used for
labeling. \emph{Marsyas} contains a number of machine-learning
classifiers, including Gaussian (MAP), Gaussian Mixture-Model (GMM),
and Support Vector Machines (SVM).  We used the ``bextract'' program
which is part of \emph{Marsyas}, which now includes a new Timeline
module that allows the import of human-annotated sections of audio
into \emph{Marsyas} as a start for a bootstrapping approach. A variety
of standard audio feature extraction algorithms such as Mel-Frequency
Cepstral Coefficients (MFCC) as well as various types of spectral
features are also provided. The integration of machine learning and
audio signal processing is essential in creating a semi-automatic
annotation interface.

To provide communication between the \emph{Flash} user-interface and the
\emph{Marsyas} classifier algorithms, we have employed the \emph{Ruby on Rails} web
framework\cite{thomas06}.  \emph{Ruby on Rails} allows for quick and easy development and
deployment of websites, and it provides a tight
interface layer to an underlying database like \emph{MySQL}.

\emph{Ruby on Rails} also has the advantage that it makes it simple to build
REST based applications\cite{fieldingphd}.  REST is the model on which
the internet is built and has the ability to minimize latency and network communication,
while simultaneously maximizing the independence and scalability of network
services. \emph{Ruby on Rails} queries the database for user data, label
data and locations of audio files.  It then generates all the \emph{XHTML/CSS}
files displayed to the user and sends the required XML data to the
\emph{Flash} application.  Once the user submits their annotated data back to
the web server, it first stores this data in the database and then
queues this data for \emph{Marsyas} to run in a separate background process,
perhaps on another machine, or network of machines.

The queuing communication is handled by \emph{Amazon SQS} which is a
reliable and scalable queue based system that stores messages for
futher processing.  The advantage of using \emph{Amazon SQS} over other home
grown queuing systems is it's high reliability and availability.
Once \emph{Marsyas} completes processing the audio, the results are
automatically sent back to the web server using REST web services.

Being able to segment and label the audio recordings into the three main
categories (voiceover, orca vocalizations and background noise) is
immensly useful to researchers working with this vast amount of
data. For example background noise comprises approximately $64\% $ of
the recordings, and is much higher in some individual recordings.  Fully annotating
the data even using a well-designed user interface is out of the
question given the size of the archive. To address this problem we
have designed a semi-supervised learning system that only requires
manual annotation of a small percentage of the data and utilizes
machine learning techniques to annotate the other part. This
recording-specific annotation bootstrapping can potentially be used
with other types of time-based multimedia data.

\newpage
\subsection{Annotation Bootstrapping} 

Annotation bootstrapping is inspired by semi-supervised learning
\cite{chapelle2006}.  It has been shown that unlabeled data, when used
in conjuction with a small amount of labeled data, can produce
considerable improvements in learning accuracy. The acquisition of
labeled data for a learning problem often requires manual annotation
which is a time consuming process so semi-supervised learning can
significantly reduce annotation time for large multimedia archives. 

We extend the idea of semi-supervised learning to take advantage 
of the strong correlation between feature vectors from the same 
audio recording.  In the \emph{Orchive} each audio recording has a duration 
of 45 minutes and corresponds to a particular date and time. 
There is considerable consistency within a recording as the same 
person is doing the voiceover sections, the mixing settings are the
same and the orcas that are vocalizing typically come from the same group. 
A recording-specific bootstrap classifier is trained as follows: 
a small percentage of the specific audio recording is manually
annotated and used to train a recording-specific classifier. 
This classifier is then used to label the remaining parts of the
recording.  Due to the consistency of the recording this classifier will 
be to some extent overfitted to the recording and will not generalize
well to other recordings.  However, that is not a problem in our case as we are
mainly interested in obtained labels for the entire recording. This
process is repeated for each recording.  Once all the recordings have 
been semi-automatically fully labeled then feature extraction is
performed for the entire archive and a generalizing classifier is
trained using the full dataset. 

\begin{table}
\centering
\caption{Recording-specific classification performance}
\begin{tabular}{|c|c|c|c|c|} \hline

&\multicolumn{2}{|c|}{Naive bayes}&\multicolumn{2}{|c|}{SMO}\\
&\multicolumn{2}{|c|}{\% correct}&\multicolumn{2}{|c|}{\% correct}\\ \hline
&self&train with&self&train with\\
&&remaining&&remaining\\ \hline

446A  &  89.42  &  93.10  &  95.00  &  73.39\\ \hline
446B  &  63.45  &  77.66  &  85.85  &  70.23\\ \hline
447B  &  75.46  &  57.32  &  82.02  &  68.17\\ \hline
448A  &  52.18  &  61.02  &  81.57  &  62.24\\ \hline
448B  &  84.63  &  67.62  &  83.64  &  67.87\\ \hline
449B  &  82.24  &  51.85  &  86.41  &  75.72\\ \hline
450A  &  94.66  &  90.91  &  96.12  &  91.58\\ \hline
450B  &  83.65  &  96.27  &  99.29  &  94.92\\ \hline
451A  &  70.92  &  89.58  &  97.04  &  78.72\\ \hline
451B  &  74.18  &  33.73  &  82.34  &  50.88\\ \hline
\end{tabular}
\label{table:classification}
\end{table}



In order to explore whether this idea would work for our data, we 
created a representative database consisting of 10 excerpts from 
our recordings with each excerpt lasting between 5 and 10 minutes. 
Table ~\ref{table:classification} shows classification results using 
10-fold cross-validation for each particular recording using a
recording specific classifier as well as using a classifier trained 
on the entire dataset. Two classifiers are used: a simple Naive Bayes
classifier (NBS), as well as a Support Vector Machine (SVM). The results shown 
are based on the use of the standard Mel-Frequency Cepstral
Coefficients (MFCC) as audio features. The ``self'' column shows the 
classification accuracy results of using a recording-specific
classifier, whereas the ``remaining'' columns 
shows the classification accuracy results using the remaining nine
recordings. As can be seen, recording-specific classifier can generate
significantly better results than generalized classifiers, which is not 
surprising as they adapt to the specific data of the recording. This
justifies the use of their annotation results to labeled the unlabeled 
parts of the audio recording. 



\begin{table}
\centering
\caption{Classification performance using annotation-bootstrapping
  (SVM classifier)}
\begin{tabular}{|c|c|c|} \hline

\% data &\%correct&F-measure\\
used    & &\\ \hline

100  &  82.38  &  0.876\\ \hline
10   &  81.98  &  0.874\\ \hline
5    &  82.04  &  0.874\\ \hline
1    &  79.95  &  0.864\\ \hline
0.1  &  78.08  &  0.857\\ \hline
0.01 &  71.42  &  0.800\\ \hline
\end{tabular}
\label{table:annotation_classification} 
\end{table}



The goal of annotation bootstrapping is to only label a small part of
each recording to train a recording-specific classifier which is then
used to annotate the remainder of the recording. Table
~\ref{table:annotation_classification} shows the results in terms of
classification accuracy and F-measure over the entire dataset for
different amounts of labeled data. As one can see the classification
accuracy remains quite good, even when only a small percentage of the
data is labeled and annotation bootstrapping is used to label the
rest. The first row shows the classification accuracy when all the
data is used for training.

\begin{figure}[htb]
\includegraphics[width=80mm]{smodata}
\label{fig:graph} 
\caption{Graph of classification accuracy as percentage of labeling required.  
Data shown is for the performance of SMO classifier for different percentages of
data used to train the classifier.}
\end{figure} 



Figure ~\ref{fig:graph} shows graphically how the classification
accuracy increases with the amount of labeled data used for
training. In both the table and the figure, the classifier used is a
Support Vector Machine (SVM) and for evaluation a variation of 10-fold
cross-validation where each of the 10 recordings is held out for
testing, the remaining ones are used for training, and the process is
iterated 10 times. We also experimented with different choices of
window size for the feature calculation as well as different audio
feature parametrization but there was no significant difference in the
obtained results. 

To make the importance of annotation bootstrapping concrete, fully
annotating the archive would take approximately 2 and half years
(assuming 24/7 manual annotation) whereas using one percent annotation
bootstrapping would take 3 months (assuming 24/7 manual annotation)
without significantly affecting the ability of the system to
succesfully label all the recordings in the 3 classes of interest.




\section{Conclusions}

By combining the expert knowledge of our scientific collaborators with
new multimedia web-based tools in an agile development strategy, we
have been able to ask new questions that had previously been out of
reach.  The large and multi-dimensional datasets in both the chant
community and in orca vocalization research provide challenging fields for
study, and new web-based technologies provide the flexibility to allow
true collaboration between scientific partners in widely disparate
fields of study.  We described an automatic technique for simplifying
melodic contours based on kernel density estimation.  Annotation of the
archive of orca vocalizations is very time-consuming, we proposed
annotation bootstrapping and show that it is an effective technique 
for automatically annotating recordings. 


\section{Acknowledgments}

We would like to thank Paul Spong and Helena Symonds of Orcalab 
and well as Daniel Biro for providing the data and inspiration for
this project. We would also like to thank the National Sciences
and Engineering Research Council (NSERC) and Social
Sciences and Humanities Research Council (SSHRC) of
Canada for their financial support. 



%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{oa}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%% %APPENDICES are optional
%% %\balancecolumns
%% \appendix
%% %Appendix A
%% \section{Headings in Appendices}
%% The rules about hierarchical headings discussed above for
%% the body of the article are different in the appendices.
%% In the \textbf{appendix} environment, the command
%% \textbf{section} is used to
%% indicate the start of each Appendix, with alphabetic order
%% designation (i.e. the first is A, the second B, etc.) and
%% a title (if you include one).  So, if you need
%% hierarchical structure
%% \textit{within} an Appendix, start with \textbf{subsection} as the
%% highest level. Here is an outline of the body of this
%% document in Appendix-appropriate form:
%% \subsection{Introduction}
%% \subsection{The Body of the Paper}
%% \subsubsection{Type Changes and  Special Characters}
%% \subsubsection{Math Equations}
%% \paragraph{Inline (In-text) Equations}
%% \paragraph{Display Equations}
%% \subsubsection{Citations}
%% \subsubsection{Tables}
%% \subsubsection{Figures}
%% \subsubsection{Theorem-like Constructs}
%% \subsubsection*{A Caveat for the \TeX\ Expert}
%% \subsection{Conclusions}
%% \subsection{Acknowledgments}
%% \subsection{Additional Authors}
%% This section is inserted by \LaTeX; you do not insert it.
%% You just add the names and information in the
%% \texttt{{\char'134}additionalauthors} command at the start
%% of the document.
%% \subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.
%% % This next section command marks the start of
%% % Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.
%% %\balancecolumns % GM June 2007
%% % That's all folks!
\end{document}
% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage{url} 
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{MM'09,} {October 19--24, 2009, Beijing, China.} 
\CopyrightYear{2009}
\crdata{978-1-60558-608-3/09/10} 
%\CopyrightYear{2009} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Improving Automatic Music Tag Annotation Using Stacked
  Generalization Of Probabilistic SVM Outputs}

%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
S. R. Ness, A. Theocharis, \\ G. Tzanetakis\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{PO Box 3055, STN CSC}\\
       \affaddr{Victoria, BC, CANADA}\\
       \email{sness@sness.net}
\alignauthor
L. G. Martins\\
       \affaddr{Portuguese Catholic University}\\
       \affaddr{School of Arts / Research Center for Science and Technology in the Arts}\\
       \affaddr{Rua Diogo Botelho, no 1327}\\
       \affaddr{Porto, Portugal}\\
       \email{lmartins@porto.ucp.pt}
}
%% \alignauthor
%% Steven R. Ness \\
%%        \affaddr{Department of Computer Science}\\
%%        \affaddr{University of Victoria}\\
%%        \affaddr{PO Box 3055, STN CSC}\\
%%        \affaddr{Victoria, BC, CANADA}\\
%%        \email{sness@sness.net}
%% % 2nd. author
%% \alignauthor
%% Anthony Theocharis \\
%%        \affaddr{School of Music}\\
%%        \affaddr{University of Victoria}\\
%%        \affaddr{PO Box 3055, STN CSC}\\
%%        \affaddr{Victoria, BC, CANADA}\\
%%        \email{anthonyt@uvic.ca}
%% % 3rd. author
%% \and
%% \alignauthor
%% L. Gustavo Martins\\
%%        \affaddr{Portuguese Catholic University}\\
%%        \affaddr{School of Arts / Research Center for Science and Technology in the Arts}\\
%%        \affaddr{Rua Diogo Botelho, no 1327}\\
%%        \affaddr{Porto, Portugal}\\
%%        \email{lmartins@porto.ucp.pt}
%% % 4th. author
%% \alignauthor
%% George Tzanetakis \\
%%        \affaddr{Department of Computer Science}\\
%%        \affaddr{University of Victoria}\\
%%        \affaddr{PO Box 3055, STN CSC}\\
%%        \affaddr{Victoria, BC, CANADA}\\
%%        \email{gtzan@cs.uvic.ca}
%}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{27 Apr 2009}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
  Music listeners frequently use words to describe music. Personalized
  music recommendation systems such as Last.fm and Pandora rely on
  manual annotations (tags) as a mechanism for querying and navigating
  large music collections. A well-known issue in such recommendation
  systems is known as the cold-start problem: it is not possible to
  recommend new songs/tracks until those songs/tracks have been
  manually annotated. Automatic tag annotation based on content
  analysis is a potential solution to this problem and has recently
  been gaining attention. We describe how stacked generalization can
  be used to improve the performance of a state-of-the-art automatic
  tag annotation system for music based on audio content analysis and
  report results on two publicly available datasets.


\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.1}{Information Storage and Retrieval}{Content Analysis
  and Indexing Methods}

\terms{Algorithms, Theory, Experimentation}

\keywords{sound analysis, music information retrieval, tags,
  folksonomies, music recommendation}

\section{Introduction}

Music information retrieval (MIR) is a research area that has been
rapidly gaining momentum due to the widespread digital distribution of
music. A central goal of MIR is to create systems that can efficiently
and effectively retrieve songs from large databases of music content.
There are various approaches to specifying queries. We focus on the
approach used by personalized music recommendation systems such as
Last.fm and Pandora, which is, essentially, to represent each track
as a collection of manually annotated words (tags). Social tags are a
key part of ``Web 2.0'' technologies and have become an important
aspect of recommendation systems. Any semantically meaningful word can
be used for this purpose---tags for music can represent a
variety of different concepts including genre, instrumentation,
emotions, geographic origins, social conditions etc. Games with a
purpose are an exciting new way of collecting tags for a variety
of multimedia annotation tasks \cite{1642623} by harnessing volunteer users who
perform the annotation as part of casual gaming.  

A well-known issue in tag-based recommendation systems is known as the
cold-start problem \cite{schein02}: it is not possible to recommend
new songs/tracks until those songs/tracks have acquired enough manual
annotations.  Recently, games-with-a-purpose have been shown to be an
effective way of acquiring reliable tags for large number of
multimedia items.  Automatic tag annotation based on content analysis
can be used to complement manual tag annotation. 

In this paper we focus on automatic tag annotation of music tracks in
which the music retrieval system learns a relationship between
acoustic features and words from a dataset of annotated audio
tracks. The resulting trained model can retrieve audio tracks based on
lists of tags and can annotate unlabelled audio tracks with tags.
Such systems can be used to both annotate novel audio content as well
as retrieve relevant audio tracks from a database of unannotated
tracks given a text-based query \cite{turnbull2008}. Similar
approaches have been explored in the context of automatic image
annotation \cite{tsai08}.

Automatic audio annotation can be formulated as a multi-label
classification problem. We describe a state-of-the-art automatic tag
annotation system for music that utilizes audio feature extraction,
the output of which is used to train a Support Vector Machine (SVM)
with probabilistic class outputs, where each class corresponds to a
tag. Stacked generalizaton is used in order to train a second level
SVM classifier that exploits possible correlations between tags. We
show that this significantly improves annotation performance on two
publicly available music datasets with verified human
annotations. There is no consistent terminology for stacked
generalization and several other terms for similar approaches have
been used in the literature of automatic content-based multimedia
annotation and classification. In the following section we attempt to
collect these different variations using common terminology and
describe the differences and similarities between them and to the
proposed approach.




\section{Related Work}

There is a large body of work in automatic image annotation
\cite{tsai08}. Early work in audio annotation for music used
web-documents associated with an artist for the text annotations
\cite{whitman02}. There are several different approaches to collecting
tags for music, each with advantages and disadvantages
\cite{turnbull08}. For example the Magnatagatune dataset used in this paper has
been collected using TagATune \cite{law07}, a game with a purpose. There has been a recent
increase in interest in automatic audio tag annotation for individual
music tracks as evidenced by the corresponding task in the Music
Information Retrieval Evaluation Exchange (MIREX) \cite{downie08}, an
annual event where different MIR algorithms are evaluated on a variety
of tasks. One of the best performing systems used a probabilistic
model with one tag-level distribution over the audio feature space for
each word in the vocabulary \cite{turnbull2008}. The parameters of a
tag-level Gaussian Mixture Model (GMM) are estimated using audio
content from a set of training tracks that are positively associated
with the tag. This system had the best performance in MIREX 2008 and
is used below as a baseline for comparison with our proposed
approach. Like our system, the output for a particular track is a tag
affinity vector that can be thresholded for tag annotation. Support
Vector Machines have been used with song-level features for automatic
tag classification trained at different granularities (track, album,
artist) \cite{mandel-ismir2008}. Unlike our approach, individual SVM
are trained separately for each tag using positive and negative
samples.  Another possibility is to use boosting of classifiers for
automatic generation of social tags for music recommendation
\cite{eck2007}.  A classifier specifically designed for multi-label
classification was used to classify music into emotions
\cite{trohidis08}.  Unlike in our approach, these systems have no second
stage to model relations between tags is employed.


Audio tag annotation can viewed as a problem of multi-label
classification \cite{tsoumakas2007}. Our approach is to use a
distribution classifier (a linear SVM with
probabilistic outputs \cite{platt99}) that can output a distribution of
affinities (or probabilities) for each tag.  This affinity vector can
either be used directly for indexing and retrieval, or thresholded to
obtain a binary vector with predicted tag associations for the
particular track. The resulting affinity vector is fed into a second
stage SVM classifier in order to better capture the relations between
tags. This approach is a specialized case of stacking generalization
\cite{wolpert92}, a method for the combination of multiple
classifiers. Similar ideas have appeared in the literature under other
terms such as anchor-based classification \cite{berenzweig03} and
semantic space retrieval \cite {slaney02mixtures}, but not necessarily
in a multi-label tag annotation context. The general idea is to map
the content-based features to a more semantically meaningful space,
frequently utilizing external information such as web resources. Stacked
generalization has been used for discriminative methods for
multi-label classification in text retrieval \cite{dodbole04} but
using a vector of binary predictions for each label to model
dependencies between them. The most closely relevant work is applied
in improving multi-label analysis of music titles again using a second
stage classifier on the binary predictions of the first stage
classifiers which the authors term the correction approach
\cite{pachet09}. To the best of our knowledge this is the first time
the probabilistic output of SVM classifiers is used for multiple label
classification for automatic audio annotation and possibly more
generally content-based multimedia annotation.

\section{Automatic Tag Annotation} 

Figure~\ref{fig:blockdiagram} shows the flow of information for our
proposed audio annotation system. For each track in the audio
collection a feature vector is calculated based on the audio content.
As each track might be annotated by multiple tags the feature vector
is fed into the multi-class {\bf Audio SVM} several times with
different tags. Once all tracks have been processed, the linear SVM is
trained and a tag affinity output vector {\bf (TAV)} is calculated. The
TAV can be used directly for retrieval and storage or converted to a
tag binary vector {\bf (TBV)} by some thresholding method.  When stacked
generalization is used, the tag affinity vector (TAV) is used as a
semantic feature vector for a second round of training over the tracks
using an affinity SVM which produces a stacked tag affinity vector
{\bf (STAV)} and a stacked tag binary vector {\bf (STBV)}. The resulting predicted
affinity and binary vector can be used to evaluate the effectiveness
of the retrieval system using metrics such as Area under Receiver
Operating Characteristic Curve {\bf (AROC)} for the TAV and 
information retrieval measures for the TBV. 



\subsection{Problem Formulation} 


We begin by considering a vocabulary $V$ that consists of $|W|$
unique words and that each ``word'' refers to a semantic concept, for
example ``techno'', ``rock'', ``hardcore'' or ``ambient''.  The goal
of annotation is the find a set $W = { w_1, \ldots, w_A}$ of $A$ words
that are semantically meaningful and describe a query audio track
$s_q$.  The process of retrieval consists of ordering a set of songs
$S = {s_1, \ldots, s_R}$ when one is given a list of query words
$W_q$.  If we describe each song as an annotation vector
$y=(y_1,\ldots,y_{|V|})$ where $y_i > 0$ if $w_i$ has a
semantic association with the audio track, and $y_i = 0$ if it does
not.  These $y_i$ are proportional to the strength of the semantic
association and are thus called semantic weights.  We then map these
semantic weights to the range ${\{0, 1\}}$ and interpret them as the class
labels.  We can then represent a song $s$ as $X = {x_1, \ldots, x_T}$
of $T$ real-valued feature vectors, with each vector $x_t$
representing audio features that have been extracted from a short
section of the song.  The data set $D$ that we use is a collection of
pairs of tracks and annotations $D
={(X_1,y_1),\ldots,(X_{|D|},y_{|D|})}$.

Automatic audio tag annotation can be viewed as a special case of
multi-label classification. Traditional {\it single-label}
classification is concerned with learning from a set of examples that
are associated with a single label $l$ from a set of disjoint labels
$L$, $|L|>1$. If $|L|=2$, then the learning problem is called {\it
  binary} classification, while if $|L|>2$ then it is called a
multi-class classification problem.  In {\it multi-label}
classification the examples are associated with a set of labels $Y
\subset L$. In addition, and in contrast to other multi-label
classification problems, tags are relatively sparse and therefore 
there is an imbalance between positive and negative examples for each
tag.  


\begin{figure}[htb]
\includegraphics[width=80mm]{blockdiagram}
\label{fig:blockdiagram} 
\caption{System flow diagram}
\end{figure} 


\subsection{Audio Feature Extraction and Stacked Classification } 

Each audio track is represented as a single feature vector. Even
though much more elaborate audio track representations have been
proposed in the literature we like the simplicity of machine learning
and similarity calculation using single feature vectors per audio
clip. It has been shown that such song-level features perform quite
well \cite{mandel-ismir2005}.

The features used are Spectral Centroid, Roll-Off, Flux and
Mel-Frequency Cepstral Coefficients (MFCC). To capture the feature we
compute a running mean and standard deviation over the past $M$ frames:

\begin{eqnarray} 
  m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
  s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)] 
\end{eqnarray} 

where $\Phi(t)$ is the original feature vector. Notice that the
dynamics features are computed at the same rate as the original
feature vector but depend on the past $M$ frames (e.g. M=40,
corresponding approximately to a so-called ``texture window'' of 1
second).  This results in a feature vector of 32 dimensions at the
same rate as the original 16-dimensional one. 
% This process is
% illustrated in Figure ~\ref{fig:featExtract}. 
The sequence of feature vectors is collapsed into a single feature
vector representing the entire audio clip by taking again the mean and
standard deviation across the 30 seconds (the sequence of dynamics
features) resulting in the final 64-dimensional feature vector per
audio clip. A more detailed description of the features can be found
in Tzanetakis and Cook \cite{TC02b}.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{featureExtraction} 
%   \caption{\it Feature extraction and texture window}
%   \label{fig:featExtract}
% \end{figure}



For training the support vector machine classifier the feature vectors
(one per audio track) are normalized so that the minimum of each
feature is 0 and the maximum in 1 (Max/Min Normalization). The {\it
  Marsyas} audio processing framework (\url{http://marsyas.sness.net})
was used for the computation of the features. Both stages of
classification (audio and stacked affinity) utilize a multi-class
Support Vector Machine implemented as a collection of binary
one-versus all discriminative classifiers. The libSVM software package
is used for training and classification \cite{cc01}.


\section{Experiments} 

We tested the system on two publicly available audio data-sets.
The Computer Audition Lab 500 (CAL500) \cite{turnbull2008} dataset
is a selection of 500 Western popular
songs recorded by 500 different artists, from between 1958 and 2008. Each
song is manually annotated with an appropriate subset of 135 total tags
(including positive and negative tags), including 29 instruments, 22
vocal characteristics, 36 genres, 18 emotions, 15 preferred listening
scenarios, and 15 concepts such as tempo and sound quality. Each song
has at least 3 annotations, with a total of 1708 annotations in the collection.
The Magnatagatune \cite{law07} dataset is a collection of 21642 songs and 188 tags.
The songs were provided by Magnatune.com and FreeSound.org, and span
the genres of classical, new age, electronica, rock, pop, world music,
jazz, blues, heavy metal, and punk. Annotations for the files were collected via
the TagATune game-with-a-purpose, in which two players were asked each to annotate
a song. The players were then shown each other's annotations and asked to
guess whether or not they had been listening to the same song.



The results generated by the SVM algorithm are in the form of an
affinity matrix, with one dimension representing all the songs in the
collection, and one dimension representing the affinity of a
particular tag for that song.  This affinity matrix can be compared to
a similarly constructed ground truth matrix via the Receiver Operating
Characteristic (ROC) curve \cite{fawcett06}, which creates
a curve by iteratively changing a cut-off level and plotting the
resulting values. We can then integrate this curve to obtain the Area
under Receiver Operating Characteristic curve (AROC). The values of
AROC vary between 0 and 1, with larger values signifying better
classifier performance.  Precision, recall, accuracy and F-measure are
also calculated in the standard way. We report these measures over
both the entire (global) binary matrix as well as seperately for each
tag and then averaged across tags. The per-tag average accuracy is a
better measure as it is not biased by popular tags.


\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|} \hline
              &     Accuracy  & F-measure &  AROC  &  AROC  \\
              &        Tag    &    Tag    &   Tag  &  Clip  \\\hline
 BTL          &        0.842  &    0.258  &  0.68  &  0.78  \\
 Audio SVM    &        0.865  &    0.394  &  0.78  &  0.86  \\
 Affinity SVM &        0.882  &    0.498  &  0.85  &  0.89  \\ \hline

\end{tabular}
\caption{CAL500 Evaluation Metrics}
\label{table:cal500}
\end{table}

Table ~\ref{table:cal500} shows various evaluation metrics comparing
the performance of the best performing system in MIREX 2008 (LTB)
\cite{turnbull2008}. To calculate these numbers, we obtained from the
authors the predicted affinity matrix associating songs and tags for
CAL500.  The second line of the table shows the performance of the
audio-based SVM system using song-level features. The third line shows
the improvement in performance using the stacked generalization where
the input to the second level classifier is the affinity vector
predicted by the first level classifier. The same thresholding was
applied in all cases. The threshold for each tag was chosen such that
the number of testing songs associated with a given tag is
proportional to the frequency in which that tag was applied to the
training songs. All the results were obtained using 2-fold
cross-validation and were not significantly different than using the
training set for testing. This is expected given the challenging
nature of multiple-label classification which makes over-fitting to the
training data more unlikely.

\begin{table}[t]
\centering
\caption{Magnatagatune : Audio and Affinity SVM - Global evaluation metrics}
\begin{tabular}{|r|r|r|r|r|} \hline
              &  Precision  &  Recall  &  Accuracy  &  F-Score  \\\hline
 Audio SVM    &      0.307  &   0.315  &     0.969  &    0.311  \\
 Affinity SVM &      0.351  &   0.354  &     0.971  &    0.353  \\\hline
\end{tabular}
\label{table:magnatune_global}
\end{table}



\begin{table}[t]
\centering
\begin{tabular}{|r|r|r|r|r|} \hline
 \# Tags     &  Precision  &  Recall  &  Accuracy  &  F-Score  \\\hline
        20  &      0.417  &    0.688  &     0.856  &    0.516  \\
        30  &      0.345  &    0.669  &     0.862  &    0.452  \\
        40  &      0.370  &    0.381  &     0.910  &    0.375  \\
        50  &      0.328  &    0.337  &     0.919  &    0.332  \\
       100  &      0.189  &    0.195  &     0.947  &    0.192  \\
 all (188)  &      0.127  &    0.130  &     0.969  &    0.129  \\\hline
\end{tabular}
\caption{Magnatagatune : Audio SVM - Per-tag evaluation metrics }
\label{table:nonstacked_per_tag}
\end{table}



\begin{table}[t]
\centering
\begin{tabular}{|r|r|r|r|r|} \hline
\# Tags      &  Precision  &   Recall  &  Accuracy  &  F-Score  \\\hline
        20  &      0.418  &    0.691  &     0.856  &    0.518  \\
        30  &      0.346  &    0.671  &     0.862  &    0.453  \\
        40  &      0.394  &    0.397  &     0.914  &    0.395  \\
        50  &      0.369  &    0.372  &     0.923  &    0.371  \\
       100  &      0.259  &    0.262  &     0.951  &    0.260  \\
 all (188)  &      0.184  &    0.186  &     0.971  &    0.185  \\\hline
\end{tabular}
\caption{Magnatagatune : Affinity SVM - Per-tag evaulation metrics}
\label{table:stacked_per_tag}
\end{table}



Table ~\ref{table:magnatune_global} shows the results for both audio
and stacked generalization using probabilistic SVM outputs for the
Magnatagatune dataset and all tags. We believe this is the first time
results are published for this dataset. The global evaluation metrics
are biased towards popular tags, so can be misleading. As this dataset
has many tags, we explore using different number of tags for training
the classifier. For example ``30'' means that the 30 most popular tags
were used to train the
classifier. Tables~\ref{table:nonstacked_per_tag}
and~\ref{table:stacked_per_tag} show similar results by averaging the
evaluation metrics across tags. This way tags that are not popular are
as important as popular tags in terms of being predicted correctly. As
can be seen, stacked generalization of probabilistic SVM outputs
improves all per-tag evaluation metrics especially when all tags are
considered. This is expected, as it can capture tag relations even
among tags that might not be represented enough for accurate
audio-based classification.








\section{Conclusions} 

Stacked generalization of the probabilistic outputs of a Support
Vector Machine classifier can be used to improve the performance of
automatic audio tag annotation. The scheme is straightforward to
implement and provides significant improvements over one stage
classification using a variety of standard evaluation measures in two
publicly available datasets. We believe that a similar approach could
be used for other tasks such as automatic image annotation. 



\bibliographystyle{abbrv}
\bibliography{acmmm2009gtzan}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage{url} 
\pdfpagewidth 8.5in
\pdfpageheight 11in
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{MM'09,} {October 19--24, 2009, Beijing, China.} 
\CopyrightYear{2009}
\crdata{978-1-60558-608-3/09/10} 
%\CopyrightYear{2009} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Improving Automatic Music Tag Annotation Using Stacked
  Generalization Of Probabilistic SVM Outputs}

%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven R. Ness, Anthony Theocharis, \\ George Tzanetakis\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{PO Box 3055, STN CSC}\\
       \affaddr{Victoria, BC, CANADA}\\
       \email{sness@sness.net}
\alignauthor
Luis Gustavo Martins\\
       \affaddr{Portuguese Catholic University}\\
       \affaddr{School of Arts / Research Center for Science and Technology in the Arts}\\
       \affaddr{Rua Diogo Botelho, no 1327}\\
       \affaddr{Porto, Portugal}\\
       \email{lmartins@porto.ucp.pt}
}
%% \alignauthor
%% Steven R. Ness \\
%%        \affaddr{Department of Computer Science}\\
%%        \affaddr{University of Victoria}\\
%%        \affaddr{PO Box 3055, STN CSC}\\
%%        \affaddr{Victoria, BC, CANADA}\\
%%        \email{sness@sness.net}
%% % 2nd. author
%% \alignauthor
%% Anthony Theocharis \\
%%        \affaddr{School of Music}\\
%%        \affaddr{University of Victoria}\\
%%        \affaddr{PO Box 3055, STN CSC}\\
%%        \affaddr{Victoria, BC, CANADA}\\
%%        \email{anthonyt@uvic.ca}
%% % 3rd. author
%% \and
%% \alignauthor
%% L. Gustavo Martins\\
%%        \affaddr{Portuguese Catholic University}\\
%%        \affaddr{School of Arts / Research Center for Science and Technology in the Arts}\\
%%        \affaddr{Rua Diogo Botelho, no 1327}\\
%%        \affaddr{Porto, Portugal}\\
%%        \email{lmartins@porto.ucp.pt}
%% % 4th. author
%% \alignauthor
%% George Tzanetakis \\
%%        \affaddr{Department of Computer Science}\\
%%        \affaddr{University of Victoria}\\
%%        \affaddr{PO Box 3055, STN CSC}\\
%%        \affaddr{Victoria, BC, CANADA}\\
%%        \email{gtzan@cs.uvic.ca}
%}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{27 Apr 2009}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
  Music listeners frequently use words to describe music. Personalized
  music recommendation systems such as Last.fm and Pandora rely on
  manual annotations (tags) as a mechanism for querying and navigating
  large music collections. A well-known issue in such recommendation
  systems is known as the cold-start problem: it is not possible to
  recommend new songs/tracks until those songs/tracks have been
  manually annotated. Automatic tag annotation based on content
  analysis is a potential solution to this problem and has recently
  been gaining attention. We describe how stacked generalization can
  be used to improve the performance of a state-of-the-art automatic
  tag annotation system for music based on audio content analysis and
  report results on two publicly available datasets.


\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.1}{Information Storage and Retrieval}{Content Analysis
  and Indexing Methods}

\terms{Algorithms, Theory, Experimentation}

\keywords{sound analysis, music information retrieval, tags,
  folksonomies, music recommendation}

\section{Introduction}

Music information retrieval (MIR) is a research area that has been
rapidly gaining momentum due to the widespread digital distribution of
music. A central goal of MIR is to create systems that can efficiently
and effectively retrieve songs from large databases of music content.
There are various approaches to specifying queries. We focus on the
approach used by personalized music recommendation systems such as
Last.fm and Pandora, which is, essentially, to represent each track
as a collection of manually annotated words (tags). Social tags are a
key part of ``Web 2.0'' technologies and have become an important
aspect of recommendation systems. Any semantically meaningful word can
be used for this purpose---tags for music can represent a
variety of different concepts including genre, instrumentation,
emotions, geographic origins, social conditions etc. Games with a
purpose are an exciting new way of collecting tags for a variety
of multimedia annotation tasks \cite{1642623} by harnessing volunteer users who
perform the annotation as part of casual gaming.  

A well-known issue in tag-based recommendation systems is known as the
cold-start problem \cite{schein02}: it is not possible to recommend
new songs/tracks until those songs/tracks have acquired enough manual
annotations.  Recently, games-with-a-purpose have been shown to be an
effective way of acquiring reliable tags for large number of
multimedia items.  Automatic tag annotation based on content analysis
can be used to complement manual tag annotation. 

In this paper we focus on automatic tag annotation of music tracks in
which the music retrieval system learns a relationship between
acoustic features and words from a dataset of annotated audio
tracks. The resulting trained model can retrieve audio tracks based on
lists of tags and can annotate unlabelled audio tracks with tags.
Such systems can be used to both annotate novel audio content as well
as retrieve relevant audio tracks from a database of unannotated
tracks given a text-based query \cite{turnbull2008}. Similar
approaches have been explored in the context of automatic image
annotation \cite{tsai08}.

Automatic audio annotation can be formulated as a multi-label
classification problem. We describe a state-of-the-art automatic tag
annotation system for music that utilizes audio feature extraction,
the output of which is used to train a Support Vector Machine (SVM)
with probabilistic class outputs, where each class corresponds to a
tag. Stacked generalizaton is used in order to train a second level
SVM classifier that exploits possible correlations between tags. We
show that this significantly improves annotation performance on two
publicly available music datasets with verified human
annotations. There is no consistent terminology for stacked
generalization and several other terms for similar approaches have
been used in the literature of automatic content-based multimedia
annotation and classification. In the following section we attempt to
collect these different variations using common terminology and
describe the differences and similarities between them and to the
proposed approach.




\section{Related Work}

There is a large body of work in automatic image annotation
\cite{tsai08}. Early work in audio annotation for music used
web-documents associated with an artist for the text annotations
\cite{whitman02}. There are several different approaches to collecting
tags for music, each with advantages and disadvantages
\cite{turnbull08}. For example the Magnatagatune dataset used in this paper has
been collected using TagATune \cite{law07}, a game with a purpose. There has been a recent
increase in interest in automatic audio tag annotation for individual
music tracks as evidenced by the corresponding task in the Music
Information Retrieval Evaluation Exchange (MIREX) \cite{downie08}, an
annual event where different MIR algorithms are evaluated on a variety
of tasks. One of the best performing systems used a probabilistic
model with one tag-level distribution over the audio feature space for
each word in the vocabulary \cite{turnbull2008}. The parameters of a
tag-level Gaussian Mixture Model (GMM) are estimated using audio
content from a set of training tracks that are positively associated
with the tag. This system had the best performance in MIREX 2008 and
is used below as a baseline for comparison with our proposed
approach. Like our system, the output for a particular track is a tag
affinity vector that can be thresholded for tag annotation. Support
Vector Machines have been used with song-level features for automatic
tag classification trained at different granularities (track, album,
artist) \cite{mandel-ismir2008}. Unlike our approach, individual SVM
are trained separately for each tag using positive and negative
samples.  Another possibility is to use boosting of classifiers for
automatic generation of social tags for music recommendation
\cite{eck2007}.  A classifier specifically designed for multi-label
classification was used to classify music into emotions
\cite{trohidis08}.  Unlike in our approach, these systems have no second
stage to model relations between tags is employed.


Audio tag annotation can viewed as a problem of multi-label
classification \cite{tsoumakas2007}. Our approach is to use a
distribution classifier (a linear SVM with
probabilistic outputs \cite{platt99}) that can output a distribution of
affinities (or probabilities) for each tag.  This affinity vector can
either be used directly for indexing and retrieval, or thresholded to
obtain a binary vector with predicted tag associations for the
particular track. The resulting affinity vector is fed into a second
stage SVM classifier in order to better capture the relations between
tags. This approach is a specialized case of stacking generalization
\cite{wolpert92}, a method for the combination of multiple
classifiers. Similar ideas have appeared in the literature under other
terms such as anchor-based classification \cite{berenzweig03} and
semantic space retrieval \cite {slaney02mixtures}, but not necessarily
in a multi-label tag annotation context. The general idea is to map
the content-based features to a more semantically meaningful space,
frequently utilizing external information such as web resources. Stacked
generalization has been used for discriminative methods for
multi-label classification in text retrieval \cite{dodbole04} but
using a vector of binary predictions for each label to model
dependencies between them. The most closely relevant work is applied
in improving multi-label analysis of music titles again using a second
stage classifier on the binary predictions of the first stage
classifiers which the authors term the correction approach
\cite{pachet09}. To the best of our knowledge this is the first time
the probabilistic output of SVM classifiers is used for multiple label
classification for automatic audio annotation and possibly more
generally content-based multimedia annotation.

\section{Automatic Tag Annotation} 

Figure~\ref{fig:blockdiagram} shows the flow of information for our
proposed audio annotation system. For each track in the audio
collection a feature vector is calculated based on the audio content.
As each track might be annotated by multiple tags the feature vector
is fed into the multi-class {\bf Audio SVM} several times with
different tags. Once all tracks have been processed, the linear SVM is
trained and a tag affinity output vector {\bf (TAV)} is calculated. The
TAV can be used directly for retrieval and storage or converted to a
tag binary vector {\bf (TBV)} by some thresholding method.  When stacked
generalization is used, the tag affinity vector (TAV) is used as a
semantic feature vector for a second round of training over the tracks
using an affinity SVM which produces a stacked tag affinity vector
{\bf (STAV)} and a stacked tag binary vector {\bf (STBV)}. The resulting predicted
affinity and binary vector can be used to evaluate the effectiveness
of the retrieval system using metrics such as Area under Receiver
Operating Characteristic Curve {\bf (AROC)} for the TAV and 
information retrieval measures for the TBV. 



\subsection{Problem Formulation} 


We begin by considering a vocabulary $V$ that consists of $|W|$
unique words and that each ``word'' refers to a semantic concept, for
example ``techno'', ``rock'', ``hardcore'' or ``ambient''.  The goal
of annotation is the find a set $W = { w_1, \ldots, w_A}$ of $A$ words
that are semantically meaningful and describe a query audio track
$s_q$.  The process of retrieval consists of ordering a set of songs
$S = {s_1, \ldots, s_R}$ when one is given a list of query words
$W_q$.  If we describe each song as an annotation vector
$y=(y_1,\ldots,y_{|V|})$ where $y_i > 0$ if $w_i$ has a
semantic association with the audio track, and $y_i = 0$ if it does
not.  These $y_i$ are proportional to the strength of the semantic
association and are thus called semantic weights.  We then map these
semantic weights to the range ${\{0, 1\}}$ and interpret them as the class
labels.  We can then represent a song $s$ as $X = {x_1, \ldots, x_T}$
of $T$ real-valued feature vectors, with each vector $x_t$
representing audio features that have been extracted from a short
section of the song.  The data set $D$ that we use is a collection of
pairs of tracks and annotations $D
={(X_1,y_1),\ldots,(X_{|D|},y_{|D|})}$.

Automatic audio tag annotation can be viewed as a special case of
multi-label classification. Traditional {\it single-label}
classification is concerned with learning from a set of examples that
are associated with a single label $l$ from a set of disjoint labels
$L$, $|L|>1$. If $|L|=2$, then the learning problem is called {\it
  binary} classification, while if $|L|>2$ then it is called a
multi-class classification problem.  In {\it multi-label}
classification the examples are associated with a set of labels $Y
\subset L$. In addition, and in contrast to other multi-label
classification problems, tags are relatively sparse and therefore 
there is an imbalance between positive and negative examples for each
tag.  


\begin{figure}[htb]
\includegraphics[width=80mm]{blockdiagram}
\label{fig:blockdiagram} 
\caption{System flow diagram}
\end{figure} 


\subsection{Audio Feature Extraction and Stacked Classification } 

Each audio track is represented as a single feature vector. Even
though much more elaborate audio track representations have been
proposed in the literature we like the simplicity of machine learning
and similarity calculation using single feature vectors per audio
clip. It has been shown that such song-level features perform quite
well \cite{mandel-ismir2005}.

\newpage

The features used are Spectral Centroid, Roll-Off, Flux and
Mel-Frequency Cepstral Coefficients (MFCC). To capture the feature we
compute a running mean and standard deviation over the past $M$ frames:

\begin{eqnarray} 
  m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
  s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)] 
\end{eqnarray} 

where $\Phi(t)$ is the original feature vector. Notice that the
dynamics features are computed at the same rate as the original
feature vector but depend on the past $M$ frames (e.g. M=40,
corresponding approximately to a so-called ``texture window'' of 1
second).  This results in a feature vector of 32 dimensions at the
same rate as the original 16-dimensional one. 
% This process is
% illustrated in Figure ~\ref{fig:featExtract}. 
The sequence of feature vectors is collapsed into a single feature
vector representing the entire audio clip by taking again the mean and
standard deviation across the 30 seconds (the sequence of dynamics
features) resulting in the final 64-dimensional feature vector per
audio clip. A more detailed description of the features can be found
in Tzanetakis and Cook \cite{TC02b}.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{featureExtraction} 
%   \caption{\it Feature extraction and texture window}
%   \label{fig:featExtract}
% \end{figure}



For training the support vector machine classifier the feature vectors
(one per audio track) are normalized so that the minimum of each
feature is 0 and the maximum in 1 (Max/Min Normalization). The {\it
  Marsyas} audio processing framework (\url{http://marsyas.sness.net})
was used for the computation of the features. Both stages of
classification (audio and stacked affinity) utilize a multi-class
Support Vector Machine implemented as a collection of binary
one-versus all discriminative classifiers. The libSVM software package
is used for training and classification \cite{cc01}.


\section{Experiments} 

We tested the system on two publicly available audio data-sets.
The Computer Audition Lab 500 (CAL500) \cite{turnbull2008} dataset
is a selection of 500 Western popular
songs recorded by 500 different artists, from between 1958 and 2008. Each
song is manually annotated with an appropriate subset of 135 total tags
(including positive and negative tags), including 29 instruments, 22
vocal characteristics, 36 genres, 18 emotions, 15 preferred listening
scenarios, and 15 concepts such as tempo and sound quality. Each song
has at least 3 annotations, with a total of 1708 annotations in the collection.
The Magnatagatune \cite{law07} dataset is a collection of 21642 songs and 188 tags.
The songs were provided by Magnatune.com and FreeSound.org, and span
the genres of classical, new age, electronica, rock, pop, world music,
jazz, blues, heavy metal, and punk. Annotations for the files were collected via
the TagATune game-with-a-purpose, in which two players were asked each to annotate
a song. The players were then shown each other's annotations and asked to
guess whether or not they had been listening to the same song.



The results generated by the SVM algorithm are in the form of an
affinity matrix, with one dimension representing all the songs in the
collection, and one dimension representing the affinity of a
particular tag for that song.  This affinity matrix can be compared to
a similarly constructed ground truth matrix via the Receiver Operating
Characteristic (ROC) curve \cite{fawcett06}, which creates
a curve by iteratively changing a cut-off level and plotting the
resulting values. We can then integrate this curve to obtain the Area
under Receiver Operating Characteristic curve (AROC). The values of
AROC vary between 0 and 1, with larger values signifying better
classifier performance.  Precision, recall, accuracy and F-measure are
also calculated in the standard way. We report these measures over
both the entire (global) binary matrix as well as seperately for each
tag and then averaged across tags. The per-tag average accuracy is a
better measure as it is not biased by popular tags.


\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|} \hline
              &     Accuracy  & F-measure &  AROC  &  AROC  \\
              &        Tag    &    Tag    &   Tag  &  Clip  \\\hline
 BTL          &        0.842  &    0.258  &  0.68  &  0.78  \\
 Audio SVM    &        0.865  &    0.394  &  0.78  &  0.86  \\
 Affinity SVM &        0.882  &    0.498  &  0.85  &  0.89  \\ \hline

\end{tabular}
\caption{CAL500 Evaluation Metrics}
\label{table:cal500}
\end{table}

Table ~\ref{table:cal500} shows various evaluation metrics comparing
the performance of the best performing system in MIREX 2008 (LTB)
\cite{turnbull2008}. To calculate these numbers, we obtained from the
authors the predicted affinity matrix associating songs and tags for
CAL500.  The second line of the table shows the performance of the
audio-based SVM system using song-level features. The third line shows
the improvement in performance using the stacked generalization where
the input to the second level classifier is the affinity vector
predicted by the first level classifier. The same thresholding was
applied in all cases. The threshold for each tag was chosen such that
the number of testing songs associated with a given tag is
proportional to the frequency in which that tag was applied to the
training songs. All the results were obtained using 2-fold
cross-validation and were not significantly different than using the
training set for testing. This is expected given the challenging
nature of multiple-label classification which makes over-fitting to the
training data more unlikely.

\begin{table}[t]
\centering
\caption{Magnatagatune : Audio and Affinity SVM - Global evaluation metrics}
\begin{tabular}{|r|r|r|r|r|} \hline
              &  Precision  &  Recall  &  Accuracy  &  F-Score  \\\hline
 Audio SVM    &      0.307  &   0.315  &     0.969  &    0.311  \\
 Affinity SVM &      0.351  &   0.354  &     0.971  &    0.353  \\\hline
\end{tabular}
\label{table:magnatune_global}
\end{table}



\begin{table}[t]
\centering
\begin{tabular}{|r|r|r|r|r|} \hline
 \# Tags     &  Precision  &  Recall  &  Accuracy  &  F-Score  \\\hline
        20  &      0.417  &    0.688  &     0.856  &    0.516  \\
        30  &      0.345  &    0.669  &     0.862  &    0.452  \\
        40  &      0.370  &    0.381  &     0.910  &    0.375  \\
        50  &      0.328  &    0.337  &     0.919  &    0.332  \\
       100  &      0.189  &    0.195  &     0.947  &    0.192  \\
 all (188)  &      0.127  &    0.130  &     0.969  &    0.129  \\\hline
\end{tabular}
\caption{Magnatagatune : Audio SVM - Per-tag evaluation metrics }
\label{table:nonstacked_per_tag}
\end{table}



\begin{table}[t]
\centering
\begin{tabular}{|r|r|r|r|r|} \hline
\# Tags      &  Precision  &   Recall  &  Accuracy  &  F-Score  \\\hline
        20  &      0.418  &    0.691  &     0.856  &    0.518  \\
        30  &      0.346  &    0.671  &     0.862  &    0.453  \\
        40  &      0.394  &    0.397  &     0.914  &    0.395  \\
        50  &      0.369  &    0.372  &     0.923  &    0.371  \\
       100  &      0.259  &    0.262  &     0.951  &    0.260  \\
 all (188)  &      0.184  &    0.186  &     0.971  &    0.185  \\\hline
\end{tabular}
\caption{Magnatagatune : Affinity SVM - Per-tag evaulation metrics}
\label{table:stacked_per_tag}
\end{table}



Table ~\ref{table:magnatune_global} shows the results for both audio
and stacked generalization using probabilistic SVM outputs for the
Magnatagatune dataset and all tags. We believe this is the first time
results are published for this dataset. The global evaluation metrics
are biased towards popular tags, so can be misleading. As this dataset
has many tags, we explore using different number of tags for training
the classifier. For example ``30'' means that the 30 most popular tags
were used to train the
classifier. Tables~\ref{table:nonstacked_per_tag}
and~\ref{table:stacked_per_tag} show similar results by averaging the
evaluation metrics across tags. This way tags that are not popular are
as important as popular tags in terms of being predicted correctly. As
can be seen, stacked generalization of probabilistic SVM outputs
improves all per-tag evaluation metrics especially when all tags are
considered. This is expected, as it can capture tag relations even
among tags that might not be represented enough for accurate
audio-based classification.








\section{Conclusions} 

Stacked generalization of the probabilistic outputs of a Support
Vector Machine classifier can be used to improve the performance of
automatic audio tag annotation. The scheme is straightforward to
implement and provides significant improvements over one stage
classification using a variety of standard evaluation measures in two
publicly available datasets. We believe that a similar approach could
be used for other tasks such as automatic image annotation. 



\bibliographystyle{abbrv}

\bibliography{acmmm2009gtzan}  % sigproc.bib is the name of the Bibliography in this case
\end{document}


\begin{thebibliography}{10}

\bibitem{berenzweig03}
A.~Berenzweig, D.~P.~W. Ellis, and S.~Lawrence.
\newblock Anchor space for classification and similarity measurement of music.
\newblock In {\em Proc. of Int. Conf. on Multimedia and Expo (ICME)}, pages
  29--32, 2003.

\bibitem{cc01}
C.~Chang and C.~Lin.
\newblock {\em {LIBSVM}: a library for support vector machines}, 2001.
\newblock Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.

\bibitem{downie08}
S.~J. Downie.
\newblock The music information retrieval evaluation exchange (2005--2007): A
  window into music information retrieval research.
\newblock {\em Acoustical Science and Technology}, 29(4):247--255, 2008.

\bibitem{eck2007}
D.~Eck, P.~Lamere, T.~Bertin-Mahieux, and S.~Green.
\newblock Automatic generation of social tags for music recommendation.
\newblock In {\em Adv. in Neural Information Processing Systems}, volume~20,
  2007.

\bibitem{fawcett06}
T.~Fawcett.
\newblock An introduction to {ROC} analysis.
\newblock {\em Pattern Recognition Letters}, 27(8):861--874, June 2006.

\bibitem{dodbole04}
S.~Godbole and S.~Sarawagi.
\newblock Discriminative methods for multi-labeled classification.
\newblock In {\em Proc. Pacific-Asia Conf. on Knowledge Discovery and Data
  Mining}, 2004.

\bibitem{law07}
E.~L.~M. Law, L.~V. Ahn, R.~B. Dannenberg, and M.~Crawford.
\newblock Tagatune: A game for music and sound annotation.
\newblock In {\em {Proc. Int. Conf. on Music Information Retrieval (ISMIR)}},
  2007.

\bibitem{mandel-ismir2005}
M.~Mandel and D.~Ellis.
\newblock Song-level features and support vector machines for music
  classification.
\newblock In {\em {Proc. Int. Conf. on Music Information Retrieval (ISMIR)}},
  2005.

\bibitem{mandel-ismir2008}
M.~Mandel and D.~Ellis.
\newblock Multiple-instance learning for music information retrieval.
\newblock In {\em {Proc. Int. Conf. on Music Information Retrieval (ISMIR)}},
  2008.

\bibitem{pachet09}
F.~Pachet and P.~Roy.
\newblock Improving multilabel analysis of music titles: A large-scale
  validation of the correction approach.
\newblock {\em Audio, Speech, and Language Processing, IEEE Transactions on},
  17(2):335--343, 2009.

\bibitem{platt99}
J.~C. Platt.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock In {\em Advances in Large Margin Classifiers}, pages 61--74. MIT
  Press, 1999.

\bibitem{schein02}
A.~Schein, A.~Popescul, L.~Ungar, and D.~Pennock.
\newblock Methods and metrics for cold-start recommendations.
\newblock In {\em Proc. ACM SIGIR Conf. on Research and Development in
  Information Retrieval}, 2002.

\bibitem{slaney02mixtures}
M.~Slaney.
\newblock Mixtures of probability experts for audio retrieval and indexing.
\newblock In {\em Multimedia and Expo, 2002. ICME '02. Proc. 2002 IEEE Int.
  Conf. on}, volume~1, pages 345--348 vol.1, 2002.

\bibitem{trohidis08}
K.~Trohidis, G.~Tsoumakas, G.~Kalliris, and I.~Vlahavas.
\newblock Multilabel classification of music into emotions.
\newblock In {\em Proc. Int. Conf. on Music Information Retrieval (ISMIR)},
  2008.

\bibitem{tsai08}
C.-F. Tsai and C.~Hung.
\newblock Automatically annotating images with keywords: A review of image
  annotation systems.
\newblock {\em Recent Patents on Computer Science}, 1:55--68, 2008.

\bibitem{tsoumakas2007}
G.~Tsoumakas and I.~Katakis.
\newblock Multi label classification: An overview.
\newblock {\em Int. Journal of Data Warehouse and Mining}, 3(3):1--13, 2007.

\bibitem{turnbull08}
D.~Turnbull, L.~Barrington, and G.~Lanckriet.
\newblock Five approaches to collecting tags for music.
\newblock In {\em Proc. Int. Conf. on Music Information Retrieval (ISMIR)},
  2008.

\bibitem{turnbull2008}
D.~Turnbull, L.~Barrington, D.~Torres, and G.~Lanckriet.
\newblock Semantic annotation and retrieval of music and sound effects.
\newblock {\em Audio, Speech, and Language Processing, IEEE Transactions on},
  16(2):467--476.

\bibitem{TC02b}
G.~Tzanetakis and P.~Cook.
\newblock Musical {G}enre {C}lassification of {A}udio {S}ignals.
\newblock {\em IEEE Trans. on Speech and Audio Processing}, 10(5), July 2002.

\bibitem{1642623}
L.~von Ahn.
\newblock Games with a purpose.
\newblock {\em Computer}, 39(6):92--94, June 2006.

\bibitem{whitman02}
B.~Whitman and R.~Rifkin.
\newblock Musical query-by-description as a multiclass learning problem.
\newblock In {\em In Proc. IEEE Multimedia Signal Processing Conf. (MMSP)},
  pages 153--156, 2002.

\bibitem{wolpert92}
D.~H. Wolpert.
\newblock Stacked generalization.
\newblock {\em Neural Networks}, 5:241--259, 1992.

\end{thebibliography}

% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage{url} 
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ACM International Conference on Multimedia}{2009 Beijing, China}
%\CopyrightYear{2007} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Music Analysis, Retrieval and Synthesis of Audio Signals \\
  MARSYAS} 

%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
% \alignauthor
% Steven R. Ness \\
%        \affaddr{Department of Computer Science}\\
%        \affaddr{University of Victoria}\\
%        \affaddr{PO Box 3055, STN CSC}\\
%        \affaddr{Victoria, BC, CANADA}\\
%        \email{sness@sness.net}
% % 2nd. author
% \alignauthor
% Anthony Theocharis \\
%        \affaddr{Department of Computer Science}\\
%        \affaddr{University of Victoria}\\
%        \affaddr{PO Box 3055, STN CSC}\\
%        \affaddr{Victoria, BC, CANADA}\\
%        \email{sness@sness.net}
% % 3rd. author
% \and
% \alignauthor
% L. Gustavo Martins\\
%        \affaddr{CITAR (Research Center for Science And Technology in Art)}\\
%        \affaddr{Rua Diogo Botelho, no 1327}\\
%        \affaddr{Porto, Portugal}\\
%        \email{lgustavomartins@gmail.com}
% % 4th. author
\alignauthor
George Tzanetakis \\
       \affaddr{University of Victoria}\\
       \affaddr{PO Box 3055, STN CSC}\\
       \affaddr{Victoria, BC, CANADA}\\
       \email{gtzan@cs.uvic.ca}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{27 Apr 2009}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
  Marsyas is an open source software framework for music analysis,
  retrieval and synthesis with specific emphasis on Music Information
  Retreival applications. It has been in development for 10 years and
  has been used for a variety of projects in both academia and
  industry in several countries. Based on a novel dataflow
  architecture named implicit patching it provides a variety of
  existing processing modules for digital signal processing, machine
  learning and audio input/output that can be combined at run-time to
  form complex dataflow networks expressing audio processing
  algorithms (black-box functionality). In addition it allows the 
  easy addition of new processing modules that need to be compiled for
  performance purposes. Finally Marsyas is designed with
  inter-operability in mind and provides various mechanisms for
  communicating with other software including bindings to the run-time
  functionality in scripting languages (Python, Ruby), run-time
  data interchange with MATLAB, support for the Music Instrument Digital
  Interface (MIDI) protocol and Open Sound Control (OSC) for
  communicating with controller devices, and infrastructure for easy
  interfacing to the GUI components of the Qt toolkit. 




\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.1}{Information Storage and Retrieval}{Content Analysis
  and Indexing Methods}

\terms{Algorithms, Theory, Experimentation}

\keywords{sound analysis, music information retrieval}


\section{Introduction}

Music has always been transformed by advances in technology. Examples
of technologies that transformed the way music was produced,
distributed and consumed include musical instruments, music notation,
recording and more recently digital music storage and
distribution. Recently portable digital music players have become a
familiar sight and online music sales have been steadily
increasing. It is likely that in the near future anyone will be able
to access digitally all of recorded music in human history. In order
to efficiently interact with the rapidly growing collections of
digitally available music it is necessary to develop tools that have
some understanding of the actual musical content. Music Information
Retrieval (MIR) is an emerging research area that deals with all
aspects of organizing and extracting information from music signals.


In the past few years, interest in Music Information Retrieval (MIR)
has been steadily increasing. MIR algorithms, especially when
analyzing music signals in audio format, typically utilize
state-of-the-art signal processing and machine learning
algorithms. The large amounts of data that is processed together with
the huge computational requirements of audio processing can stress
current hardware to its limits. Therefore efficient processing is
critical for building functional MIR systems that scale to large
collections of music and eventually to all of recorded
music. Moreover, MIR is an inherently interdisciplinary field with
practitioners with varying degrees of computer and programming
expertise (examples of fields involved include musicology, information
science, and cognitive psychology). Therefore it is desirable for MIR
systems to support multiple hierarchical levels of usage and
extensibility. These issues make the design and development of MIR
systems and frameworks especially challenging.

MARSYAS (Music Analysis, Retrieval and SYnthesis for Audio Signals),
is an open source audio processing framework with specific emphasis on
building MIR systems. It has been under development since 1998 and has
been used for a variety of projects both in academia and industry. The
guiding principle behind the design of MARSYAS has always been to
provide a flexible, expressive and extensive framework without
sacrificing computational efficiency. Addressing these conflicting
requirements is the major challenge facing the software engineer of
MIR systems.




\section{Related Work}

Music Information Retrieval is a new area of content-based multimedia
information retrieval. Although there was sporadic earlier work, a
good reference starting point is the first international conference on
MIR (ISMIR) which was held in 2000. These conferences (ISMIR) have
been a forum for bringing together music researchers, audio engineers,
computer scientists, musicologists, librarians, and music industry
(\url{http://www.ismir.net}). MIR with audio signals typically requires
signal processing and machine learning algorithms in order to achieve
tasks such as classification, similarity-retrieval and segmentation.

The current version of Marsyas evolved from MARSYAS 0.1, a framework
that focused mostly on audio analysis. One of the motivating factors
for the rewrite of the code and architecture was the desire to add
audio synthesis capabilities and was influenced by the design of the
Synthesis Toolkit
(\url{http://ccrma.stanford.edu/software/stk/}). Other influences
include the powerful but more complex architecture of CLAM
(\url{http://clam-project.org/}) , the patching model and strong
timing of Chuck (\url{http://chuck.cs.princeton.edu/} ). The default naming scheme for
controls is inspired by the Open Sound Control (OSC) protocol
(\url{http://opensoundcontrol.org}). 

The idea of dataflow programming has been fundamental in the design of
MARSYAS.  Dataflow programming has a long history. The original (and
still valid) motivation for research into dataflow was to take
advantage of parallelism. Motivated by criticisms of the classical von
Neumann hardware architecture dataflow architectures for hardware were
proposed as an alternative in the 1970s and 1980s. During the 1990s
there was a new direction of growth in the field of dataflow visual
programming languages that were domain specific. In such visual
languages programming is done by connecting processing objects with
wires to create patches.  Successful examples include Labview
(\url{http://www.ni.com/labview/}), SimuLink
(\url{http://www.mathworks/com/products/simulink/}) and in the field
of Computer Music Max/MSP
(\url{http://www.cycling74.com/products/max5}) and Pure Data
(\url{http://puredata.info/Puckette, 2002}).


\section{Evidence of impact}

The impact of Marsyas has been steadily increasing especially since
2007 where a more comprehensive website was launched and the core team
increased to approximately {\bf 4-5 developers}. Since November {\bf
  2007} when the new website was launched it has received 18236
(approximately {\bf 30 visits/day}) from {\bf 114 countries}. The
software framework has approximately {\bf 300-400 downloads} per month
since {\bf 2007} an increase from less than 50 downloads/month in
2003. 

Marsyas has been used for a variety of projects in academia and
industry several of which are described in
\url{http://marsyas.sness.net/about/projects}. Industrial applications
include prototyping the patented audio fingerprinting technology of
Moodlogic Inc. which has been used by more than {\bf 100,000} users to
link mp3 audio files to a large database ({\bf 1.5 million songs}) of
metadata and a gender identification system for voice messages
designed for Teligence Inc. that is processing approximately {\bf
  25,000 voice recordings per day}. Although Marsyas is available
under the GNU Public Licence (GPL) it can also be provided with
commercial licenses that facilitate development of closed-source 
applications. 



Academic projects include emotion recognition in music (Greece,
Canada), novel interfaces for browsing music (Japan), a dancing music
robot (Portugal), and a multi-model search engine for music in YouTube
videos (Singapore). A large number of publications have been written
about Marsyas (6), using Marsyas (31) and citing Marsyas (12). More
details can be found at
\url{http://marsyas.sness.net/about/publications}. Algorithms based on
Marsyas are regularly submitted to the Music Information Retrieval
Evaluation Exchange
(\url{http://www.music-ir.org/mirex/2009/index.php/Main_Page}) where
they exhibit state-of-the-art performance while being orders of
magnitude faster than other submissions.










\section{Exploring Marsyas}

Marsyas is a large open source software framework that can be used in
a variety of different ways therefore there is no single easy path to
learning the software and its capabilities. Detailed downloading,
compiling and installation instructions are available at
\url{http://marsyas.sness.net/docs/manual/marsyas-user/Source-installation.html}. A
taste of different applications, projects and usage scenarios can be
found by looking at the videos and webdemos
(\url{http://marsyas.sness.net/about/videos},
\url{http://marsyas/sness.net/about/webdemos}). The documentation
consists of a user manual, developer manual, library reference
(automatically generated from the source code) as well as a cookbook
that contains short code examples for simple tasks. The tour chapter
of the user manual provides information about how to run several
examples of the tools included with the Marsyas distribution for
demonstration purposes. 




\section{Contributors} 

As with most open source software the contributors to Marsyas have
fluctuated over the years. Currently there is a core team of
approximately {\bf 4-5} developers as well as a more extended
community of approximately {\bf 20-30} regular users who occassionally
contribute bug fixes and small amounts of code. The current core
development team and their current (June 2009) affiliations consists
of:
\begin{itemize} 
\item{George Tzanetakis - University of Victoria, \\ 
Victoria, Canada -  {\it gtzan@cs.uvic.ca}} 
\item{Luis Gustavo Martins - Catolica University, \\ 
Porto, Portugal - {\it lgustavomartins@gmail.com} }
\item{Luis Teixeira - Fraunhofer Centre for Assistive Information and
    Communication Solutions (AICOS), \\ Porto Portugal - {\it
      luis.f.teixeira@gmail.com}}  
\item{Graham Percival - National University of Singapore \\ 
Singapore - gpermus@gmail.com}
\item{Mathieu Lagrange - TELECOM ParisTech, \\ 
Paris, France - mathieu.lagrange@enst.fr}
\item{Emiru Tsunoo - University of Tokyo, \\ 
Tokyo, Japan - tsunoo@hil.t.u-tokyo.ac.jp}
\item{Stefaan Lippens - Ghent University, \\ 
Ghent, Belgium - stefaan.lippens@gmail.com}
\end{itemize}

More details about previous developers and more extended lits of users
and projects can be found at the marsyas webpage
\url{http://marsyas.sness.net}. 




\bibliographystyle{abbrv}
\bibliography{acmmm2009gtzan}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage{url} 
\pdfpagewidth 8.5in
\pdfpageheight 11in
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{MM'09,} {October 19--24, 2009, Beijing, China.} 
\CopyrightYear{2009}
\crdata{978-1-60558-608-3/09/10} 
%\conferenceinfo{ACM International Conference on Multimedia}{2009 Beijing, China}
%\CopyrightYear{2007} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Music Analysis, Retrieval and Synthesis of Audio Signals \\
  MARSYAS} 

%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
% \alignauthor
% Steven R. Ness \\
%        \affaddr{Department of Computer Science}\\
%        \affaddr{University of Victoria}\\
%        \affaddr{PO Box 3055, STN CSC}\\
%        \affaddr{Victoria, BC, CANADA}\\
%        \email{sness@sness.net}
% % 2nd. author
% \alignauthor
% Anthony Theocharis \\
%        \affaddr{Department of Computer Science}\\
%        \affaddr{University of Victoria}\\
%        \affaddr{PO Box 3055, STN CSC}\\
%        \affaddr{Victoria, BC, CANADA}\\
%        \email{sness@sness.net}
% % 3rd. author
% \and
% \alignauthor
% L. Gustavo Martins\\
%        \affaddr{CITAR (Research Center for Science And Technology in Art)}\\
%        \affaddr{Rua Diogo Botelho, no 1327}\\
%        \affaddr{Porto, Portugal}\\
%        \email{lgustavomartins@gmail.com}
% % 4th. author
\alignauthor
George Tzanetakis \\
       \affaddr{University of Victoria}\\
       \affaddr{PO Box 3055, STN CSC}\\
       \affaddr{Victoria, BC, CANADA}\\
       \email{gtzan@cs.uvic.ca}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{27 Apr 2009}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
  Marsyas is an open source software framework for music analysis,
  retrieval and synthesis with specific emphasis on Music Information
  Retreival applications. It has been in development for 10 years and
  has been used for a variety of projects in both academia and
  industry in several countries. Based on a novel dataflow
  architecture named implicit patching it provides a variety of
  existing processing modules for digital signal processing, machine
  learning and audio input/output that can be combined at run-time to
  form complex dataflow networks expressing audio processing
  algorithms (black-box functionality). In addition it allows the 
  easy addition of new processing modules that need to be compiled for
  performance purposes. Finally Marsyas is designed with
  inter-operability in mind and provides various mechanisms for
  communicating with other software including bindings to the run-time
  functionality in scripting languages (Python, Ruby), run-time
  data interchange with MATLAB, support for the Music Instrument Digital
  Interface (MIDI) protocol and Open Sound Control (OSC) for
  communicating with controller devices, and infrastructure for easy
  interfacing to the GUI components of the Qt toolkit. 




\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.1}{Information Storage and Retrieval}{Content Analysis
  and Indexing Methods}

\terms{Algorithms, Theory, Experimentation}

\keywords{sound analysis, music information retrieval}


\section{Introduction}

Music has always been transformed by advances in technology. Examples
of technologies that transformed the way music was produced,
distributed and consumed include musical instruments, music notation,
recording and more recently digital music storage and
distribution. Recently portable digital music players have become a
familiar sight and online music sales have been steadily
increasing. It is likely that in the near future anyone will be able
to access digitally all of recorded music in human history. In order
to efficiently interact with the rapidly growing collections of
digitally available music it is necessary to develop tools that have
some understanding of the actual musical content. Music Information
Retrieval (MIR) is an emerging research area that deals with all
aspects of organizing and extracting information from music signals.


In the past few years, interest in Music Information Retrieval (MIR)
has been steadily increasing. MIR algorithms, especially when
analyzing music signals in audio format, typically utilize
state-of-the-art signal processing and machine learning
algorithms. The large amounts of data that is processed together with
the huge computational requirements of audio processing can stress
current hardware to its limits. Therefore efficient processing is
critical for building functional MIR systems that scale to large
collections of music and eventually to all of recorded
music. Moreover, MIR is an inherently interdisciplinary field with
practitioners with varying degrees of computer and programming
expertise (examples of fields involved include musicology, information
science, and cognitive psychology). Therefore it is desirable for MIR
systems to support multiple hierarchical levels of usage and
extensibility. These issues make the design and development of MIR
systems and frameworks especially challenging.

MARSYAS (Music Analysis, Retrieval and SYnthesis for Audio Signals),
is an open source audio processing framework with specific emphasis on
building MIR systems. It has been under development since 1998 and has
been used for a variety of projects both in academia and industry. The
guiding principle behind the design of MARSYAS has always been to
provide a flexible, expressive and extensive framework without
sacrificing computational efficiency. Addressing these conflicting
requirements is the major challenge facing the software engineer of
MIR systems.




\section{Related Work}

Music Information Retrieval is a new area of content-based multimedia
information retrieval. Although there was sporadic earlier work, a
good reference starting point is the first international conference on
MIR (ISMIR) which was held in 2000. These conferences (ISMIR) have
been a forum for bringing together music researchers, audio engineers,
computer scientists, musicologists, librarians, and music industry
(\url{http://www.ismir.net}). MIR with audio signals typically requires
signal processing and machine learning algorithms in order to achieve
tasks such as classification, similarity-retrieval and segmentation.

The current version of Marsyas evolved from MARSYAS 0.1, a framework
that focused mostly on audio analysis. One of the motivating factors
for the rewrite of the code and architecture was the desire to add
audio synthesis capabilities and was influenced by the design of the
Synthesis Toolkit
(\url{http://ccrma.stanford.edu/software/stk/}). Other influences
include the powerful but more complex architecture of CLAM
(\url{http://clam-project.org/}) , the patching model and strong
timing of Chuck (\url{http://chuck.cs.princeton.edu/} ). The default naming scheme for
controls is inspired by the Open Sound Control (OSC) protocol
(\url{http://opensoundcontrol.org}). 

The idea of dataflow programming has been fundamental in the design of
MARSYAS.  Dataflow programming has a long history. The original (and
still valid) motivation for research into dataflow was to take
advantage of parallelism. Motivated by criticisms of the classical von
Neumann hardware architecture dataflow architectures for hardware were
proposed as an alternative in the 1970s and 1980s. During the 1990s
there was a new direction of growth in the field of dataflow visual
programming languages that were domain specific. In such visual
languages programming is done by connecting processing objects with
wires to create patches.  Successful examples include Labview
(\url{http://www.ni.com/labview/}), SimuLink
(\url{http://www.mathworks/com/products/simulink/}) and in the field
of Computer Music Max/MSP
(\url{http://www.cycling74.com/products/max5}) and Pure Data
(\url{http://puredata.info/Puckette, 2002}).


\section{Evidence of impact}

The impact of Marsyas has been steadily increasing especially since
2007 where a more comprehensive website was launched and the core team
increased to approximately {\bf 4-5 developers}. Since November {\bf
  2007} when the new website was launched it has received 18236
(approximately {\bf 30 visits/day}) from {\bf 114 countries}. The
software framework has approximately {\bf 300-400 downloads} per month
since {\bf 2007} an increase from less than 50 downloads/month in
2003. 

Marsyas has been used for a variety of projects in academia and
industry several of which are described in
\url{http://marsyas.sness.net/about/projects}. Industrial applications
include prototyping the patented audio fingerprinting technology of
Moodlogic Inc. which has been used by more than {\bf 100,000} users to
link mp3 audio files to a large database ({\bf 1.5 million songs}) of
metadata and a gender identification system for voice messages
designed for Teligence Inc. that is processing approximately {\bf
  25,000 voice recordings per day}. Although Marsyas is available
under the GNU Public Licence (GPL) it can also be provided with
commercial licenses that facilitate development of closed-source 
applications. 



Academic projects include emotion recognition in music (Greece,
Canada), novel interfaces for browsing music (Japan), a dancing music
robot (Portugal), and a multi-model search engine for music in YouTube
videos (Singapore). A large number of publications have been written
about Marsyas (6), using Marsyas (31) and citing Marsyas (12). More
details can be found at
\url{http://marsyas.sness.net/about/publications}. Algorithms based on
Marsyas are regularly submitted to the Music Information Retrieval
Evaluation Exchange
(\url{http://www.music-ir.org/mirex/2009/index.php/Main_Page}) where
they exhibit state-of-the-art performance while being orders of
magnitude faster than other submissions.










\section{Exploring Marsyas}

Marsyas is a large open source software framework that can be used in
a variety of different ways therefore there is no single easy path to
learning the software and its capabilities. Detailed downloading,
compiling and installation instructions are available at
\url{http://marsyas.sness.net/docs/manual/marsyas-user/Source-installation.html}. A
taste of different applications, projects and usage scenarios can be
found by looking at the videos and webdemos
(\url{http://marsyas.sness.net/about/videos},
\url{http://marsyas/sness.net/about/webdemos}). The documentation
consists of a user manual, developer manual, library reference
(automatically generated from the source code) as well as a cookbook
that contains short code examples for simple tasks. The tour chapter
of the user manual provides information about how to run several
examples of the tools included with the Marsyas distribution for
demonstration purposes. 




\section{Contributors} 

As with most open source software the contributors to Marsyas have
fluctuated over the years. Currently there is a core team of
approximately {\bf 4-5} developers as well as a more extended
community of approximately {\bf 20-30} regular users who occassionally
contribute bug fixes and small amounts of code. The current core
development team and their current (June 2009) affiliations consists
of:
\begin{itemize} 
\item{George Tzanetakis - University of Victoria, \\ 
Victoria, Canada -  {\it gtzan@cs.uvic.ca}} 
\item{Luis Gustavo Martins - Catolica University, \\ 
Porto, Portugal - {\it lgustavomartins@gmail.com} }
\item{Luis Teixeira - Fraunhofer Centre for Assistive Information and
    Communication Solutions (AICOS), \\ Porto Portugal - {\it
      luis.f.teixeira@gmail.com}}  
\item{Graham Percival - National University of Singapore \\ 
Singapore - {\it gpermus@gmail.com}}
\item{Mathieu Lagrange - TELECOM ParisTech, \\ 
Paris, France - {\it mathieu.lagrange@enst.fr}}
\item{Steven Ness - University of Victoria, \\ 
Victoria, Canada - {\it sness@sness.net}}
\item{Emiru Tsunoo - University of Tokyo, \\ 
Tokyo, Japan - {\it tsunoo@hil.t.u-tokyo.ac.jp}}
\item{Stefaan Lippens - Ghent University, \\ 
Ghent, Belgium - {\it stefaan.lippens@gmail.com}}
\end{itemize}

More details about previous developers and more extended lits of users
and projects can be found at the marsyas webpage
\url{http://marsyas.sness.net}. 




%\bibliographystyle{abbrv}
%\bibliography{acmmm2009gtzan}  % sigproc.bib is the name of the Bibliography in this case

\end{document}

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{cbmi2009}
\usepackage{times}

%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Content-aware web browsing and visualization tools \\ for
  cantillation and chant research}

\author{
	Steven R. Ness\\
	Department of Computer Science\\ 
	University of Victoria, BC, Canada. \\
	sness@sness.net\\
\and
	D\'{a}niel P\'{e}ter Bir\'{o} \\
	School of Music\\
    University of Victoria, BC, Canada\\
	dpbiro@uvic.ca\\
\and
	George Tzanetakis\\
	Department of Computer Science\\ 
	University of Victoria, BC, Canada. \\
	gtzan@cs.uvic.ca\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
	The abstract
\end{abstract}



\Section{Introduction}

\Section{Related Work}

\Section{Discussion}

\Section{Conclusion}




%------------------------------------------------------------------------- 
\bibliographystyle{cbmi2009}
\bibliography{cbmi2009}

\end{document}


%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{cbmi2009}
\usepackage{times}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{url} 

%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Content-aware web browsing and visualization tools \\ for
  cantillation and chant research}

\author{
	Steven R. Ness, George Tzanetakis\\
	Department of Computer Science\\ 
	University of Victoria, BC, Canada. \\
	sness@sness.net,gtzan@cs.uvic.ca\\
\and
	D\'{a}niel P\'{e}ter Bir\'{o} \\
	School of Music\\
    University of Victoria, BC, Canada\\
	dpbiro@uvic.ca\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
  

  Chant and cantillation research is particularly interesting as it
  explores the transition from oral to written transmission of music.
  The goal of this work to create web-based computational tools that
  can assist the study of how diverse recitation traditions, having
  their origin in primarily non-notated melodies, later became
  codified. One of the authors is a musicologist and music theorist
  who has guided the system design and development by providing manual
  annotations and participating in the design process. We describe
  novel content-based visualization and analysis algorithms that can
  be used for problem-seeking exploration of audio recordings of chant
  and recitations.



\end{abstract}



\section{Introduction}

In recent years there has been increasing research activity in the
areas of multimedia learning and information retrieval. Most of it has
been in traditional specific domains, such as sports video
\cite{hauptman97}, news video \cite{hauptman03} and natural
images. There is broad interest in these domains and in most cases
there are clearly defined objectives such as identifying highlights in
sports videos, explosions in news video or sunsets in natural
images. Our focus in this paper is a niche domain that shares
the challenge of effectively accessing large amounts of data but has
specific characteristics that preclude the use of
existing multimedia tools.

Although there is much related work little of it is directly relevant
to our particular application. Work on melodic similarity is typically
based on symbolic representations \cite{Hanna2007} and therefore not
applicable. Even in the cases where audio recordings are used
\cite{Duggan2008} there are no interactive visualizations which limits
their use by expert musicologists. An earlier version of the web-based
system we describe that did not have support for content-based
similarity retrieval of pitch contours was presented in Ness et. al
\cite{sness2008}.

The goal of this project is to develop tools to study chants from
various traditions around the world including Hungarian \emph{siratok}
(laments)\cite{kodaly60}, Torah cantillation\cite{wigoder89}, tenth
century St. Gallen plainchant\cite{karp98}, and Koran
recitation\cite{nelson85}. These diverse traditions share the common
theme of having an origin in primarily non-notated melodies which then
later became codified. The evolution and spread of differences in the
oral traditions of these different chants are a current topic of
research in Ethnomusicology \cite{ce2008}.

It has proved difficult to study these changes using traditional
methods and it was decided that a combined approach, using field
recordings marked up by experts, mathematical models for analyzing the
pitch content, automatic alignment for pitch contour similarity and a
flexible GUI, would help figure out what questions needed to be
asked. Unlike traditional multimedia data where most users can be used
as annotators, in our case annotation requires trained experts. This
is a problem seeking domain where there are no clearly defined
objectives and formulating problems is as important as solving
them. We believe that despite these challenges it is possible to
develop semi-automatic tools that can assist researchers in
formulating questions regarding how symbols are used in chant and
recitation.

Web-based software has been helping connect communities of researchers
since its inception. Recently, advances in software and in computer
power have dramatically widened its possible applications to include a
wide variety of multimedia content. These advances have been primarily
in the business community, and the tools developed are just starting
to be used by academics. We have been working on applying these
technologies to ongoing collaborative projects \cite{sness2008}. We
leverage several new technologies including \emph{Flash}, \emph{haXe},
\emph{AJAX} and \emph{Ruby on Rails}, to rapidly develop web-based
tools. Rapid prototyping and iterative development have been key
elements of our collaborative strategy. Although our number of users
is limited compared to other areas of multimedia analysis and
retrieval, this is to some degree compensated by their passion and
willingness to work closely with us in developing these tools.



\section{Chant research}

Our work in developing tools for chant research is a collaboration
with Dr. Daniel Biro, a professor in the School of Music at the
University of Victoria. He has been collecting and studying recordings
of chant with specific focus on how music transmission based on oral
transmission and ritual was gradually changed to one based on writing
and music notation. The examples studied come from improvised,
partially notated, and gesture-based \cite{krumhansl90} notational
chant traditions: Hungarian siratok (laments) \footnote{Archived
  Examples from Hungarian Academy of Science (1968-1973)}, Torah
cantillation \cite{zimmerman00} \footnote{Archived Examples from
  Hungary and Morocco from the Feher Music Center at the Bet
  Hatfatsut, Tel Aviv, Israel}, tenth century St. Gallen plainchant
\cite{treitler82} \footnote{Godehard Joppich and Singphoniker:
  Gregorian Chant from St. Gallen (Gorgmarienh√ºtte: CPO 999267-2,
  1994)}, and Koran recitation \footnote{Examples from Indonesia and
  Egypt: in Approaching the Koran (Ashland: White Cloud, 1999)}. This
work falls under the more general area of Computational
Ethnomusicology \cite{ce2008}.


Although Dr. Biro has been studying these recordings for some time and
has considerable computer expertise for a professor in music, the
design and development of our tools has been challenging. This is
partly due to difficulties in communication and terminology as well as
the fact that the work is exploratory in nature and there are no
easily defined objectives. The tool has been developed through
extensive interactions with Dr. Biro with frequent frustration on both
sides. At the same time, a wonderful thing about expert users like
Dr. Biro is that they are willing to spend considerable time preparing
and annotating data as well as testing the system and user interface
which is not the case in more traditional broad application domains.



\section{Analysis and Browsing}

\subsection{Melodic Contour Analysis} 

Our tool takes in a (digitized) monophonic or heterophonic recording
and produces a series of successively more refined and abstract
representations of the segments it contains as well as the
corresponding melodic contours. More specifically the following
analysis stages are performed:

\begin{itemize}\addtolength{\itemsep}{-0.5\baselineskip}
\item{Hand Labeling of Audio Segments}
\item{First Order Markov Model of Sign Sequences}
\item{F0 Estimation}
\item{F0 Pruning}
\item{Scale Derivation: Kernel Density Estimation}
\item{Quantization in Pitch}
\item{Scale-Degree Histogram}
\item{Histogram-Based Contour Abstraction}
\item{Dynamic Time Warping for Contour Similarity} 
\item{Plotting and Recombining the Segments}
\end{itemize} 


The recordings are manually segmented and annotated by the
expert. Even though we considered the possibility of creating an
automatic segmentation tool, it was decided that the task was too
subjective and critical to automate. Each segment is annotated with a
word/symbol that is related to the corresponding text or performance
symbols (for example cantillation marks) used during the recitation.


\begin{figure}[htb]
\includegraphics[width=80mm]{transition_matrix}
\label{fig:transitions} 
\caption{ Syntagmatic analysis with a 1st-order Markov model of
  Torah trope signs for Shir Ha Shirim (``Song of Songs'').}
\end{figure} 


In order to study the transitions between signs/symbols we calculate a
first order Markov model of the sign sequence for each recording. We
were asked to perform this type of syntagmatic analysis by
Dr. Biro. Although it is completely straightforward to perform
automatically using the annotation, it would be hard, if not
impossible, to calculate manually. Figure ~\ref{fig:transitions} shows
an example transition matrix. For a given trope sign (a row) it shows
how many total times does it appear in the example (numeral after row
label), and in what fraction of those appearances is it followed by
each of the other trope signs. The darkness of each cell corresponds
to the fraction of times that the trope sign in the given row is
followed by the trope sign in the given column.  (NB: Cell shading is
relative to the total number of occurrences of the trope sign in the
row, so, e.g., the black square saying that ``darga'' always precedes
``revia'' represents 1/1, while the black square saying that ``zakef''
always precedes ``katon'' represents 9/9.) This type of analysis can
help identify the syntactic role of different signs.

After the segments have been identified, the fundamental frequency
(``F0'' in this case equivalent to pitch) and signal energy (related
to loudness) are calculated for each segment as functions of time. We
use the SWIPEP fundamental frequency estimator \cite{camachophd} with
all default parameters except for hand-tuned upper and lower frequency
bounds for each example. For signal energy we simply take the sum of
squares in 10-ms rectangular windows.


\begin{figure}[t]
\includegraphics[width=80mm]{f0contour}
\label{fig:contour}
\caption{F0 contour} 
\end{figure} 

The next step is to identify pauses between phrases, so as to
eliminate the meaningless and wildly varying F0 estimates during these
noisy regions. We define an energy threshold, generally 40 decibels
below each recording's maximum. If the signal energy stays below this
threshold for at least 100 ms then the quiet region is treated as
silence and its F0 estimates are ignored. Figure~\ref{fig:contour}
shows an excerpt of the F0 and energy curves for an excerpt from the
Koran sura (``section'') Al-Qadr (``destiny'') recited by the renowned
Sheikh Mahmud Khalil al-Husari from Egypt.



\begin{figure}[t] 
\includegraphics[width=80mm]{scalehistogram}
\label{fig:scale}
\caption{Recording-specific scale derivation} 
\end{figure} 


Following the pitch contour extraction is pitch quantization, which is
the discretization of the continuous pitch contour into discrete notes
of a scale. Rather than externally imposing a particular set of
pitches, such as an equal-tempered chromatic (the piano keys) or
diatonic scale, we have developed a novel method for extracting a
scale from an F0 envelope that is continuous (or at least very densely
sampled) in both time and pitch. Our method is inspired by Krumhansl's
time-on-pitch histograms adding up the total amount of time spent on
each pitch \cite{krumhansl90}. We demand a pitch resolution of one
cent \footnote{One cent is 1/100 of a semitone, corresponding to a
  frequency difference of about 0.06\%}, so we cannot use a simple
histogram. Instead we use a statistical technique known as
non-parametric kernel density estimation, with a Gaussian kernel
\footnote{Thinking statistically, our scale is related to a
distribution given the relative probability of each possible
pitch. We can think of each F0 estimate (i.e each sampled value of
the F0 envelope) as a sample drawn from this unknown distribution so
our problem becomes one of estimation the unknown distribution given
the samples}. More specifically a Gaussian (with standard deviation
of 33 cents) is centered on each sample of the frequency estimate and
the Gaussians of all the samples are added to form the kernel density
estimate. The resulting curve is our density estimate; like a
histogram, it can be interpreted as the relative probability of each
pitch appearing at any given point in time. Figure ~\ref{fig:scale} shows this
method's density estimate given the F0 curve from Figure ~\ref{fig:contour}.


We interpret each peak in the density estimate as a note of the
scale. We restrict the minimum interval between scale pitches
(currently 80 cents by default) by choosing only the higher peak when
there are two or more very close peaks. This method's free parameter
is the standard deviation of the Gaussian kernel, which provides an
adjustable level of smoothness to our density estimate; we have
obtained good results with a standard deviation of 30 cents. 


Once we have determined the scale, pitch quantization is the trivial
task of converting each F0 estimate to the nearest note of the scale.
In our opinion these derived scales are more true to the actual nature
of pitch-contour relationships within oral/aural and semi-notated
musical traditions. Instead of viewing these pitches to be deviations
of pre-existing ``normalized'' scales our method defines a more
differentiated scale from the outset. With our approach the scale
tones do not require ``normalization'' and thereby exist in an
autonomous microtonal environment defined solely on statistical
occurrence of pitch within a temporal unfolding of the given melodic
context. Once the pitch contour is quantized into the
recording-specific scale calculated using Kernel density estimation,
we can calculate how many times a particular scale degree appears
during an excerpt. The resulting data is a scale-degree histogram
which is used create simplified abstract visual contour
representations.




\begin{figure}[htb]
\centering
\includegraphics[width=40mm]{cantillion-30pashta-histogramlevels}
\label{fig:contours_histogram} 
\caption{Melodic contours at different levels of abstraction (top:
  original, middle: quantized, bottom: simplified using 3 most
  prominent scale degrees}
\end{figure} 

The basic idea is to only use the most salient discrete scale degrees
(the histogram bins with the highest magnitude) as significant points
to simplify the representation of the contour. By adjusting the number
of prominent scale degrees used to represent the simplified
representation the researchers can view/listen to the melodic contour
at different levels of abstraction and detail. Figure
~\ref{fig:contours_histogram} shows an original continuous contour,
the quantized representation using the recording-specific derived
scale and the abstracted representation using only the 3 most
prominent scale degrees.



\begin{figure*}[t]
\centering
\subfigure[F0 Contour of 11 Pashta]
{
    \label{fig:sub:contour-11pashta}
    \includegraphics[width=2cm,height=2cm]{contour-11pashta.pdf}
}
\hspace{1cm}
\subfigure[F0 Contour of 42 Pashta]
{
    \label{fig:sub:contour-42pashta}
    \includegraphics[width=2cm,height=2cm]{contour-42pashta.pdf}
}
\hspace{1cm}
\subfigure[F0 Contour of 18 Sof Pasuq] 
{
    \label{fig:sub:contour-18sofpasuq}
    \includegraphics[width=2cm,height=2cm]{contour-18sofpasuq.pdf}
}
\hspace{1cm}
\subfigure[F0 Contour of 11 Pashta Doubled]
{
    \label{fig:sub:contour-11pashta-doubled}
    \includegraphics[width=4cm,height=2cm]{contour-11pashta-doubled.pdf}
}
\caption{
F0 contours of 4 different gestures from a Torah recitation from
Hungary.  The first two show different versions of the pashta gesture
and the third shows the gesture for sof pasuq.  The last is a version
of the first pashta gesture stretched by two.
}
\end{figure*}

\begin{figure*}[t]
\centering
\subfigure[DTW of 11 Pashta vs 11 Pashta]
{
    \label{fig:sub:dtw-11pashta-11pashta}
    \includegraphics[width=2cm,height=2cm]{dtw-11pashta-11pashta.pdf}
}
\hspace{1cm}
\subfigure[DTW of 11 Pashta vs 42 Pashta]
{
    \label{fig:sub:dtw-11pashta-42pashta}
    \includegraphics[width=2cm,height=2cm]{dtw-11pashta-42pashta.pdf}
}
\hspace{1cm}
\subfigure[DTW of 11 Pashta vs 18 Sof Pasuq] 
{
    \label{fig:sub:dtw-11pashta-18sofpasuq}
    \includegraphics[width=2cm,height=2cm]{dtw-11pashta-18sofpasuq.pdf}
}
\hspace{1cm}
\subfigure[DTW of 11 Pashta vs 11 Pashta Doubled]
{
    \label{fig:sub:dtw-11pashta-11pashta-doubled}
    \includegraphics[width=4cm,height=2cm]{dtw-11pashta-11pashta-doubled.pdf}
}
\caption{Similarity Matrices of the above four gestures compared with
  the first pashta gesture. Superimposed on the figures is the
  DTW curve showing the alignment between the signs.}
\end{figure*}


\subsection{Dynamic Time Warping for Contour Similarity Calculation} 

One of the main aspects in the studying of signs in the context of
chant and recitation is to what extent they convey gesture information
that is invariant with respect to the underlying text. To study this
question it was necessary to develop a method to compare the pitch
contours of different realizations of the same sign. Dynamic Time
Warping (DTW) is a technique by which the similarity between two
different time sequences can be measured. It allows a computer to find
an optimal match between two sequences by performing a non-linear
warping of one sequence to the other. The technique of dynamic
programming is used for efficient implementation. An example of DTW in
Music Information Retrieval is comparing the tempo variations between
two different performances of a symphony. The DTW algorithm would
identify the parts of the two symphonies that were played at the same
tempo as a diagonal line, with the line varying above and below the
diagonal due to tempo variations.

First the similarity matrix between the two pitch contours is
calculated. The DTW algorithm finds the optimal alignment of the two
sequences and calculates the cost for that alignment. When the
contours are similar the alignment cost will be small compared to when
the contours are dissimilar. The matching process is pitch shift
invariant and allows variations and tempo stretching. That way for any
particular sign (pitch contour) we can sort the signs (pitch contours)
by similarity.

To illustrate the technique we use the gestures of two separate
annotated recordings of a section of the Torah. One of these was
recorded in Morocco, and the other was recorded in Hungary. Figures
~\ref{fig:sub:contour-11pashta}, ~\ref{fig:sub:contour-42pashta},
~\ref{fig:sub:contour-18sofpasuq} and
~\ref{fig:sub:contour-11pashta-doubled} show the F0 contour of the
sections of the audio file from a Torah recording from Hungary.
Figure ~\ref{fig:sub:contour-11pashta} shows a pashta sign, Figure
~\ref{fig:sub:contour-42pashta} shows another pashta sign from
further along in the audio file.  Figure
~\ref{fig:sub:contour-18sofpasuq} shows a sof pasuq gesture and Figure
~\ref{fig:sub:contour-11pashta-doubled} shows the first pashta
gesture, but with the sample stretched by a factor of two.

The figures
\ref{fig:sub:dtw-11pashta-11pashta},\ref{fig:sub:dtw-11pashta-42pashta}
, \ref{fig:sub:dtw-11pashta-18sofpasuq} and
\ref{fig:sub:dtw-11pashta-11pashta-doubled} show Similarity Matrices
and the alignment paths computed using DTW for these four gestures
compared to the first. White areas are highly similar and black areas
have low similarity. In Figure \ref{fig:sub:dtw-11pashta-11pashta} the
first pashta gesture is compared to itself.  The DTW curve is
overlayed in black and is basically a straight diagonal line from one
corner to the opposite corner showing direct alignment. Figure
\ref{fig:sub:dtw-11pashta-11pashta-doubled} shows a similar behavior,
except that the slope of the line is shallower. Figure
\ref{fig:sub:dtw-11pashta-42pashta} shows the comparison of one pashta
gesture to another.  This path had a DTW cost of 23.8442.  Figure
\ref{fig:sub:dtw-11pashta-18sofpasuq} shows an alignment between the
pashta gesture and a sofpasuq gesture.  One can see that the line is
not only not diagonal, but that the line is often on dark areas which
results in high alignment cost.



\begin{table} 
\begin{tabular}{|lr|lr|}
\hline
 Gesture   &  Average  &  Gesture   &   Average   \\
 (Hungary)   &  Precision   & (Morocco)   &   Precision       \\
    &   (Hungary)  &    &   (Morocco)       \\
\hline
 tipha     &    0.662  &  katon     &  0.453  \\
 pashta    &    0.647  &  mapah     &  0.347  \\
 mapah     &    0.641  &  tipha     &  0.303  \\
 katon     &    0.604  &  sofpasuq  &  0.285  \\
 etnachta  &    0.601  &  pashta    &  0.242  \\
 sofpasuq  &    0.591  &  merha     &  0.251  \\
 merha     &    0.537  &  etnachta  &  0.150  \\
 revia     &    0.372  &  zakef     &  0.125  \\
 zakef     &    0.201  &  revia     &  0.091  \\
 kadma     &    0.200  &  kadma     &  0.043  \\
\hline
\end{tabular}
\caption{Average precision for different signs}
\label{table:precisions}
\end{table}


Table ~\ref{table:precisions} shows the average precision for
particular signs for two recordings of the same excerpt from the Torah
- one from Hungary and one from Morocco. Each recordings contains
approximately 130 realizations of each sign with a total of 12 unique
signs. Two pitch contours are considered relevant to each other if
they are annotated by the same sign. For each ``query'' contour we
return a list of results which are the pitch contours sorted by the
alignment cost of the DTW. Average precision emphasizes returning more
relevant contours earlier. It is the average of precisions computed
after truncating the list of returned results after each of the
relevant documents in turn. Unlike traditional retrieval systems where
the mean average precision can be used to characterize the overall
system performance in our cases we are more interested in the
individual difference in precision among different signs. These
differences show which signs have well-defined gestural
characteristics and which signs are not interpreted
consistently. Ultimately the numbers are only meaningful after careful
interpretation by an expert. For example based on Table
~\ref{table:precisions} one can infer that the performer in the
Hungarian version had more consistent interpretations of the signs
than the performer in the Moroccan version.


\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm]{cbmi2009-cantillion}
\end{center}
\caption{
Web-based \emph{Flash} interface to allow users to listen to audio, and to
enable interactive querying of gesture contour diagrams.}
\label{fig:cantillion} 
\end{figure} 




\subsection{Cantillion interface} 

We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways
(\url{http://cantillation.sness.net}). Each recording is manually
segmented into the appropriate units for each chant type (such as
trope sign, neumes, semantic units, or words). The pitch contours of
these segments can be viewed at different levels of detail and
smoothness using a histogram-based method. The segments can also be
rearranged in a variety of ways both manually and automatically. The
audio analysis (pitch extraction and dynamic time warping) are
performed using the Marsyas audio processing framework
\footnote{\url{http://marsyas.sourceforge.net}} \cite{Marsyas}. 

The interface (Figure 7) has four main sections: a sound
player, a main window to display the pitch contours, a control window,
and a histogram window.  The sound player window displays a
spectrogram representation of the sound file with shuttle controls to
let the user choose the current playback position in the sound
file. The main window shows all the pitch contours for the song as
icons that can be repositioned automatically based on a variety of
sorting criteria, or alternatively can be manually positioned by the
user. The name of each segment (from the initial segmentation step)
appears above its F0 contour.

When an icon in the main F0 display window is clicked, the histogram
window shows a histogram of the distribution of quantized pitches in
the selected sign. Below this histogram is a slider to choose how many
of the largest histogram bins will be used to generate a simplified
contour representation of the F0 curve. In the limiting case of
selecting all histogram bins, the reduced curve is exactly the
quantized F0 curve. At lower values, only the histogram bins with the
most items are used to draw the reduced curve, which has the effect of
reducing the impact of outliers and providing a smoother
``abstract'' contour.  Shift-clicking selects multiple signs; in this
case the histogram window includes the data from all the selected
signs. We often select all segments with the same word, trope sign, or
neume; this causes the simplified contour representation to be
calculated using the sum of all the pitches found in that particular
sign, enhancing the quality of the simplified contour representation.
Figure 7 shows a screenshot of the browsing interface.

% The identity of chant formulae in oral/aural chant traditions is to a
% large extent determined by gesture/contour rather than by discrete
% pitches. Computational approaches assist with the analysis of these
% gestures/contours and enables the juxtaposition of multiple views at
% different levels of detail in a variety of analytical (paradigmatic
% and syntagmatic) contexts.  The possibilities for such complex
% analysis methods would be difficult if not impossible without such
% computer-assisted analysis. Employing these tools we hope to better
% understand the role of and interchange between melodic formulae in
% oral/aural and written chant cultures. While our present analysis
% investigates melodic formulae primarily in terms of their gestural
% content and semantic functionality, we hope that these methods might
% allow scholars to reach a better understanding of the historical
% development of melodic formulae within various chant traditions.

In the current work we implemented a mode that allows the researcher
to sort the samples based on the Dynamic Time Warping cost from one
sample to the other.  The interface allows the user to select an
arbitrary gesture from the interface, and then perform a sorting of
all other gestures to it.  In the example shown in Figure
~\ref{fig:cantillion} the user has chosen a ``revia'', and has sorted
all the other gestures based on their DTW-based alignment distance
from this first revia.  One can see that the gesture closest to this
revia is another revia gesture from a different section of the audio
file.


\section{Summary and discussion}

By combining the expert knowledge of our scientific collaborators with
new multimedia web-based tools in an agile development strategy, we
have been able to ask new questions that had previously been out of
reach. Chant research is a challenging domain where problem seeking is
important. Participatory design together with content-aware
visualizations and analysis tools can help researchers interact with
large collections of annotated audio recordings of chant in
interesting new ways. The integration of all the different components
in a single web-based interface is critical for an effective
system. Given the subjective interpretive nature of musicological
research each algorithm in isolation would be of little use. This
necessitates the development of the system as a whole and makes
evaluation harder. Ultimately we only have few experts users (one in
our case) and the only feedback we can receive is through them. By
including them in the design we having been able to create a system
that our expert finds useful and is willing to spend significant time
interacting with it. 

There are many directions for future work. We are planning to explore
the histogram-based contour simplification in conjunction with the
dynamic time warping alignment process to identify what is the
``optimal'' simplification of the pitch contours. More careful study
of the results by musicologists is also required. Making the system
available on the web can help collaborative approaches and reduce the
learning curve required for usage. We also hope to make the annotation
process part of the web interface and enable uploading of recordings
from researchers around the world.



\section{Acknowledgments}


We would like to thank Matt Wright for initial work and Emiru Tsunoo
for the implementation of dynamic time warping as well as the
Social Sciences and Humanities Research Council (SSHRC) of Canada for
financial support.


%------------------------------------------------------------------------- 
\bibliographystyle{cbmi2009}
\bibliography{cbmi2009gtzan}

\end{document}

\documentclass{chi2009}
\usepackage{times}
\usepackage{url}
\usepackage{graphics}
\usepackage{color}
\usepackage[pdftex]{hyperref}
\usepackage{subfigure}
\hypersetup{%
pdftitle={Your Title},
pdfauthor={Your Authors},
pdfkeywords={your keywords},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}
\newcommand{\comment}[1]{}
\definecolor{Orange}{rgb}{1,0.5,0}
\newcommand{\todo}[1]{\textsf{\textbf{\textcolor{Orange}{[[#1]]}}}}

\pagenumbering{arabic}  % Arabic page numbers for submission.  Remove this line to eliminate page numbers for the camera ready copy

\begin{document}
% to make various LaTeX processors do the right thing with page size
\special{papersize=8.5in,11in}
\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

% use this command to override the default ACM copyright statement 
% (e.g. for preprints). Remove for camera ready copy.
\toappear{Submitted for review to CHI 2010.}

\title{Content aware music discovery using \\ self-organizing tag clouds}
%% \numberofauthors{3}
%% \author{
%%   \alignauthor Steven R. Ness\\
%%     \affaddr{Department of Computer Science}\\
%%     \affaddr{University of Victoria}\\
%%     \email{sness@sness.net}
%%   \alignauthor Melissa Ness\\
%%     \affaddr{Vision Critical}\\
%%     \email{melissaness@gmail.com}
%%   \alignauthor George Tzanetakis\\
%%     \affaddr{Department of Computer Science}\\
%%     \affaddr{University of Victoria}\\
%%     \email{author2@b.com}
%% }

\numberofauthors{3}
\author{
  \alignauthor Author A\\
    \affaddr{Department of Computer Science}\\
    \affaddr{University of Somewhere}\\
    \email{a@somewhere.edu}
  \alignauthor Author B\\
    \affaddr{Department of Computer Science}\\
    \affaddr{University of Somewhere}\\
    \email{b@somewhere.edu}
  \alignauthor Author C\\
    \affaddr{Department of Computer Science}\\
    \affaddr{University of Somewhere}\\
    \email{c@somewhere.edu}
}

\maketitle

\begin{abstract}

  Personal digital music collections are continuously growing and
  frequently feature thousands of tracks. Browsing and navigating
  these large collections is challenging. The most common way of
  interaction is using textual meta-data such as artist name or
  genre. More recently tag folksonomies have also been utilized. A
  variety of visualizations based on automatically analyzed musical
  content have also been proposed.  Tag clouds are a two-dimensional
  stylized visual representation of a list of words with different
  visual design characteristics for each word. Tag clouds are commonly
  ordered either alphabetically or randomly, and in this paper we
  examine the utility and engageability of placing similar tags next to
  each other.  In order to determine similarity of tags we use a
  self-organizing map based on acoustical features derived from songs
  rather than using the more common tag co-occurrence to measure
  similarity. To evaluate the proposed approach a subset of the
  Magnatune database tagged using the Targeting game-with-a-purpose was
  utilized. Experimental results showing that using a self-organizing
  tag cloud is both faster and more fun are presented.


\end{abstract} 


\keywords{multimedia annotation, multimedia analysis, audio feature extraction, semi-automatic annotation, machine learning} 

\category{H.5.2}{Information Interfaces and Presentation}{User Interfaces, Miscellaneous}

\section{Introduction}


The size of personal digital audio collections has been steadily
increasing due to a combination of factors including digital music
distribution, increases in storage capacity, advances in audio
compression and the wide popularity of portable digital music players
and phones with music playing capabilities. Effective interaction with
these large audio collections poses significant challenges to
traditional user interfaces. Portable players and music management
software typically allow users to select artist, genres or individual
tracks by essentially browsing long sortable lists of text. This mode
of interaction, although adequate for small music collections, becomes
increasingly problematic as collections become larger. A variety of
alternative ways of browsing music collections have been proposed in
the emerging area of Music Information Retrieval (MIR) which deals
with all aspects of managing, analyzing, and organizing music in
digital formats. They typically rely on a combination of audio signal
analysis to automatically extract features that describe the musical
content followed by visualization techniques to map the
high-dimensional feature space to a 2D or 3D representation that can
be used for browsing and navigation.

Tagging-based systems rely on users for categorizing objects by means
of tags (freely chosen words). Tags are aggregated from many users
forming ``folksonomies'' which, although not as accurate as
well-designed ontologies, have the advantage of reflecting how users
perceive the data and how their vocabulary and perception evolve over
time. Tagging is simple and does not require a lot of thinking. Tags
form an essential part of personalized internet radio and music
community websites such as
Last.fm \footnote{\url{http://www.last.fm}}. Tag clouds are the most
common way of visualizing tags. They are two-dimensional stylized
visual representations of a list of words where the more prominent
words are typically assigned a larger font. They are useful for
quickly giving users the gist of a set of words. Tag clouds are in
common usage on a number of different social networks, including
Flickr \footnote{\url{http://www.flickr.com}} ,
del.icio.us \footnote{\url{http://dilicius.com}} and
wordle \footnote{\url{http://www.wordle.net}}, but trace their origins
back at least 90 years to Soviet Constructivist art
\cite{viegas08}. The first true example may be that of a psychological
experiment where participants were asked to create a collective mental
map of landmarks in Paris \cite{milgram76}.  Later, tag clouds were
featured prominently in the book \textit{Microserfs}\cite{coupland95},
and first were introduced onto the web by Jim Flanagan in the program
``Search Referral Zeitgeist''.  However, it was the use of tag clouds
on the popular photo sharing site Flickr that made their use
ubiquitous on Web 2.0 sites \cite{brusilovsky96}.  Today, many social
websites use tag clouds as a way to make large quantities of data more
accessible and as a friendly interface for users.

Interacting with large music collections like most information
retrieval tasks involves both querying (or direct search) in which the
user has a well-defined search goal in mind as well as browsing (or
indirect search) in which the goal is to explore, with some degree of
serendipity, an information space. Summarization is the ability to
extract the gist of a collection without going into
details. Interfaces based on long sortable lists of text are effective
for querying but provide little support for browsing and especially
for finding music by artists that are not known to the user. In
contrast, content-aware visualization-based interfaces can be quite
effective for browsing, and music discovery but have weak support for
direct searching. Tag clouds provide both an overview of the
information space as well as direct search support. In order to
satisfy all these possibly conflicting user information needs a
straightforward solution would be to provide all these three different
ways of interacting with a music collection as separate
views/interface components that are coupled. The disadvantage of such
a design is that the user interface becomes unnecessary complicated
and confusing.

In this paper, we present content-aware self-organizing tag clouds a
technique that attempts to support querying, browsing, and
summarization using the familiar information model of a tag cloud. Tag
clouds are commonly ordered either alphabetically or randomly. In some
cases, tag clouds are ordered based on clustering using some kind of
tag similarity metric such as tag co-occurance. In other applications,
like wordle, users position each word in a tag cloud by hand.  In this
paper, we examine the utility and engagedness of placing similar
(based on content not co-occurance) tags next to each other using the
Self-Organizing Map (SOM) \cite{kohonen95a} algorithm.  Specifically,
we use techniques from Music Information Retrieval (MIR) to extract
high-dimensional feature vectors characterizing each song, and then
use the SOM algorithm to map these high dimensional feature vectors
onto coordinates on a discrete 2D grid. The tags are then placed by
using the centroid of the 2D grid coordinates of each set of songs
associated with a particular tag. A final post-processing step using
force-directed placement is utilized for better visual appearance and
overlap removal.

A proof-of-concept implementation in the music collection browsing
domain is described. Most existing music browsing interfaces proposed
in the literature are prototype systems that have not been evaluated
with user studies. This can be partly attributed to their publication
in other fields in which user evaluation is not as important. However,
it is also caused by the challenge of evaluating such interfaces due
to the highly subjective nature of music similarity. We tried to
address some of these challenges in the design of our user study and
we hope that the insights gained will be of value for future
research. Evaluation of the proposed interface shows that
self-organizing tag clouds can result in more effective browsing
especially for the case of music by artists that are unfamiliar to the
user. This is supported by reporting both quantitative results as well
as discussing qualitative reactions to the interface. To close, we
discuss lessons learned and directions for future work.






\section{Background}

\subsection{Tagging and Tag Clouds}

Tagging systems allow users to add keywords, or tags, to resources
without relying on a controlled vocabulary \cite{john06} and have
become ubiquitous in web-based systems.  It has been observed that
they have the potential to enhance many types of online interaction
and because of their free-form nature, they take advantage of the
underlying and pre-existing social organization of web communities
\cite{marlow06}.  While controlled ontologies and taxonomies hold the
promise of providing a regular and well-defined structure for
organizing knowledge, in practice this taxonomic rigidity becomes too
heavyweight and can stifle input and collaboration from user
communities. Tag clouds are one of the most common methods of
visualizing tag information.

There has been considerable research in recent years into the design,
use and effectiveness of tag clouds.  The Dogear system
\cite{millen06}, uses tags to organize social bookmarks for large
enterprise organizations.  A system that uses tags in an eCommerce
application is described in Ganesan et al. \cite{ganesan08}, and has
the feature that it automatically mines tags from feedback and
presents the results in a visually appealing manner.  In Halvey and
Keane \cite{halvey07}, a variety of tag presentation techniques are
evaluated, and show that the use of different techniques can affect
the ease with which users can find tags.  A historical look at tag
clouds is presented in Viegas and Wattenburg \cite{viegas08}, which
looks at the development of tag clouds since their inception a decade
ago, and speculates about their development in the future.  A novel
way of determining the size of tags in a tag cloud by examining the
entropy of the tag, which is then related to the emotional impact of
the tag, is presented in Eda et al. \cite{eda09}.  Another study
examined tag clouds derived from Automatic Speech Recognition (ASR) as
surrogates for tag clouds generated by human listeners and found that
the ASR determined tag clouds perform equally well \cite{tsagkias08}.
In the paper ``Seeing things in clouds'' \cite{bateman08}, an
extensive evaluation of different types of visual features in tag
clouds, including font size, font weight, intensity, number of
characters and area were investigated, and while font size and font
weight had the largest impact, when multiple variables were changed at
once, no one property stood out amongst the others. Tag navigation in
general has been examined in detail with particular focus on
``last.fm'', an online social community for music \cite{mesnage09}.
The Qtag \cite{lee07} system investigates collaborative tagging in the
domains of Information Filtering and Information Retrieval and
presents a tag visualization model \cite{lee07}.  A context aware
browser for mobile devices that uses tag clouds is presented in
Mizzaro et al. \cite{mizzaro09}.

Music databases are typically very large, containing thousands to
millions of songs, and the number of users interested in listening to
and browsing music is similarly large.  The effectiveness of browsing
large scale social annotations has been examined using the Effective
Large Scale Annotation Browser (ELSABer) algorithm \cite{li07}.
Another paper describes the Topiography system, a visualization for
large scale tag cloud \cite{fujimura08}.

More artistic applications of tagging and tag clouds have also been
explored, one of these is ArsMeteo \cite{acotto09}, a Web 2.0 portal
that allows users to collect and share digital artworks, including
videos, pictures and music. Tag-based retrieval of video content has
been explored using a variety of tag sources including social tags,
professional metadata and automatically generated metadata
\cite{melenhorst08}. The paper ``Your place or mine?'' \cite{danis08}
explores the Many Eyes website, a place that allows for collaborative
visualization of datasets, and examines the themes that reoccur across
various scenarios of the use of the data in the visualizations.


There are a large number of papers that focus on the application of
various mathematical techniques to tagging in general, of which we
have chosen only two as a very small representative sample.  The
Folksoviz \cite{lee08} project uses a statistical method for deriving
subsumption relationships based on the frequency of tags in Wikipedia
texts and also uses the Tag Sense Disambiguation (TSD) method for
mapping each tag to a Wikipedia article.

An exploration of information seeking in the socio-semantic web shows
how items can be viewed semiotically depending on tags, topics and
points of view \cite{cahier07}.

\subsection{Music Collection Browsing Interfaces}


Currently the most common interfaces for browsing music collections
such as iTunes by Apple are based on long sortable lists of
text. Although effective for direct searching they provide limited
support for music discovery and exploration. In the field of Music
Information Retrieval, data of high dimensionality and of considerable
complexity is generated. Various visualization interfaces have been
proposed to make this data accessible and useful to users. Frequently
these interfaces rely on automatically extracted audio features.

Islands of Music \cite{pampalk03} is an example of such a
visualization of audio information which uses Self-Organizing Maps to
generate a two-dimensional representation of a collection of music.
MusiCream \cite{goto05musicream} is an interface that allows users to
interact with a music collection using a dynamic visualization
interface.  MusicRainbow \cite{goto06musicRainbow} is a similar system
that uses web-based labelling and audio similarity to visualize music
collections.  Another relevant system is MusicSun \cite{PampalkGoto07}
which combines three different similarity measures to generate music
recommendations for users.  The Databionic/MusicMiner system
\cite{morchen05} allows users to organize large collections of music
and employs Emergent Self-Organizing Maps to generate visualizations
of the data involved.  A very large web based system for helping users
find new music is part of the Last.fm website
\url{http://playground.last.fm/iom} which provides advanced
functionality for music recommendation and visualization based on a
self-organized map calculated solely based on tag data. A simplified
2D grid representation with no text support based on audio content
analysis has been proposed for assistive music browsing
\cite{tzanetakis09}.

Conceptually, the closest work to our approach is Salonen that also
describes tags clouds using self-organizing maps \cite{salonen07}. The
two main differences from our work are 1) tag instead of content
information is used for training the SOM 2) the lack of user
evaluation. A 2006 review of visualization in audio based music
information retrieval can be found in Cooper \cite{cooper06}. Examples
of visualizations for music discovery in commercial and research
systems can be found in the Visualizing Music
blog \footnote{\url{http://visualizingmusic.com/}}.


\subsection{Motivation and Design Goals} 

Music browsing and discovery in large digital collections is a
particularly interesting domain with unique challenges and
opportunities for interaction design. It is an activity that many
computer users engage daily. As the primary goal is entertainment in
many cases the user can be satisfied with little effort. For example
most portable music players feature a shuffle button that just plays
random songs from a collection. It is highly unlikely that the user of
a text search engine would be in any way satisfied with such random
retrieval. At the same time, the notion of music similarity is highly
personal and subjective compared to relevance in other fields of
information retrieval. Music from unfamiliar styles or cultures is
typically perceived as sounding all the same by listeners and the same
pair of tracks might be considered similar by one listener and
dissimilar by another. As in most information retrieval tasks there is
a need for both querying (direct search) and browsing (indirect
search). However, in browsing, listeners frequently have only a vague
idea of what they want to hear so the ability to quickly and
effectively explore a large information space and discover new music
by unfamiliar artists is important.

The technique of content-aware self-organizing maps, proposed in this
paper, evolved over experimentation with the ideas and techniques
presented in the previous subsections and informal feedback through
participatory design activities.  It can be viewed as a fusion of
concepts from text-based visualization interfaces and more abstract
content-aware visualization interfaces. In order to motivate the
design goals we briefly mention some of the issues that users raised
when experimenting with various different interfaces for browsing
large music collections. A rough classification of existing systems
along two dimensions will be used to illustrate these issues. An
additional simplification used throughout this paper is the use of the
word tags to denote any textual data associated with a particular
music track. For example the genre of a song, the year of release, or
the artist can be viewed as tags (albeit with some constraints such as
that a track can only be associated with a single artist). Existing
systems can be characterized either as tag-based (using the more
generalized tag definition) or content-based. In addition they can be
characterized either as simple or complex. Simple interfaces have
minimum requirements in terms of screen real-estate and can be
navigated using simple mouse or even just keyboard interaction. In
contrast complex interfaces require large screen real-estate and
require complex user interactions and gestures to control. Existing
systems are combinations of these extremes.

Traditional complex tag-based systems based on long lists of sortable
text such as iTunes provide very little support for browsing,
discovery and summarization. An alternative is visualization
interfaces that are based on automatic analysis of musical content. By
mapping the music collection onto a visual 2D or 3D representation
they enable quick browsing and navigation especially in the case of
music that is not known to the user or that has not been
tagged. Simple content-based interfaces typically only provide tag
data once a particular track is selected
\cite{pampalk03,tzanetakis09,morchen05}. User quickly learn a mental
map of the representation (such as the lower corner of the display
contains mostly fast, energetic rap songs) but have trouble
understanding the display (for example a frequent question might be
what is the meaning of the x-axis or how are the tracks placed). Some
of the proposed content-based interfaces can be visually complex (for
example displaying hundreds of dots representing songs in a 3D space)
and require complex interactions such as 3D rotation and zooming
\cite{goto05musicream,goto06musicRainbow}.  Even though such
interfaces make great demos they are frustrating to use
regularly. Adding text/tags on the visualization further increases
complexity.

Tag-clouds provide a simple, familiar interface that partly overcomes
these limitations. For example they support both direct searching as
well as browsing and navigation. However they come with their own
problems. In order for a tag to assist search or browsing it is
necessary for the user to have some notion of its meaning. For example
a specialized term such as indie pop might be completely unfamiliar to
a particular listener while at the same time essential to
another. This problem becomes even more acute using the more
generalized notion of tags that includes information such as artist or
album.  As one of the goals for an effective interface of music
collection browsing is the discovery of new music by artists not known
to the listener this is an important disadvantage. Simple tag clouds
also do not provide the user with any information about the
connections and similarity relations between tags. More sophisticated
approaches rely on analyzing and visualizing tag similarity calculated
based on co-occurrence relations. A final problem with any system
based solely on tag information is that there is no way to access
music tracks that have not been tagged (the so-called cold start
problem). In contrast content-based visualizations allow any track to
be accessed and do not require familiarity with the music explored.

Based on these considerations, we identified five distinct design goals for our music discovery interface: 
\begin{itemize} 
\itemsep -0.1cm

\item{{\bf Simplicity:} Both the visual display and the user
  interaction should be simple, straightforward and familiar to
  users. The design should not hinder implementation on small
  displays, touch surfaces or general accessibility by users with
  special needs. Both direct and indirect search should be supported
  by the same user actions. }

 \item{{\bf Discovery:} The interface should support browsing,
   discovery and exploration of music not familiar to the user without
   this support affecting direct searching and retrieval. Tracks that
   have not been tagged should be integrated and accessible.}

\item{{\bf Consistency:} Frequently multiple views of a music
  collection are desired. For example a listener might want to see all
  the artists in a particular collection or playlist as well as all
  the associated tags. Using existing techniques the tag clouds
  generated for these two views (facets) would have no relation to
  each other. Although clustering approaches that rely on tag
  similarity based on co-occurance provide a more semantically
  meaningful layout they can not provide layout consistency among
  different facets.}

\item{{\bf Disambiguation:} Typically there is no imposed structure or
  consistency in tagging especially in a highly subjective fields such
  as music listening. Polysemy and synonyms are well known problems of
  tagging. For example some music tracks might be labeled with the tag
  ``female voice'' and some with the tag ``woman singing'' even though
  they essentially refer to the same type of musical content. It is
  likely users will use one or the other therefore tag similarity
  based on co-occurance will not provide any help. Furthermore
  frequently tags might be completely unfamiliar to a user. Such
  disambiguation problems can be addressed If tags are placed based on
  analyzing musical content. For example a listener unfamiliar with
  the tag ``motet'' should be able to use neighboring tags such as
  ``classical'' or ``vocal music'' to infer the meaning.  }
\end{itemize} 

Using these goals as requirements we propose a hybrid of text-based
and content-based approaches to music collection browsing that we term
content-aware self-organizing tags. In the following sections we
describe the various processing steps required to create the
visualization interface and present a proof-of-concept prototype
implementation. There is a lack of empirical data offering insights
into the design of music discovery interfaces. We present the results
of a user study evaluating our proposed design and discuss some of the
methodological issues we had to deal with.



\begin{figure*}[t]
\centering
\subfigure[Feature Extraction]
{
    \label{fig:sub1:featureExtraction}
    \includegraphics[width=60mm]{featureExtraction.pdf}
}
\hspace{1cm}
\subfigure[Self-Organizing Map] 
{
    \label{fig:sub1:som_grid}
    \includegraphics[width=45mm]{som_grid.pdf}
}
\\
\subfigure[Self-Organizing Tag Cloud before MID]
{
    \label{fig:sub1:som_tag_cloud_before_msd}
    \includegraphics[width=60mm]{som_tag_cloud_before_msd.pdf}
}
\hspace{1cm}
\subfigure[SOTC After MSD]
{
    \label{fig:sub1:som_tag_cloud}
    \includegraphics[width=60mm]{som_tag_cloud.pdf}
}
\label{fig:fig1}
\caption{Processing stages for creating a content-aware self-organizing tag cloud} 
\end{figure*}





\section{System Description}

We describe a new method for organizing music tag clouds that makes a
persistent map that takes into account the musical similarity between
songs. Figure 1 shows the various stages of the process of creating a
content-aware self-organizing tag cloud. The first step
(Fig. ~\ref{fig:sub1:featureExtraction}) uses techniques from the
field of Music Information Retrieval (MIR) to calculate a
high-dimensional feature vector representation for each track in the
music collection. Once all the feature vectors are calculated each
track is mapped onto a discrete position on a 2D grid using a
Self-Organizing Map (Fig. ~\ref{fig:sub1:som_grid}). Each generalized
tag is associated with a set of tracks that have been annotated with
it. As the tracks have been mapped to feature vectors and subsequently
to 2D grid coordinates each tag can be associated with a set of 2D
grid coordinates. The self-organized map process ensures that
neighboring points (tracks) will have similar high-dimensional audio
features and therefore similar musical content. In the third step the
tags are placed on the centroids of their corresponding set of 2D grid
coordinates. Their placement will reflect the underlying musical
content but results in visual overlap between them
(Fig. ~\ref{fig:sub1:som_tag_cloud_before_msd}). The final step
(Fig. ~\ref{fig:sub1:som_tag_cloud}) is applying a force-based layout
drawing algorithm to reduce overlap and result in a more aesthetically
pleasing tag cloud.



\subsection{Music Feature Extraction}


The goal of audio feature extraction is to represent each song in a
music collection as a single vector of features that characterize
musical content. Using suitable features, songs that ``sound'' similar
should have vectors that are ``close'' in the high dimensional feature
space.  First low-level features such as the Spectral Centroid,
Rolloff, Flux and the Mel-Frequency Cepstral Coefficients (MFCC) that
summarize information about the sound are computed approximately every
20 milliseconds. To capture the feature dynamics we compute a running
mean and standard deviation over the past M frames (the so-called
``texture window'' typically around 1 second). Figure
~\ref{fig:sub1:featureExtraction} shows this process of audio feature
extraction.

% \begin{eqnarray}
%   m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
%   s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)]
% \end{eqnarray}

% \noindent 

% where $\Phi(t)$ is the original feature vector. 

The results is a feature vector of 32 dimensions at the same rate as
the original 16D feature vector. The sequence of feature vectors is
collapsed into a single feature vector representing the entire audio
clip by taking again the mean and standard deviation across the 30
seconds (of the sequence of dynamics features) resulting in the final
64D feature vector per audio clip. A more detailed description of the
features and their motivation can be found in Tzanetakis and Cook
\cite{TC02b}. For the calculation of the self-organizing map described
in the next section all features are normalized so that the minimum of
each feature across the music collection is 0 and the maximum value is
1. This feature set has shown state-of-the-art performance in audio
retrieval and classification tasks for example in the Music
Information Retrieval Evaluation Exchange (MIREX) 2008 and was
computed using the free Marsyas audio processing
framework \footnote{\url{http://marsyas.sness.net}}. Most audio
feature sets proposed exhibit similar performance so we expect that
any audio feature front end can be used.



\subsection{Self-Organizing Maps}

For creating the visualization layout we utilized the self-organizing
map (SOM) which is a type of neural network used to map a high
dimensional input feature space to a lower dimensional representation
while preserving the topology of the high dimensional feature
space. This facilitates both similarity quantization and visualization
simultaneously. The SOM was first documented in 1982 by T. Kohonen,
and since then, it has been applied to a wide variety of diverse
clustering tasks \cite{kohonen95a}. In our system the SOM is used to
map the audio features (64-dimensions) to two discrete coordinates on
a rectangular grid.

The traditional SOM consists of a 2D grid of neural nodes each
containing an $n$-dimensional vector, $ {\bf x(t)} $ of data. The goal
of learning in the SOM is to cause different neighbouring parts of the
network to respond similarly to certain input patterns. This is partly
motivated by how visual, auditory and other sensory information is
handled in separate parts of the cerebral cortex in the human
brain. The network must be fed a large number of example vectors that
represent, as closely as possible, the kinds of vectors expected
during mapping. The data associated with each node is initialized to
small random values before training. During training, a series of
$n$-dimensional vectors of sample data are added to the map.  The
``winning'' node of the map known as the {\it best matching unit}
(BMU) is found by computing the distance between the added training
vector and each of the nodes in the SOM. This distance is calculated
according to some pre-defined distance metric which in our case is the
standard Euclidean distance on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding nodes
reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. The time-varying
learning rate and neighborhood function allow the SOM to gradually
converge and form clusters at different granularities. Once a SOM has
been trained, data may be added to the map simply by locating the node
whose data is most similar to that of the presented sample,
\textit{i.e.} the winner.  The reorganization phase is omitted when
the SOM is not in the training mode.


The update formula for a neuron with representative vector N(t) can be
written as follows:
\begin{equation}
    {\bf N}(t + 1) = {\bf N}(t) + \Theta(v,t) \alpha(t) ({\bf x}(t) -
    {\bf N}(t))
\end{equation}

where $\alpha(t)$ is a monotonically decreasing learning coefficient
and $x(t)$ is the input vector.

The neighborhood function $\Theta(v,t)$ depends on the lattice
distance between the BMU and neuron v.

In our implementation, $\alpha(t)$ is a linearly-decaying function
with $t$.


\subsection{Self-organizing Tag Clouds}

Once the self-organized map of tracks is created each track is mapped
to a set of 2D coordinates $(x,y)$. The centroid of this set of 2D
coordinates is used as the position of the corresponding tag. To
generate the content-aware self-organized tag cloud we iterate over
each of the song in the collection and place it using the
centroid. Figure ~\ref{fig:sub1:som_grid} shows how the
self-organizing map can move tags using an underlying content
space. The top of the figure shows a random distribution of songs for
the tags ``Flute'' and ``Electronic''. As can be seen after the SOM
processing the songs corresponding to each tag are clustered although
there is some overlap (for example an electronic piece might contain
flute) and the corresponding tag positions are moved.

This initial layout contains many overlapping words, so the position
of each tag is repositioned using a mass, spring and damper
force-based algorithm for drawing \cite{malvern69}
\cite{zienkiewicz77} \cite{ellson01}.  In our implementation each tag
is anchored to its original position using a spring and an
electrostatic-like force is applied between every pair of tags that is
proportional to the inverse of their squared distance.  Therefore tags
that are close and overlapping will be pushed away while still trying
to remain close to their original location.  An additional wall force
term was added to keep all tags within the designated window. Figures
~\ref{fig:sub1:som_tag_cloud_before_msd},\ref{fig:sub1:som_tag_cloud}
shows the self-organizing tag cloud before and after applying the
Mass-Spring damper algorithm.


\begin{figure*}[htb]
\centering
\includegraphics[width=160mm]{playtagnow_interface}
\caption{Play Tag Now! Interface} 
\label{fig:playtagnow_interface} 
\end{figure*} 


In order to present this data to users, we designed an interactive
web-based interface utilized multiple facets and synchronized tags
clouds \cite{hernandez08}. Three views/panes that are coupled are
provided: 1) a track view 2) an artist view and 3) a tag view. The
interface is shown in Figure \ref{fig:playtagnow_interface}.  On the
right is the tag pane, and in this figure the tags are organized
according to the Self-Organizing Map.  In the center is the artist
pane, which shows the names of the artists that created the tracks,
and the songs are shown in the left hand pane.  Because there are many
more tags than can be shown in either the tag pane or the song pane,
only a small subset of the tags are displayed at any one time, and
above each pane is a ``Shake'' button which selects a different random
subset of tags to show the user. The display area is partitioned into
a 5x5 grid and tags in each subgrid are rotated during each ``Shake''.
In Figure \ref{fig:playtagnow_interface}, the user has clicked on the
``Flute'' tag in the tag window, which then displays all the songs
that have the tag ``Flute'' associated with them in orange.  To give
the user a feeling of the overall song organization, a subset of
tracks is shown in black in the track pane.


\section{Evaluation}



\begin{figure*}[t]
\centering
\subfigure[Random Tag Cloud]
{
    \label{fig:sub:random_tag_cloud}
    \includegraphics[width=45mm]{random_tag_cloud.pdf}
}
\hspace{1cm}
\subfigure[alphabetical tag cloud]
{
    \label{fig:sub:alphabetical_tag_cloud}
    \includegraphics[width=45mm]{alphabetical_tag_cloud.pdf}
}
\hspace{1cm}
\subfigure[SOM After MSD]
{
    \label{fig:sub:som_tag_cloudrock}
    \includegraphics[width=45mm]{som_tag_cloud.pdf}
}
\caption{Tag-cloud configurations}
\label{fig:configurations}
\end{figure*}


The goal of the evaluation was to compare the effectiveness of three
different ways of tag clouds visualization: random placement,
alphabetical layout, and the self-organized tag clouds based on music
content similarity proposed in this paper. Figure
~\ref{fig:configurations} shows these three configurations.

The evaluation of music browsing and discovery interfaces is
challenging. Frequently the effectiveness of the interface is
evaluated indirectly through specific tasks which tend to be more
related to directed search. Examples of such tasks include find a song
of a particular genre or artist or using the interface to create a
playlist. One methodological problem with such tasks is that the
stopping condition from the task is dictated by the experiment
designer and does not take into account individual patterns of
usage. Although the artificial nature of such tasks for evaluation is
to some extent unavoidable they can be designed to be more open and
flexible. Throughout our participatory design process and pilot study
we observed that users tend to have very different behaviors when
interacting with large collections. Some users are easily satisfied
with a quick match that approximately corresponds to what they are
looking for while others spend considerable time refining and
narrowing their search in order to find a much more tighter match.

In order to accommodate this variety in usage patterns we decided to
let the users specify the stopping criterion rather than the
experimenter. For all tasks the users were asked to indicate when they
locate a music track that was similar to the provided query according
to them. In contrast other user studies frequently ask the user to
keep looking for tracks until the ``correct'' answer is found and
measure the time to complete such task. In our opinion our approach
provides a more valid assessment of browsing effectiveness across
different usage patterns but has the unfortunate side effect of
largely varying task completion times among users. Therefore in order
to compare the three configuration across different users we
normalized the task completion times. For each task the ``slowest''
configuration was used to normalize the task completion times for a
particular user. As an example a user with task completion times $(A:
60, B: 14, C:30)$ in seconds would have normalized task completion
times of $(A:1, B:0.23, C:05) $ which can be interpreted that the time
to complete the task using configuration B was 0.23 faster than using
configuration A (or 14 seconds for configuration B compared to 60
seconds for configuration A).

When examining the results of the user study, we found that the
average time to complete the tasks varied widely between different
users.  The time to complete all the tasks averaged across all the
participants was 436 seconds with a standard deviation of 224.35
seconds.  The shortest time to complete all the tasks was 202 seconds,
and the longest time was 1162 seconds.  The times did not follow a
normal distribution curve.

Another issue we had to wrestle with was whether the configuration
used was known to the users. The problem was that unless the users
were aware of the underlying content-based mapping they would not
expect similar tags to be located near each other. Therefore we
decided that the users would see which configuration was used each
time. Given that the differences between each configuration are
obvious anyway we don't think that this choice affected our
results. We collected both quantitative data such as task completion
times as well as qualitative data. The following sections describe the
experimental setup and results.


\subsection{Experimental Setup}


Fourteen participants were recruited from graduate Computer Science
students.  Three were female and 11 were male. All subjects had normal
or corrected-to-normal vision, enjoyed listening to music and were
experienced computer users. None of the participants had previous
knowledge of the Magnatune dataset.


The Magnatagatune dataset is a new dataset for MIR applications that
contains a large collection of songs that has associated tags
generated by users. It contains songs from the Magnatunes record label
that aims to treat both artists and customers fairly by releasing
music under permissive licences.  Magnatunes made available a large
number of songs to the scientific community for use in research.  The
Tagatune game \cite{law09} is a new game-with-a-purpose
\cite{vonahn08} in which two users are both presented with a song.
Both users are then asked to guess what tag the other user would
select for this song, and if both users agree, the tag is added to the
song.  This tag is then not allowed to be used for this song by
subsequent pairs of players.  The Magnatagatune dataset contains over
25,000 songs and over 180 of the most common tags derived from the
Tagatune game and is freely available for
research. \footnote{\url{http://tagatune.org/Magnatagatune.html}} One
thousand clips were randomly chosen from the Magnatunes dataset and
were placed on a self-organizing map using music feature similarity.
These 1000 clips represented 278 songs by 24 artists and have 188
different tags.

Subjects were instructed in the use of the interface and were then
asked to first practice with the interface for 5-10 minutes.  Subjects
then performed each task in sequence.  During all tasks, subjects were
encouraged to speak aloud and their comments were recorded along with
the completion times for each task. After the experiments, users
filled in a 5-point System Usability Survey (SUS) and were
interviewed.



\subsection{Task 1}

In Task 1, subjects were asked the question: ``Play a classical music
track by clicking on the classical tag.''  After the corresponding tag
was clicked, a piece of classical music was played. The subjects were 
than asked to: ``Find another track that sounds
similar, according to you, using a different tag.'' and the time required 
was recorded. The mean and standard deviations of response times are detailed in
Table \ref{table:task1}.  From this table we can see that the SOM
condition had the lowest mean normalized time, of 0.48, which is
considerably better than the mean normalized times of either the
Random or Alphabetical conditions. A one-way between subject ANOVA was conducted. 
There was no significant effect of tag cloud configuration for this task (F(2,39)=2.66, p=0.0830$>$0.05).
This task is more representative of direct searching and therefore participants did not need 
to utilize the underlying representation. For example a user could locate the ``Baroque'' tag 
visually in any configuration and know that it probably contains similar tracks. 

\begin{table}
\centering
\caption{Task 1}
\begin{tabular}{ccc} \hline
Sorting & Mean & SE \\  \hline
Random  &  0.73 & 0.33\\
SOM &  0.48 & 0.34 \\
Alphabetical  & 0.73 & 0.32 \\ \hline
\end{tabular}
\label{table:task1}
\end{table}



\subsection{Task 2}

In Task 2, subjects were asked the question: ``Play Asteria by
clicking on Asteria in the artists pane.''  After the corresponding
tag was clicked, a piece of music by the artist Asteria was played. The subjects were asked to: 
``Find another track that sounds similar, according to you, using a different artist.'' and the response time was recorded. The subject was then asked to  ``Find another track that sounds similar, according to you, using a tag.''

The mean and standard deviations of response times are detailed in Table \ref{table:task2}.  From this table we can see that for Task 2a the SOM condition had the lowest mean normalized time, of 0.4, which is considerably better than the mean normalized times of either the Random or Alphabetical conditions.  A one-way between subjects ANOVA was conducted 
showing that this result was statistically significant (F(2,39)=12.38, p$<$0.001).  For Task 2b, the SOM condition also had the lowest mean normalized time (0.42), which was considerably better than the mean normalized times of either the Random or Alphabetical conditions.  We also found that this result was statistically significant (F(2,39)=3.56, p$<$0.05). This task was the most representative of browsing unfamiliar music as the participants had no knowledge of the artists involved. It also illustrates the importance of visual consistency in different facets. For example as can be seen in Figure ~\ref{fig:playtagnow_interface} the tags ``Monks'' and ``Opera'' are probably relevant for the artists 
``Asteria'' and ``Ensemble Sreteniye''. 

\begin{table}
\centering
\caption{Task 2}
\begin{tabular}{cccc} \hline
& Sorting & Mean & SE \\  \hline
Task 2a & &  \\ 
& Random  & 0.9  & 0.23 \\
& SOM & 0.4 & 0.28  \\
& Alphabetical & 0.53 & 0.31  \\ \hline
Task 2b & & & \\
& Random & 0.70 & 0.34 \\
& SOM & 0.42 &  0.24 \\
& Alphabetical & 0.69 & 0.34\\ \hline
\end{tabular}
\label{table:task2}
\end{table}

\subsection{Task 3}

In Task 3, subjects were asked the question: ``In the next exercise,
I'll ask you to find a song that you enjoy using the interface in any
way you like.''  The response time was intentionally not recorded. 
The subjects were then asked to  ``Find another song that sounds similar to your selection,
according to you in any way you want using the interface.'' as well as ``Find a song that
sounds very different to it, according to you.''. Both response times were recorded. 

The mean and standard deviations of response times are detailed in Table \ref{table:task3}.  From this table we can see that for Task 3a the SOM condition had the lowest mean normalized time, of 0.52, which is considerably better than the mean normalized times of either the Random or Alphabetical conditions. A one-way between subjects ANOVA was conducted showing no statistically significant difference (F(2,39)=1.04, p=0.3640$>$0.05).  For Task 2b, the SOM condition also had the lowest mean normalized time (0.42), which was considerably better than the mean normalized times of either the Random or Alphabetical conditions. This result was also not statistically significant (F(2,39)=2.71, p=0.0794$>$0.05). For similar reasons to the ones described in Task 1 this can be attributed to knowledge of tag semantics. At the same time it shows that there is no significant penalty in any of these tasks by using the self-organizing tag cloud. 

\begin{table}
\centering
\caption{Task 3}
\begin{tabular}{cccc} \hline
& Sorting & Mean & SE \\  \hline
Task 3a & & \\ 
& Random  & 0.68  & 0.33 \\
& SOM & 0.52 & 0.33 \\
& Alphabetical & 0.68  & 0.35 \\ \hline
Task 3b & & & \\
& Random  & 0.83 & 0.26 \\
& SOM & 0.60 & 0.29 \\
& Alphabetical & 0.65 & 0.31 \\ \hline
\end{tabular}
\label{table:task3}
\end{table}

\subsection{System Usability Survey}

After the conclusion of the three timed tasks, the participants were
asked to fill out a short System Usability Survey \cite{brooke96}
consisting of 6 questions, each rated on a 5 point scale, where ``1''
was labelled ``Strongly disagree'' and ``5'' was labelled ``Strongly
agree''. The 6 questions were as follows:


\begin{enumerate}
\itemsep -0.1cm
\item I thought the application was easy to use
\item I needed to learn a lot before I could accomplish tasks
with the application
\item I think people would need technical supported to be able
to learn how to use the application
\item I think most people would learn to user the application
very quickly
\item Overall, accomplishing tasks using the self-organizing map
was easier than with other methods
\item Overall, accomplishing tasks using the self-organizing map
was more fun than with other methods

\end{enumerate}

Results from survey  are detailed in Table
\ref{table:sus}. On average users rated Question 4 highest, which 
indicated that they thought most other people would be able to learn the application 
quickly. This question also had the lowest variance.  In Table \ref{table:sus} we
detail all the responses from the participants, and we can see that 2
participants chose the middle check box, 6 chose the next one to the
right, and 6 chose the checkbox labelled ``Strongly agree''.

In a similar vein, participants also rated questions 5 and 6 highly,
although notably, two participants rated this question as one box to
the right of ``Strongly Disagree''.  This shows that certain users
found our interface facile to use and fit in well with their
expectations of an interface to explore music collections, but for
other users it did not.  Different people enjoy different ways of
interacting with media, some are more spatially oriented, and others
prefer to have options presented in a linear form.  In subsequent
versions of this application, we would like to explore the possibility
of using different visual design strategies to make this an enclusive
environment for a wide community of users.

For Question 2, the average response was 1.85, which means that on
average, users mostly strongly disagree that they would have to learn
a lot before accomplishing tasks with this application.  It is
important to include negative examples on such a user study to ensure
that participants are not just choosing answers to questions randomly,
and this question performs this control function. 

\begin{table}
\centering
\caption{System Usability Survey}
\begin{tabular}{ccccccccc} 
\hline
Question & 1 & 2 & 3 & 4 & 5 & Mean & SE \\  \hline
\\ 
1 & 0 & 1 & 3 & 8 & 2 & 3.79  & 0.8   \\
2 & 5 & 7 & 1 & 1 & 0 & 1.86  & 0.86  \\
3 & 5 & 3 & 3 & 1 & 2 & 2.43  & 1.45  \\
4 & 0 & 0 & 2 & 6 & 6 & 4.29  & 0.73  \\
5 & 0 & 2 & 1 & 4 & 7 & 4.14  & 1.1   \\
6 & 0 & 2 & 0 & 6 & 6 & 4.14  & 1.03  \\
\\ \hline
\end{tabular}
\label{table:sus}
\end{table}

\subsection{Interview}

We also carried out an interview with all the participants after the
SUS survey.  The participants were first asked which of the three
conditions they felt took the least amount of time to complete, and
were then asked which of the three conditions they found most fun.  We
then asked the participants to feel free to give us feedback on the
software and algorithms.

Of the 14 users, 10 users felt the Self-Organized Map condition was
the fastest, 2 users felt the random condition was the fastest, 1
felt the alphabetical condition was the fastest, and 1 expressed no
preference.  When asked which condition was the most fun, 9 users felt
the Self-Organized Map condition was the most fun, 2 users felt the
random condition was the most fun, 1 felt the alphabetical condition
was the most fun, and 2 expressed no preference.










\section{Conclusions and Future Directions}

In this paper we describe our investigations in designing an interface for content-aware music browsing and discovery based on synchronized self-organizing tag clouds. The experimental results show that self-organizing tag clouds can result in more effective retrieval especially in the case of browsing unknown artists and relating different facets such as artists and tags. We also discuss evaluation issues for music browsing interfaces. The proposed interface provides a simple, consistent interface for music discovery that can easily be adapted to small screen real-estate and touch surfaces.

There are many directions for future work. We are planning to explore visualizing tag-based similarities as edges between tags with proportional thickness. Another interesting direction is the use of more complex layout algorithms that take into account the shape of words to approximate the aesthetic seen in manually created tag clouds. Several of the user study participants suggested using the same interface for tag annotation. Another interesting possibility is the use of self-organizing tag clouds for collaborative music browsing and comparison of collections between different listeners. 
Finally, although we focus on music browsing in this work, we hope the ideas in this paper can be applied to any application domain where the underlying objects that are tagged can be automatically analyzed based on their content.





\section{Acknowledgements}

We thank the funding agency XXXX and grant XXXX for providing the
funding necessary to do this research.  We would also like to thank
all the participants in this study for their time and valuable
comments.

\bibliographystyle{abbrv} \bibliography{chi2010gtzan}

\end{document}

% LocalWords:  Polysemy occurance Flickr assistive Salonen
Ôªø% CHI Extended Abstracts template.
% Tested with XeTeX on Mac OS X (Get it from http://tug.org/mactex)
% The latest version is available at <http://manas.tungare.name/software/latex/>
% 
% Filename: chi-ext.cls
% 
% CHANGELOG:
%   2010-10-18   Manas Tungare      Restored support for \figures.
%   2010-08-09   Manas Tungare      Updated copyright info for CHI 2011
%   2009-12-04   Stephen Voida      Updated copyright info for CHI 2010
%   2008-11-25   Manas Tungare      Initial create.
%   2009-11-17   Manas Tungare      Refactored the title & author sections.
% 
% LICENSE:
%   Public domain: You are free to do whatever you want with this template.
%   If you improve this in any way, please drop me a note <manas@tungare.name>,
%   so I can share the updates with everyone.
%   
%   PLEASE RECONSIDER BEFORE FORKING THIS TEMPLATE; there are already
%   several versions of the chiproceedings template for no good reason.
%   DO NOT REDISTRIBUTE THIS FILE UNDER A DIFFERENT FILENAME unless you
%   have a very good reason to change its name.
 
\documentclass{chi-ext}
 
\title{Teaching Children Computer Interaction with Robotics and Music}
 
\author{
 \textbf{Sarah Carruthers} \\
Depts. of Computer Science and Psychology \\
University of Victoria, BC Canada\\
 scarruth@uvic.ca \\
 \\
 \textbf{Steven Lonergan} \\
Department of Computer Science \\
 University of Victoria, BC Canada\\
 stevenl@uvic.ca \\
\\
 \textbf{Steven Ness} \\
 Department of Computer Science \\
 University of Victoria, BC Canada\\
 sness@uvic.ca \\
\\
 \textbf{Shawn Trail} \\
 Depts. of Computer Science, Electrical Engineering, Music\\
 University of Victoria, BC Canada\\
trl77@uvic.ca \\
\\
}
 
 
\keywords{Gestural Input, Pedagogy, Young Children, Music, Education, Robotics }
 
\acmclassification{H.5.2 Information Interfaces and Presentation: User interfaces ‚Äö
Evaluation/ methodology}
 
\copyrightinfo{
 Copyright is held by the author/owner(s). \\
 \emph{CHI 2011}, May 7--12, 2011, Vancouver, BC, Canada. \\
 ACM  978-1-4503-0268-5/11/05. \\
}
 
% Repeat author names (minus affiliations and addresses) and title here 
% for PDF metadata.
\hypersetup{
 pdfauthor={Sarah Carruthers, Steven Lonergan, Steven Ness, Shawn Trail},
 pdfkeywords={Gestural Input, Pedagogy, Young Children, Music, Education, Robotics },
 pdfsubject={Child Computer Interaction},
 pdftitle={Teaching Children Computer Interaction with Robotics and Music},
}
 
\begin{document}
\maketitle
 
\begin{multicols}{2}
 
\makeauthors
\makecopyright
 
\section{Abstract}
 
With rapidly evolving digital interfaces, it is important to investigate how children interact with computing devices, but also to teach children to take ownership of the evolution of computer interfaces.  This workshop presents an introduction to human computer interaction through a hands-on robotics and music workshop, designed specifically for young learners.  Considerable work has been done creating robotic versions of percussive instruments, some of the most recent and well publicized being on the recent ‚ÄúOrchestration‚Äù tour featuring 17-time Grammy winning guitarist Pat Metheny, featuring a variety of pitched melodic percussion instruments \footnote{http://www.patmetheny.com/orchestrioninfo/}.  Drawing on constructivist and constructionist theories, and tied tightly with math, music and science curricula, this workshop has the potential to provide children with a strong cognitive experiential connection to the technologies they interact with on a daily basis. 
\section{Keywords}
\makeatletter \@keywords \makeatother
 
\section{ACM Classification Keywords}
\makeatletter \@acmclassification \makeatother
 
%------------------------------------------------------------------------
 
\section{Introduction}
 
According to constructivist theory, individuals learn through interactions with their environment and each other. A teacher's role in this learning model is to encourage learners to question and wonder.  Papert's theory of constructionism extends this further to suggest that this learning is better achieved when learners actively interact with tangible objects in the real world \cite{Papert}.  Play and imagination are key components of learning \cite{resnick}, and flexible and dynamic learning technologies that support many different kinds of learning, like LEGO Mindstorms \cite{LEGO}, and provide a natural tool for introducing kids to interface design.  
 
In this workshop participants will design and build robots to play musical instruments, and  implement interfaces to modulate the robotic music using gestural input.  Melodic percussion instruments are amenable to interaction with robotic musicians because they sound by being struck, yet remain relatively stationary.  
 
%A variety of tunings can be used for melodic percussion instruments, though most western instruments are based on a chromatic scale, with 12 tones per octave.  In other cultures, however, tetratonic, pentatonic, heptatonic or diatonic scales are used, in some of which it is simpler to create chords of multiple notes that sound harmonious together.  Depending on the cultural embedding, many different types of melodic percussive instruments could be used.
 
The Orff method of music education can be extended to address modern musical scenarios that incorporate music technology.  Parallels can be made between the pervasive nature of music in ancient cultures, and computing in our culture.  By digitally extending melodic percussion to interface with computers, and vice-versa, the Orff concepts can be adapted to introduce sophisticated topics in music technology such as physical computing, HCI, DSP and robotically controlled instruments.
 
% in a way that is intuitive, innate, and inviting for children to approach and begin to understand and implement.  
 
In keeping with these theories and pedagogies, we describe a two-day workshop in which children learn the basics of computer human interaction by designing and developing their own robot-instrument and human-robot interfaces, while at the same time exploring music, math and the physical sciences.
 
\section{The Workshop}
The two-day workshop follows a cyclical model of: imagine, create, play and share \cite{resnick}.  Students are encouraged to explore, think about and analyse a problem, imagine a solution, create their solution (either physically with LEGOs or through programming), play with the product and then share their success with their peers and community.  
 
\subsection{Day 1:  Design I}
%steven How do humans play instruments? Get kids to draw 2D pictures, then use these to: %calculate angles, velocity etc - then Transfer of this to design ideas for Lego.
 
Our goal for the first day is to have the children explore questions like ‚ÄúHow do people play musical instruments?‚Äù \cite{dahl} . Students will analyze and model in 2D how an instrument is played, and annotate their diagrams with angles and velocities. The main goal of this stage is for the children to investigate how humans interact with musical instruments, and transfer this understanding to a 2D diagram \cite{boone}.
 
\subsection{Day 1: Engineering I}
%Steven L - -Get them to design it on paper and then take these ideas and put them into %concrete lego designs. Build actual motor components from designs.
At this stage participants will focus on taking the 2D figures produced in the design phase and realizing them as tangible LEGO constructions. This section will focus on questions like ‚ÄúHow can robots mimic humans?‚Äù The children will design robotic components so that they can interact with different instruments in a musically appropriate manner.  Participants are encouraged to incorporate a cyclical test and design model in this phase.
 
%pic of kids and robots
\subsection{Day 1:  Mini-performance}
% sness
%What music do we want to play?  Decide on a what sequence of actual physical movements are needed to generate tones, and then build some cases with Lego software.
%- specific instruments
%- what movements do we need to do to make tones
%- single tones, not phrases yet
%- by the end of the first day, have robots hooked up and making noise/music
 
%Different research groups have taken a variety of approaches to building music-playing robots, and the 
 
Participants will be encouraged to explore various approaches for building music-playing robots, tying into constructivist and constructionist theories of knowledge acquisition.  By the end of the first day they will have built, interfaced and explored the robots and instruments, giving participants the satisfaction of witnessing what they have learned and implemented thus far.  Performance allows them to hear what their peers have achieved, and thereby engage with their community \cite{resnick}.  
 
%The students in this workshop will be encouraged to explore different ideas for creating sound %and music using the Lego Mindstorms and melodic percussion instruments.  The children %then use these tangible artifacts to explore sonic landscapes and to create music, a %methodology supported by the constructionist viewpoint.  It is one thing to create a sound by %pressing a button on a computer, and another matter entirely when a physical object, like a %vibraphone bar, produces a sound.  
 
%At the end of the two day workshop, the students will present and perform a concert showcasing their achievements for their peers, family, and teachers. They will gaining invaluable physical and musical experiences designing and performing with the robots they created, while also growing closer from the collaborative aspect of creating collectively and developing confidence speaking publically about those experience.
 
\subsection{Day 2:  Music Theory}
% sness
%Intro to basics of music:  tones, rhythms, volume, and tempo.  Compose a piece of music for robots to play.
%- music instruments we‚Äôre actually going to play
%- composition - compose a little phase, interlocking phrases, rounds (‚ÄúIn C‚Äù and ‚ÄúSix Marimbas‚Äù)
 
%Music is an ancient art form in which sounds are organized by people, and p
 
Percussive rhythm is widely regarded as a natural form of human expression, and melodic percussive instruments are a logical choice for this type of workshop.  Students will investigate tone production, dynamics, melody, improvisation, and compositional arranging using melodic percussion instruments as a window into this subject.  By exploring musical concepts such as syncopation, polyrhythm, polymeter, and harmonic relationships students will gain a practical understanding of the mathematical concepts that underlie music.  
 
Students will be guided to create compositions suitable for the robotic mechanisms they are constructing.  One of the primary challenges in musical robotics is that of synchronization, that is, making sure that all the notes that should be sounding at a particular time are in fact being played together.  By introducing participants to the genre of Minimalism \cite{reich} \cite{riley} participants will see the possibility to create music that is complex and engaging, without the need for explicit synchronization. 
 
\subsection{Day 2:  Design II}
%stevenl Screen shot of programming here. Show how they can use the programming language here. 
%How can we interact with robotics to modify the music in real time?  We can change, pitch, tone, tempo, volume, velocity, reverse/forward, add DSP... Which sensors?
%- Mapping sensor values to musical ideas
 
The participants can now incorporate sensors to allow human interaction with the instruments. Questions that will drive this activity include ‚ÄúHow can we use the sensors of the LEGO Mindstorms to gather information from the user?‚Äù.  Touch, light, ultrasonic and colour sensors allow the children to design various interfaces, allowing them to bring the world around them to life. Research of this type is being pioneered at MIT‚Äôs Lifelong Kindergarten lab, with projects such as: Colour Code \cite{colour} which allow programming to be done using colours; Drawdio \cite{drawdio} which turns a drawing into a musical instrument; and Singing Fingers \cite{Rosenbaum} which brings finger painting and music together. 
 
%Not sure about this section here. Could be put on the chopping block! 
%At this stage the children will be expected to again map out on paper the outline of how their program should work and to outline what parts of the robot will use. This pseudo code can then be taking and translated into the Mindstorm program at the next stage. Because the students will be designing how they will control each motor on paper, they will be generating a mapping between sensors and motors which will be used in the final engineering stage to complete the interface. 
 
%Translate the composition into plan for programming.
%stevenl Screen shot of programming here. Show how they can use the programming language %here. 
%How can we interact with robotics to modify the music in real time?  We can change, pitch, %tone, tempo, volume... Which sensors?
%- Mapping sensor values to musical ideas
 
\subsection{Day 2:  Engineering II}
%steven L
%make sure sensors/motors are:  accessible, appropriate physical design/location
%limited resources, engineering principles
 
The final engineering stage will be driven by questions like ‚ÄúCan we build our interfaces so that they are accessible to every user?‚Äù. The children will use sensors to interface with the robotic components they have built. Students will test their designs to make sure they are robust and intuitive.  They will design their interfaces to work in the real world, incorporating engineering principles such as resource constraints and accessibility.  By the end of this phase, the users will be able to interact with the robots in real time to produce music.
 
%By the end of this phase, the participants‚Äô various robots will be able to respond in a musically appropriate context to user inputs.
 
 
%have robots that can accept user input in real time to modify music played.
 
%gestural input:  volume, velocity...
 
%tie sensor/motor to musical theory:  tempo, pitch etc. (mapping)
 
%steven L
\subsection{Day 2:  Performance}
Performance is a vital part of the process of music making, it allows participants to share their creation with their peers and community, in keeping with Resnick‚Äôs philosophy \cite{resnick}.  In addition, the use of the interactive multimodal interface controls in a real-world and synchronous performing environment teaches participants the intricacies of interface design.  
 
\section{Curriculum Map}
 
%\subsection{Music}
% sness
%music
%tone
%rhythm and rhythmic patterns
%tempo
%melody
 
Music instruction combines art and science, and helps students gain confidence and have fun while learning complex concepts.  In our workshop we use a variety of melodic percussion instruments, marimbas, consisting of wooden bars.  These types of instruments allow the participants to explore such concepts as tone, rhythm, tempo, melody and harmony in a collaborative and stimulating environment.  They also allow students to immediately engage without having to develop complex proprietary techniques. By immersing students in this tangible and tactile learning environment and by linking mathematical and musical theory we intend to enhance learning and retention.
 
Mathematics helps us understand, interpret and describe the world in which we live \cite{MathIRP}.  This workshop provides a way for participants to investigate how musicians interact with musical instruments, and translate this understanding to build robots to achieve the same goal. The British Columbia K-7 curriculum (Math IRP) identifies a number of components that are integral to mathematics curricula, this workshop supports the development of {\it Number Sense}, {\it Spatial Sense}, {\it Patterns}, and {\it Relationships} through the investigation of gestural and touch interfaces, music and mechanics of robotic design and creation.
 
This workshop maps into the Physical Science component of the K-7 science curriculum in British Columbia \cite{SciIRP}.  It provides hands-on opportunities for participants to explore: {\it Properties of Objects and Materials, Force and Motion, Properties of Matter, Materials and Structures, Light and Sound, and Forces and Simple Machines}.    It also supports the following scientific processes:  {\it Observing, Communicating, Interpreting Observations, Making Inferences, Questioning, Measuring and Reporting, and Scientific Problem Solving} \cite{SciIRP}.
 
\section{Conclusion}
In order to keep pace with evolving technologies, we must improve and adapt learning to not only include new technologies but also leverage the affordances they allow.  In this workshop we use LEGO Mindstorms to introduce young learners to the fundamentals of interface design and technologies like gestural input, response systems and real-time interactive controls.  By encouraging participants to explore these technologies from multiple perspectives: that of musician, designer, and engineer, the workshop is designed to facilitate deep learning \cite{serafin}.   
 
%Future work could incorporate mobile technology such as smartphones and tablets.  A vital %component of any pilot workshop like this is gathering data for future work.  Participants will %be asked to self-report on their experiences to inform the development of subsequent %workshops.
 
\bibliography{papers}
\bibliographystyle{abbrv}
 
\end{multicols}
 
\end{document}
% !sness!!Ôªø% CHI Extended Abstracts template.
% Tested with XeTeX on Mac OS X (Get it from http://tug.org/mactex)
% The latest version is available at <http://manas.tungare.name/software/latex/>
% 
% Filename: chi-ext.cls
% 
% CHANGELOG:
%   2010-10-18   Manas Tungare      Restored support for \figures.
%   2010-08-09   Manas Tungare      Updated copyright info for CHI 2011
%   2009-12-04   Stephen Voida      Updated copyright info for CHI 2010
%   2008-11-25   Manas Tungare      Initial create.
%   2009-11-17   Manas Tungare      Refactored the title & author sections.
% 
% LICENSE:
%   Public domain: You are free to do whatever you want with this template.
%   If you improve this in any way, please drop me a note <manas@tungare.name>,
%   so I can share the updates with everyone.
%   
%   PLEASE RECONSIDER BEFORE FORKING THIS TEMPLATE; there are already
%   several versions of the chiproceedings template for no good reason.
%   DO NOT REDISTRIBUTE THIS FILE UNDER A DIFFERENT FILENAME unless you
%   have a very good reason to change its name.
 
\documentclass{chi-ext}
 
\title{Teaching Children Computer Interaction with Robotics and Music}
 
\author{
 \textbf{Sarah Carruthers} \\
Depts. of Computer Science and Psychology \\
University of Victoria, BC Canada\\
 scarruth@uvic.ca \\
 \\
 \textbf{Steven Lonergan} \\
Department of Computer Science \\
 University of Victoria, BC Canada\\
 stevenl@uvic.ca \\
\\
 \textbf{Steven Ness} \\
 Department of Computer Science \\
 University of Victoria, BC Canada\\
 sness@uvic.ca \\
\\
 \textbf{Shawn Trail} \\
 Depts. of Computer Science, Electrical Engineering, Music\\
 University of Victoria, BC Canada\\
trl77@uvic.ca \\
\\
}
 
 
\keywords{Gestural Input, Pedagogy, Young Children, Music, Education, Robotics }
 
\acmclassification{H.5.2 Information Interfaces and Presentation: User interfaces ‚Äö
Evaluation/ methodology}
 
\copyrightinfo{
 Copyright is held by the author/owner(s). \\
 \emph{CHI 2011}, May 7--12, 2011, Vancouver, BC, Canada. \\
 ACM  978-1-4503-0268-5/11/05. \\
}
 
% Repeat author names (minus affiliations and addresses) and title here 
% for PDF metadata.
\hypersetup{
 pdfauthor={Sarah Carruthers, Steven Lonergan, Steven Ness, Shawn Trail},
 pdfkeywords={Gestural Input, Pedagogy, Young Children, Music, Education, Robotics },
 pdfsubject={Child Computer Interaction},
 pdftitle={Teaching Children Computer Interaction with Robotics and Music},
}
 
\begin{document}
\maketitle
 
\begin{multicols}{2}
 
\makeauthors
\makecopyright
 
\section{Abstract}
 
With rapidly evolving digital interfaces, it is important to
investigate how children interact with computing devices, but also to
teach children to take ownership of the evolution of computer
interfaces.  This workshop presents an introduction to human computer
interaction through a hands-on robotics and music workshop, designed
specifically for young learners.  Considerable work has been done
creating robotic versions of percussive instruments, some of the most
recent and well publicized being on the recent ``Orchestrion'' tour
featuring 17-time Grammy winning guitarist Pat Metheny, featuring a
variety of pitched melodic percussion
instruments \footnote{http://www.patmetheny.com/orchestrioninfo/}.
Drawing on constructivist and constructionist theories, and tied
tightly with math, music and science curricula, this workshop has the
potential to provide children with a strong cognitive experiential
connection to the technologies they interact with on a daily basis.

\section{Keywords}
\makeatletter \@keywords \makeatother
 
\section{ACM Classification Keywords}
\makeatletter \@acmclassification \makeatother
 
%------------------------------------------------------------------------
 
\section{Introduction}
 
According to constructivist theory, individuals learn through
interactions with their environment and each other. A teacher's role
in this learning model is to encourage learners to question and
wonder.  Papert's theory of constructionism extends this further to
suggest that this learning is better achieved when learners actively
interact with tangible objects in the real world \cite{Papert}.  Play
and imagination are key components of learning \cite{resnick}, and
flexible and dynamic learning technologies that support many different
kinds of learning, like LEGO Mindstorms \cite{LEGO} provide a natural
tool for introducing kids to interface design.
 
In this workshop participants will design and build robots to play
musical instruments, and implement interfaces to modulate the robotic
music using gestural input.  Melodic percussion instruments are
amenable to interaction with robotic musicians because they sound by
being struck, yet remain relatively stationary.

The Orff method of music education \cite{orff} can be extended to
address modern musical scenarios that incorporate music technology.
Parallels can be made between the pervasive nature of music in ancient
cultures, and computing in our culture.  By digitally extending
melodic percussion to interface with computers, and vice-versa, the
Orff concepts can be adapted to introduce sophisticated topics in
music technology such as physical computing, HCI, DSP and robotically
controlled instruments.
 
In keeping with these theories and pedagogies, we describe a two-day
workshop in which children learn the basics of computer human
interaction by designing and developing their own robot-instrument and
human-robot interfaces, while at the same time exploring music, math
and the physical sciences.
 
\section{The Workshop}

The two-day workshop follows a cyclical model of: imagine, create,
play and share \cite{resnick}.  Students are encouraged to explore,
think about and analyse a problem, imagine a solution, create their
solution (either physically with LEGOs or through programming), play
with the product and then share their success with their peers and
community.
 
\subsection{Day 1:  Design I}
 
Our goal for the first day is to have the children explore questions
like ``How do people play musical instruments?''\cite{dahl}  Students
will analyze and model in 2D how an instrument is played, and annotate
their diagrams with angles and velocities. The main goal of this stage
is for the children to investigate how humans interact with musical
instruments, and transfer this understanding to a 2D diagram
\cite{boone}.
 
\subsection{Day 1: Engineering I}

At this stage participants will focus on taking the 2D figures
produced in the design phase and realizing them as tangible LEGO
constructions. This section will focus on questions like ``How can
robots mimic humans?'' The children will design robotic components so
that they can interact with different instruments in a musically
appropriate manner.  Participants are encouraged to incorporate a
cyclical test and design model in this phase.
 
\subsection{Day 1:  Mini-performance}
 
Participants will be encouraged to explore various approaches for
building music-playing robots, tying into constructivist and
constructionist theories of knowledge acquisition.  By the end of the
first day they will have built, interfaced and explored the robots and
instruments, giving participants the satisfaction of witnessing what
they have learned and implemented thus far.  Performance allows them
to hear what their peers have achieved, and thereby engage with their
community\cite{resnick}.
  
\subsection{Day 2:  Music Theory}
 
Percussive rhythm is widely regarded as a natural form of human
expression, and melodic percussive instruments are a logical choice
for this type of workshop.  Students will investigate tone production,
dynamics, melody, improvisation, and compositional arranging using
melodic percussion instruments as a window into this subject.  By
exploring musical concepts such as syncopation, polyrhythm, polymeter,
and harmonic relationships students will gain a practical
understanding of the mathematical concepts that underlie music.
 
Students will be guided to create compositions suitable for the
robotic mechanisms they are constructing.  One of the primary
challenges in musical robotics is that of synchronization, that is,
making sure that all the notes that should be sounding at a particular
time are in fact being played together.  By introducing participants
to the genre of Minimalism \cite{riley} participants will
see the possibility to create music that is complex and engaging,
without the need for explicit synchronization.
 
\subsection{Day 2:  Design II}
 
The participants can now incorporate sensors to allow human
interaction with the instruments. Questions that will drive this
activity include ``How can we use the sensors of the LEGO Mindstorms to
gather information from the user?''.  Touch, light, ultrasonic and
colour sensors allow the children to design various interfaces,
allowing them to bring the world around them to life. Research of this
type is being pioneered at MIT‚Äôs Lifelong Kindergarten lab, with
projects such as: Colour Code \cite{colour} which allow programming to
be done using colours; Drawdio \cite{drawdio} which turns a drawing
into a musical instrument; and Singing Fingers \cite{Rosenbaum} which
brings finger painting and music together.
 
\subsection{Day 2:  Engineering II}
 
The final engineering stage will be driven by questions like ``Can we
build our interfaces so that they are accessible to every user?''. The
children will use sensors to interface with the robotic components
they have built. Students will test their designs to make sure they
are robust and intuitive.  They will design their interfaces to work
in the real world, incorporating engineering principles such as
resource constraints and accessibility.  By the end of this phase, the
users will be able to interact with the robots in real time to produce
music.
 
\subsection{Day 2: Performance}

Performance is vital to the process of music making, it allows
participants to share their creation with their peers and community,
in keeping with Resnick's philosophy \cite{resnick}.  In addition, the
use of the interactive multimodal interface controls in a real-world
and synchronous performance environment teaches participants the
intricacies of interface design.
 
\section{Curriculum Map}
 
 
Music instruction combines art and science, and helps students gain
confidence and have fun while learning complex concepts.  In our
workshop we use a variety of melodic percussion instruments.  These
types of instruments allow the participants to explore such concepts
as tone, rhythm, tempo, melody and harmony in a collaborative and
stimulating environment.  They also allow students to immediately
engage without having to develop complex proprietary techniques. By
immersing students in this tangible and tactile learning environment
and by linking mathematical and musical theory we intend to enhance
learning and retention.

Mathematics helps us understand, interpret and describe the world in
which we live \cite{MathIRP}.  This workshop provides a way for
participants to investigate how musicians interact with musical
instruments, and translate this understanding to build robots to
achieve the same goal. The British Columbia K-7 curriculum (Math IRP)
identifies a number of components that are integral to mathematics
curricula, this workshop supports the development of {\it Number
  Sense}, {\it Spatial Sense}, {\it Patterns}, and {\it Relationships}
through the investigation of gestural and touch interfaces, music and
mechanics of robotic design and creation.
 
This workshop maps into the Physical Science component of the K-7
science curriculum in British Columbia \cite{SciIRP}.  It provides
hands-on opportunities for participants to explore: {\it Properties of
  Objects and Materials, Force and Motion, Properties of Matter,
  Materials and Structures, Light and Sound, and Forces and Simple
  Machines}.  It also supports the following scientific processes:
{\it Observing, Communicating, Interpreting Observations, Making
  Inferences, Questioning, Measuring and Reporting, and Scientific
  Problem Solving} \cite{SciIRP}.
 
\section{Conclusion}

In order to keep pace with evolving technologies, we must improve and
adapt learning to not only include new technologies but also leverage
the affordances they allow.  In this workshop we use LEGO Mindstorms
to introduce young learners to the fundamentals of interface design
and technologies such as gestural input, response systems and real-time
interactive controls.  By encouraging participants to explore these
technologies from multiple perspectives: that of musician, designer,
and engineer, the workshop is designed to facilitate deep learning
\cite{serafin}.
 
\bibliography{chi2011sness}
\bibliographystyle{abbrv}
 
\end{multicols}
 
\end{document}
% !sness!!

% LocalWords:  IRP
%!TEX TS-program = xetex
%!TEX encoding = UTF-8 Unicode

% story.tex, with special characters converted to Unicode, and Zapfino font declarations
\nopagenumbers

\font\body="Zapfino" at 10pt \body
\font\title="Zapfino:Stylistic Variants=First variant glyph set" at 12pt
\font\author="Zapfino:Stylistic Variants=Second variant glyph set" at 10pt

\hrule
\vskip 1in
\centerline{\title A \ SHORT \ STORY}
\vskip 6pt
\centerline{\author by A. U. Thor}
\vskip .5cm
Once upon a time, in a distant galaxy called √ñ√∂√ß,
there lived a computer named R.~J. Drofnats.

Mr.~Drofnats‚Äîor ‚ÄúR. J.,‚Äù as he preferred to be called‚Äîwas happiest when he was at work
typesetting beautiful documents.
\vskip 1in
\hrule
\bye
\documentclass[letterpaper]{acm_proc_article-sp}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\usepackage{url} 

\begin{document}

\toappear{To be submitted for review to CSCW 2011}

\title{Distributed Cognition and Intelligence Augmentation for the
  solution of difficult Multidisciplinary problems}

\numberofauthors{1} 

\author{
\alignauthor
Steven R. Ness \\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{sness@sness.net}
}

\maketitle
\begin{abstract}
In recent years there have been numerous scientific endeavours that
are attempting to solve difficult problems that require the skills of
a large number of researchers from a variety of diverse domains.
Three of these include researchers studying fundamental theories of
physics using such equipment as the Large Hadron Collider, and
researchers in the field of genomics studying the genomes of organisms
as diverse as humans, plants and bacteria.  Our specific research
involves studying orca vocalizations, and in this paper we present a
strategy that uses a combination of distributed cognition,
intelligence augmentation and crowdsourcing to the study of this
problem.
\end{abstract}

\category{H.4}{Information Systems Applications}{Miscellaneous}
\keywords{distributed cognition, intelligence augmentation,
  crowdsourcing, machine learning, orca vocalizations}

\section{Introduction}

There has been a long and fruitful history of the application of
distributed cognition in the sciences.  If one adopts a loose
definition of distributed cognition as thought processes that happen
not just in one individual brain, but are instead distributed through
a community and are mediated by artifacts, the whole history of
science itself, even back as far as Aristotle, has been an example of
distributed cognition.  In the words of Isaac Newton:

 ``If I have seen further, it is by standing on the shoulder of
giants''.

However, recent advances in the understanding of the concept of
distributed cognition from researchers in the social sciences,
combined with advances in computer and communication technology could
dramatically increase the speed, facility and ease of communication
between scientists.  These increases could potentially be
revolutionary changes in certain very difficult problem domains.  In
such domains, the problem itself is too big to be solved by one single
researcher and must instead be solved by teams of researchers with the
aid of artifacts that aid cognition.

There are many different such problem domains that require
collaborations between large numbers of users, three such examples are
high energy physics, genomics research and the study of the
vocalizations of whales.

\subsection{High-Energy Physics}

In the field of particle physics, collaborative software has been used
for a number of years, and indeed, the World Wide Web (WWW) was
developed at CERN by Tim Berners-Lee \cite{bernerslee92}.  The WWW was
developed in order to help physicists collaborate with one another by
creating a ``global information universe''.  One of the primary goals
of the WWW was to enable the sharing of data between researchers.  The
Large Hadron Collider is a new multi-billion experiment at CERN, one
of the goals of which is to find evidence for the Higgs-Boson
\cite{wells09}.  The LHC will generate truly vast quantities of data
and will require enormous processing power \cite{wiebalck03}, because
of this, a huge network of computer systems called ``The Grid'' has
been developed.  This system allows researchers to distribute and
access data and to share computational resources at sites around the
world.  However, perhaps due to the culture in the high-energy physics
community, there has been only a small amount of development of tools
\cite{birnholtz03} to allow people to collaborate with other people
\cite{horn04}.  The Grid was designed to allow researchers to
efficiently access machines, but not for people to collaborate
efficiently with other people.  In an ideal system, these two
communication channels would both be facilitated.

\subsection{Human Genome}

Another area of research that involves large numbers of scientists
working together to solve a large problem was the sequencing of the
human genome \cite{venter01}.  The public effort to sequence the human
genome involved researchers from many different universities in
different countries, and required communication and collaboration
between all of these laboratories in order to coordinate sequencing
efforts.

The basic organizational structure was that the human genome was split
into large 100,000 base pair chunks called Bacterial Artificial
Chromosomes or BACs, which were then distributed to labs around the
world.  Each of these labs operated independently and sequenced the
BACs they had been allocated.  Concurrently a team at the University
of Washington at St. Louis built a global map of the genome which was
generated using physical sequencing techniques.  These techniques
operate at a much larger granularity than base pair sequencing, and
operate at the level of entire BACs.  Near the end of the project
scientists assembled the sequences from the labs around the world
using the global map from Washington University to generate the final
sequence of the human genome.

Some parts of this process required tightly coupled interactions, for
example, the task of assembling the final version was very tightly
coupled, for this task, most of the work was done by a small team of
scientists who were collocated at one university.  Communication
between members of this team was primarily through face-to-face and
email interactions.

Other aspects of this project were loosely coupled, the sequencing
efforts are an example of this.  To coordinate this task, the
researchers gathered together in one location for a meeting and
,through face-to-face interactions, decided on the global process of
how to distribute BACs to the various labs.  Subsequent interactions
were primarily mediated through email and telephone conversations.

In contrast to the particle physics work previously described,
communication between people was of primary importance in the public
human genome project.  Collaboration was a key part in this process,
and without it, no one university or even country would have had the
resources or political will to sequence the entire human genome.
However, perhaps due to the culture in molecular biology, computers
were primarily used to simply provide algorithmic support to the
assembly of sequences, and the ability of computers to mediate
communication and collaboration between researchers was limited to the
sending of data and emails back and forth.

One interesting aside is that there were in fact two efforts to
sequence the human genome, the public one that I have described here,
and a private one led by Craig Venter.  The private effort used a very
different sequencing strategy that was relied solely on sequence
alignment algorithms.  It was carried out in a single location by a
single team and required many fewer people and thus less
collaboration.  It also required less time than the strategy used by
the public effort.  It would be interesting to imagine what impact
collaborative software would have had on the public human genome
sequencing effort, and perhaps if more advanced collaboration software
had been available, the public human genome effort might have been
able to proceed faster than the less collaborative private effort.

A paper that describes the nature of communication and
collaboration in the biological sciences is ``Capturing and supporting
contexts for scientific data sharing via the biological sciences
collaboratory'' \cite{chin04}.  In this paper the authors talk about
the challenges in sharing data between scientists in the field of
biology.  Another paper \cite{tabard08} describes Prism, a hybrid
laboratory notebook that allows scientists to collaborate together in
the solution of biological problems by using a hybrid electronic
laboratory notebook.  This labbook brings together cross-linked
streams of data, both physical and electronic and integrates activity
streams from different users.  In a very recent paper \cite{segal09},
Segal discusses the challenges faced by software developers in
creating systems to allow scientists to collaborate.  She makes the
observation that it is important for the developers to internalize the
contextual structure with which the scientists communicate in order to
make an effective system.

An interesting paper by Kuchinsky et al. \cite{kuchinsky02} describes
a software tool for creating a synthesis of the complex concepts in
molecular biology, and uses the framework of storytelling to help
scientists make sense of data.  They find that the addition of a
narrative structure to data helps scientists organize, retrieve, share
and reuse biological data.  Most data is transmitted in a dense code
between biological scientists, and by many scientists in general, and
this paper highlights the fascinating idea of using the innate ability
of people to understand stories in order to transmit data between
scientists more efficiently.  We can transmit data from computer to
computer very quickly using codes that are natural to computers, and
it is important to remember that to transmit data to a person, one
should use methods of communication that are natural to people.

\subsection{Orca Vocalizations}

Another large and difficult problem domain that requires the skills of
researchers from many different fields is the investigation of orca
vocalizations.  The whale species \emph{Orcinus orca}, commonly known
as Killer Whales \cite{ford00_book_killer_whales}, are large toothed
whales found around the world, in places as far afield as Antarctica
and Alaska\cite{estes09_orca_alaska_decline}.  There are three
distinct types of Orcas, Transients, Residents and Offshores, each of
which have different feeding behaviours and different styles of
communication.  The vocalizations of orcas are complex and diverse,
and consist of a wide variety of vocalizations, which include
echolocation clicks, tonal whistles and pulsed calls \cite{deecke00}.

Around Vancouver Island there are two distinct communities of Orcas,
the Northern Residents, which have a range north of Campbell River,
and the Southern Residents, which spend time around Victoria.  Orcalab
is a research station located on Hanson Island, a small island up near
the top of Vancouver Island, directed by Dr. Paul Spong and Helena
Symonds and which studies the Northern Resident population of Orcas.
Orcalab has been recording Orca song for over 20 years, and has
amassed a huge archive of over 20,000 hours of audio on magnetic
cassette tapes.  In a collaboration with them, we are digitizing and
analyzing this huge and rich archive in a project called "The
Orchive".

In this paper, we will examine the combination of three components of
distributed cognition to help work on the problem of analyzing orca
vocalizations:

- Collaborative technologies to enable distributed cognition between
expert users in with potentially diverse domain knowledge

- The use of machine learning algorithms to provide technologies for
Intelligence Augmentation

- The use of large numbers of people from the general public in a
crowdsourcing methodology to mine through and annotate large databases
of information

Our goal is to create a system that enables distributed cognition
between these three facets, allowing expert users to collaborate,
giving them advanced machine learning algorithms to help them analyze
data, and in cases where datasets are enormous, using the power of
crowdsourcing to pore through large datasets.  In this paper we
describe an application of this approach to the problem of analyzing
the orca vocalizations.

\section{Related Work}

\subsection{Distributed Cognition}

The field of Distributed Cognition studies the social and
environmental aspects of cognition and realizes that thinking is not
just something that happens in isolated minds, but can also take place
in the interactions between minds and their environment.  It was first
formulated into a discrete discipline by Edwin Hutchins, which is
described extensively in his book ``Cognition in the Wild''
\cite{hutchins96} after his research into the native people of Papua
New Guinea and their system of public litigation \cite{hutchins80}.
In a classic paper by Hollan, Hutchins and Kirsh \cite{hollan00}
distributed cognition in different environments is described.  One of
these is on the bridge of a ship, a location where multiple persons
have to work together to accomplish many simultaneous and shared
goals.  In this situation, people work together, sharing information
via intentional and consequential communication and also offload
cognitive tasks to their environment by the use of maps, displays and
other physical objects.

The practice of Cognitive Ethnography \cite{hollan00} is often used to
study distributed cognition.  It studies the cognitive processes that
underlie the work that occurs in an environment, and takes into account
the effect of the social context of the participants as well as the
way the physical world affects the process.  Ethnography is a
methodology in which researchers study a system by making direct
observations in as natural a setting as possible \cite{mcgrath95}.

In the book ``Distributed cognitions: Psychological and educational
considerations'', \cite{salomon97} a number of authors investigate
distributed cognition, and give examples of this in a number of
fields, including daily life and education.  One central theme of this
book is that people think in conjunction or partnership with others
and use tools to help them think.  They also note that it has been
observed that the performance of people in teams supported by
computational tools is often superior to people working alone.  They
then investigate how distributed cognition can be harnessed in the
field of education, with the goal of helping students to learn more
effectively.


\subsection{Intelligence Augmentation}

The goal of early research in Artificial Intelligence (AI) was to
build independently intelligent agents that could perform cognitive
tasks.  There were a number of approaches to build such systems, some
researchers, such as John McCarthy, chose to build symbolic
manipulation systems where knowledge was represented by formal logic.
Others, such as efforts by Marvin Minsky, instead attempted to combine
several ad-hoc solutions to create an independent agent.  Most of
these systems failed to take into account that in the real world,
intelligent agents, such as people, are embodied, that is they have a
physical form, and this physical body interacts both with other agents
in social interactions as well as with its environment. 

In the 1980's instead of Artificial Intelligence, researchers instead
began to investigate Intelligence Augmentation \cite{fischer92}, where
instead of trying to build intelligent systems that were designed to
replace humans, researchers started building systems that would help
people to solve problems, combining the best of human and artificial
problem solving strategies.  These agents were designed to assist
humans and inherently carried within them the idea that intelligence
was embodied and interacts with people and other artificial agents
through social interactions.

A interesting study was carried out by Sumner \cite{sumner97} and
investigated embedding agents that functioned as critics into a design
environment.  In a critiquing approach, an artificial system
communicates in a dialog with a human, suggesting changes to places
where there are problems with the design.  The system described in
this paper is that of voice menus for a phone-based interface.  The
system is designed by a person or team, and the system critiques the
design in places where the designer has violated previously determined
rule-based constraints.  This work was extended in a subsequent
article \cite{fischer98a} where a system to help designers create
kitchens was described.  One interesting result from these studies was
that designers reacted negatively to being critiqued, and began to
anticipate the response of the system and took steps to avoid being
critiqued.  This highlights the importance of making sure the system
that is built makes the users feel that they are empowered and are
being supported by the system.

In a debate entitle ``Direct manipulation vs. interface agents''
\cite{schneiderman97}, Ben Schneiderman and Pattie Maes debate the
relative advantages of direct manipulation and software agents.
Direct manipulation is described to be user interface techniques where
large amounts of data are presented to the user on the screen at one
time.  Through a user interface with fast feedback (<100ms), the user
navigates through the data.  Software agents, on the other hand, are
described to be long-lived software systems that learn user
preferences and adapt to users, presenting them with data that the
system estimates will be useful to the user.  In this paper, these two
approaches are discussed and contrasted.  It is interesting to note
that by the end of the debate, both sides acknowledge benefits to the
others position, and indeed, as we have seen in the decade since this
debate, systems are being designed that support both direct
manipulation and software agents.  In The Orchive, we are attempting
to design a system that supports both of these paradigms, allowing the
user to directly manipulate large quantities of data, and also provide
software agents that can mediate interactions between users, other
users, annotations from other software agents, and the data.

In the paper ``Distributed intelligence: extending the power of the
unaided, individual human mind'', \cite{fischer06} Fischer provides an
excellent overview of the field of distributed intelligence, relating
it back to work in distributed cognition by Hollan et. al
\cite{hollan00} as well as tying in other concepts, such as those of
Intelligence Augmentation, advanced visualization interfaces that
exploit the power of the human visual system, making user specific
systems, and exploring computing off the desktop, which includes both
large scale and small scale displays.  This paper also explores the
difference between ``Tools for Learning'', which have the goal of
educating the user so that they will be independent of the system and
``Tools for Living'' that help people do things they could not do
themselves.  In The Orchive, we are developing tools to support both
these paradigms, first, to help users learn the calls used by the
orcas, and second, to help more advanced users interact with the data,
allowing them to ask and answer research questions. 

In the paper ``Collaborative, programmable intelligent agents''
\cite{nardi98}, a system that extracts semantic data from documents
using intelligent agents is described.  This paper talks about the
difficulty of creating systems that approximate real intelligence and
instead proposes that that we should investigate what tasks computers
are good for, what tasks people are good for, and how they can best
complement each other.  Their goal is to create a system that works
like a reference librarian, taking imprecise requests from clients and
returning relevant results.  They then present a system called ``Apple
Data Detectors'', which uses a collaborative system to work with the
user to fulfill their requests. 
 
\subsection{Crowdsourcing}

Crowdsourcing is a new type of collaboration where non-specialists
help expert scientists \cite{howe08_crowdsourcing} and has been used
to great advantage \cite{surowiecki05_crowdsourcing} in a number
\cite{bradham08_crowdsourcing} of research programs
\cite{travis08_crowdsourcing}.  One of the most successful such
programs is Galaxy Zoo \cite{anze08_galaxyzoo}.  In this project,
astronomers had collected images of many thousands of galaxies and
wanted to characterize them by the chiral handedness of their spiral
structure, that is, were they spinning clockwise or counterclockwise?
It was assumed that there would be an even distribution of these the
two chiral hands, left and right, and any deviations from this would
be an important and surprising result.  Even the most advanced current
computer algorithms are not able to categorize galaxies based on their
handedness, but humans can do this classification easily.  In this
paper, the authors describe their system and results, and present
results that indicate that there is a hint of positive correlation for
galaxies nearer than 0.5 Megaparsecs.

Another research program that benefited from crowdsourcing was the
Stardust@home project \cite{mendez06_stardust}.  Stardust
\cite{atkins97_stardust} was a NASA space mission that flew a
spacecraft through the tail of comet Wild 2, collected dust from the
comet and from interstellar matter using an aerogel, and then returned
the satellite to earth.  Comet dust was collected on one side of the
aerogel, and the other side of the aerogel was exposed to interstellar
matter during the entire mission.  The analysis of the particles from
the comet were straightforward due to the high number of particles,
but the analysis of interstellar grains was much more difficult due to
the small size and number of particles.  The Stardust@home project
allowed users from around the world to signup on a website and
interactively view and annotate microscopic images of the aerogel.
There was overwhelming participation by the public, and they were able
to generate results that were useful to the scientists on the project
\cite{atkins97_stardust}.

There have been a number of articles that investigate the benefits of
crowdsourcing.  Hong \cite{hong04_crowdsourcing} presents results that
show that a group of problem solvers with a diverse background can
outperform smaller groups of experts.  This is of interest in the
current work because there are only a very small number of orca
vocalization experts in the world, but there is a large number of
people who are interested in listening to orca calls, as is evidenced
by the traffic on the Orca Live \url{http://orcalive.net} forums.

A recent article \cite{kittur08_crowdsourcing} describes crowdsourcing
with the Amazon Mechanical Turk system, a web based system where
people can sign up to work on small tasks in return for micropayments.
The advantage with using the Mechanical Turk system is that because
people are paid for their work and have to pass a scientist defined
test, the results obtained might be of higher quality.  The drawback
to this system is that the workers must be paid.  Most scientific
projects have limited budgets, and thus this type of solution is often
not practical.

One exciting new development in crowdsourcing is games-with-a-purpose
(GWAP) \cite{vonahn08}, which are computer games that harness the
ability of people to solve tasks in a game setting.  The first GWAP
was the ESP game \cite{vonahn04} in which two users on computers
connected to the internet try to guess words that describe an image.
If both users guess the same word, this word is then added to the tags
to that image, and is also added to a list of forbidden words which
forces people to choose new words to describe an image.  This game has
proven immensely popular, with many millions of tags added to images.
Another such game is Tag-A-Tune \cite{law09}, an input agreement game
that has people add tags to music clips.  These games, and others like
them, exploit the powerful reward system of the human limbic system
and are an excellent way to hold the attention of large numbers of
users and generate high quality data.  The challenge with
games-with-a-purpose is to develop a captivating game experience, but
with time and skill, they perhaps hold the largest potential in terms
of sheer size of population for performing crowdsourcing.

\section{Application - The Orchive}

\subsection{Relevance of this work}

An important aspect in the design of a tool to support collaborative
work is to consider what user communities will use the tool.  In the
case of the Orchive, there are a number of different scientific
communities that will be using this tool and the data this tool
provides access to.  The primary scientific community that will
benefit from this work will be cetacean biologists.  In order to study
the rich archive orca vocalizations that have been recorded by
Orcalab, researchers must currently travel to Hanson Island, search
through the lab books and incidence reports to find which recordings
contain the data they are interested in, locate the physical cassette
tape corresponding to this recording, and then either manually listen
to the tape, or perhaps digitize the tape and analyze it in the
computer.  Each researcher typically then keeps the annotations and
data generated from this procedure themselves, if future researchers
want to obtain this data for further analysis, they must first be
aware of the fact that this researcher has the data, and then request
it from them.  With the distributed collaborative system we have
designed, not only can these biologists easily listen to any recording
in the entire archive from any internet connected computer in the
world, and compare different recordings, they can also add their
annotations to the system.  These annotations can be either private or
public, if they are for use in a publication, after the article has
been accepted for publication, the researcher can make their private
annotations public.  These researchers are less interested in the
details of audio feature extraction and machine learning algorithms,
and are instead more focused on asking biologically informed
questions, like dialect change in cetacean call repertoire
\cite{deecke00}.

Another scientific community that will receive benefits from this
archive are the developers of bioacoustic algorithms.  These
scientists are typically computer scientists with interests in Music
Information Retrieval and bioacoustics.  This archive represents a
site where researchers can get large amounts of high quality and
uniformly collected data.  Researchers interested in bioacoustic
algorithms have different goals and skill sets from cetacean
biologists, for example, many have extensive knowledge of Digital
Signal Processing and audio feature extraction algorithms.  This
system should be flexible and powerful enough to allow these
researchers to ask questions that are relevant to them.  The required
features for this group of users include allowing them to choose
different audio feature extraction algorithms, and to then take the
resulting data and run it against a variety of machine learning
algorithms in as flexible a manner as possible.

Another group of scientists that have expressed interest in the
Orchive are Environmental and Conservation scientists. A research
question particular interest is the effect of boat noise
\cite{foote04_orca_boat_noise} on cetaceans and on the marine
environment in general.  For these researchers, the data they will be
most interested in is the frequency and nature of orca vocalizations,
and the intensity and spectral characteristics of boat noise
\cite{holt09_orca_speaking_up}.  There are large differences in the
intensity and frequency content of boat noise depending on the type of
boat that creates it, speed pleasure craft often create a high pitched
noise that quickly moves away, tug boats have a lower pitched sound
and take a long time to move through an area, and cruise ships make a
loud and distinctively high pitched sound.  Analyzing the effects of
these various types of boat noise will help researchers to establish
guidelines for boat noise as it affects this sensitive population of
marine mammals \cite{doksaeter09_orca_herring_feeding}.

Another group of scientists that this work will benefit are those
studying the social organization of whale communities
\cite{bigg90_orca_genealogy}.  There have been studies that
investigate the transmission of culture in orca societies
\cite{deecke00} and have found evidence of this through the
examination of dialect change.  In a similar vein, other studies have
investigated social learning \cite{janik00_orca_social_communication}
in communities of orcas.  With a large database such as this, more of
these type of studies will be made possible in the future.

\subsection{The Orchive}

In the current work, we present a web-based collaborative system to
assist with the task of identifying and annotating the sections of
these audio recordings that contain Orca vocalizations.  This system
consists of a dynamic and user-informed front end written in
\emph{XHTML/CSS} and \emph{Flash} which lets a researcher identify and
label sections of audio as Orca vocalization, voice-over or background
noise. By using annotation boot-strapping \cite{tzanetakis04}, an
approach inspired by semi-supervised learning, we show that it is
possible to obtain good classification results while annotating only a
small subset of the data. This is critical as it would take several
human years to fully annotate the entire archive, a daunting task even
with the use of crowdsourcing to help annotate the archive.  Once the
data is annotated it will be easier to focus on data of interest such
as all the orca vocalizations for a particular year without having to
manually search through the audio file to find the corresponding
relevant sections.

The system that we are building combines ideas from the fields of
distributed cognition, intelligence augmentation and crowdsourcing
into a single system that allows researchers to investigate this huge
collection of data easily and efficiently.  The main Orchive interface
is shown in Figure ~\ref{fig:orchive_full}.

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=160mm]{orchive_full}
\end{center}
\caption{ An annotated region of audio from the \emph{Orchive}, with
  orca vocalization and background annotations shown.  Also shown in
  green are the predictions of orca vocalizations by a Machine
  Learning algorithm.  At the bottom left is shown controls to display
annotations by different users, and an Incidence Report which gives
data on which individuals were observed on this day.  On the bottom
right is the lab book entry for this recording. }
\label{fig:orchive_full} 
\end{figure*} 

This system supports distributed cognition by providing awareness
support of what annotations other people have entered into the system
by showing a list of all the users that have annotated a particular
recording.  Users are allowed to edit only their own annotations, but
can see annotations from all other users.  The interface to this is
shown in the middle on the left of Figure ~\ref{fig:orchive_full}.
This figure also shows annotations by the ``anniebot'' user, a
software agent that makes independent annotations of a region of audio
using machine learning algorithms.  Software agents are designed to be
used primarily by the cetacean biologist community and wrap up
advanced machine learning interfaces in the form of an agent that can
interact with users using a social interaction modality.

We realized that in addition to these software agents, some users of
the Orchive would be advanced researchers in bioacoustics.  These
researchers would have different goals and skill sets from the
biologists, and would want to interact directly with the feature
extraction and machine learning algorithms.  We allow these users to
make predictions directly with these algorithms and then visualize the
results.  An example of these is shown in Figure ~\ref{fig:orchive_detail}.

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=160mm]{orchive_detail}
\end{center}
\caption{ A recording from 1993 showing annotations for three
  different calls, N4, N47, and N7.  Predictions for N4 are shown in
  purple, for N47 in cyan and for N7 in yellow.  As can be seen the
  system predicts these regions with quite good accuracy.
 }
\label{fig:orchive_detail} 
\end{figure*} 

Crowdsourcing and games-with-a-purpose are planned additions to the
Orchive website, and we are in active development of an interface to
provide for a compelling and enticing interface to allow people from
the general public to help us annotate this huge collection of audio.
In addition, the games that we are designing will be used to help
train people to learn orca calls, a task that has traditionally taken
much time and effort.  By framing this task in terms of a fun game, we
hope to speed and enhance the learning process.


\subsection{Evaluation}

It is vitally important to evaluate the system that we have created
with the actual researchers who will be using this system.  One could
imagine using a variety of methods to evaluate this system, including
Laboratory Experiments, Sample Surveys and Field Study.  

We plan to execute a laboratory experiment to compare the amount of
time it takes expert users to annotate a recording using three
methods, with cassette tapes, with a standard audio editing program
such as Audacity and using the web based system that we have developed
for the Orchive.  The experiment would consist of taking users that
have experience with annotating orca vocalizations and would time how
long it takes these users to annotate a full tape of recordings using
these three methods.  In addition, we would measure the accuracy of
the annotations, both in terms of how many annotations were correctly
chosen, and the accuracy of the start and end times of these
recordings.  We hypothesize that using computer based methods will be
much faster and more accurate than using a cassette tape, as with the
computer it is easier to skip over sections of tape that have no orca
vocalizations.  We also hypothesize that users will be able to
annotate recordings faster using the Orchive interface than using
Audacity, as the Orchive interface is designed specifically for the
purpose of annotation.

After the completion of this laboratory experiment, we propose to use
a survey to ask the users about their experience with all three
modalities of interaction with the recordings.  We will ask the users
questions such as ``Which of the interfaces did you think was the
fastest?'', and will compare this responses with the actual time it
took the users to complete the tasks.  We will also ask them about the
perceived accuracy and ease of use of the various interfaces.  We will
also ask them to write down their experiences and give them the
opportunity to let us know what features they found would be good to
add to the Orchive.

We have completed a very small Ethnographic based Field Study in which
several cetacean biologists made annotations using the Orchive
interface.  In this study, we observed 5 different scientists of
varying levels of expertise, two of these were researchers with over
40 years of experience with orca vocalizations, one was a
post-graduate researcher who had done her Ph.D. studying orca
vocalizations, one was a M.Sc. student who had not studied orca
vocalizations before, but had studied pilot whales, and one was an
undergraduate student who had extensive experience with visually
observing whales.

The results from this field study were very interesting and
encouraging.  One of the respondents said:

``I wish I had this when I was doing my Ph.D.  It would have made my
research much easier''

another said:

``This makes it possible to finally access all this data easily.''

When shown the predictions from machine learning, one of the
researchers became quite excited and said:

``Wow!  It can tell the difference between the different calls!''

One respondent had concerns that she would not know if the people who
were making the annotations would really know what they were doing.
This same sentiment was shared by most of the other respondents.
Another discussion ensued about inter-observer reliability, the quality
of annotations, and to know who had made which annotation, this
highlighted the importance for reputation management systems to be
implemented in this application.

Another topic that the subjects talked about was the importance for
being able to temporarily hide data that would be used in an upcoming
paper, so that other researchers would not be able to scoop them in
the race to publication in scholarly journals.  They said that:

``I want to share data with people in my lab, but not with people in
other labs, at least until I publish''

Another comment that was made was:

``I want to be able to write my field notes and have them attached to
a recording, but also be able to see all of them at once''

We must note that our Field Study borders on Critical Theory, because
we are not just interested in studying these researchers, but in
actively providing them tools to help them in their research.  For
this reason, it is difficult to remain completely objective about this
research.  In our opinion, it would be useful to have independent
field studies carried out by dispassionate researchers.


\section{Acknowledgements}

We would like to thank Dr. Paul Spong and Helena Symonds for their
generous contribution of time and resources to the Orchive.  We would
also like to thank all the participants in the Field Study for their
time and insightful comments.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{cscw2011sness}  % sigproc.bib is the name of the Bibliography in this case
\end{document}
\documentclass{chi2009}
\usepackage{times}
\usepackage{url}
\usepackage{graphics}
\usepackage{color}
\usepackage[pdftex]{hyperref}
\hypersetup{%
pdftitle={Citizen science with serious casual games : Talking to Orcas},
pdfauthor={Steven Ness, George Tzanetakis},
pdfkeywords={your keywords},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}
\newcommand{\comment}[1]{}
\definecolor{Orange}{rgb}{1,0.5,0}
\newcommand{\todo}[1]{\textsf{\textbf{\textcolor{Orange}{[[#1]]}}}}

\pagenumbering{arabic}  % Arabic page numbers for submission.  Remove this line to eliminate page numbers for the camera ready copy

\begin{document}
% to make various LaTeX processors do the right thing with page size
\special{papersize=8.5in,11in}
\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

% use this command to override the default ACM copyright statement 
% (e.g. for preprints). Remove for camera ready copy.
\toappear{Submitted for review to CHI 2009.}

\title{Citizen science with serious casual games : Orca Talk and The Orchive}
\numberofauthors{2}
\author{
  \alignauthor Steven Ness\\
    \affaddr{Department of Computer Science}\\
    \affaddr{University of Victoria}\\
    \email{sness@sness.net}
  \alignauthor George Tzanetakis\\
    \affaddr{Department of Computer Science}\\
    \affaddr{University of Victoria}\\
    \email{gtzan@cs.uvic.ca}
}

\maketitle

\begin{abstract}

\end{abstract}

\keywords{put author keywords here} 

\category{H.5.2}{Information Interfaces and Presentation}{Miscellaneous}[Optional sub-category]

\section{Introduction}


~\cite{acm_categories}

\bibliographystyle{abbrv}
\bibliography{cscw2013gtzan}

\end{document}
% Template LaTeX file for DAFx-09 papers
%
% To generate the correct references using BibTeX, run
%     latex, bibtex, latex, latex
% modified...
% - from DAFx-00 to DAFx-02 by Florian Keiler, 2002-07-08
% - from DAFx-02 to DAFx-03 by Gianpaolo Evangelista
% - from DAFx-05 to DAFx-06 by Vincent Verfaille, 2006-02-05
% - from DAFx-06 to DAFx-07 by Vincent Verfaille, 2007-01-05
%                          and Sylvain Marchand, 2007-01-31
% - from DAFx-07 to DAFx-08 by Henri Penttinen, 2007-12-12
%                          and Jyri Pakarinen 2008-01-28
% - from DAFx-08 to DAFx-09 by Giorgio Prandi, Fabio Antonacci 2008-10-03
%
% Template with hyper-references (links) active after conversion to pdf
% (with the distiller) or if compiled with pdflatex.
%
% 20060205: added package 'hypcap' to correct hyperlinks to figures and tables
%                      use of \papertitle and \paperauthorA, etc for same title in PDF and Metadata
%
% 1) Please compile using latex or pdflatex.
% 2) If using pdflatex, you need your figures in a file format other than eps! e.g. png or jpg is working
% 3) Please use "paperftitle" and "pdfauthor" definitions below

%------------------------------------------------------------------------------------------
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
% Please use these commands to define title and author of the paper:
\def\papertitle{Templates for DAFx-09, Como, Italy}
\def\paperauthorA{The DAFx Crew and All Their Friends}
\def\paperauthorB{The Previous DAFx Crew}
\def\paperauthorC{Mary Goround}
\def\paperauthorD{The Lost Crew}


%------------------------------------------------------------------------------------------
\documentclass[twoside,a4paper]{article}
\usepackage{dafx_09}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{subfigure,color}
\usepackage{euscript}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\setcounter{page}{1}
\ninept

\usepackage{times}
% Saves a lot of ouptut space in PDF... after conversion with the distiller
% Delete if you cannot get PS fonts working on your system.

% pdf-tex settings: detect automatically if run by latex or pdflatex
\newif\ifpdf
\ifx\pdfoutput\relax
\else
   \ifcase\pdfoutput
      \pdffalse
   \else
      \pdftrue
\fi

\ifpdf % compiling with pdflatex
  \usepackage[pdftex,
    pdftitle={\papertitle},
    pdfauthor={\paperauthorA, \paperauthorB, \paperauthorC, \paperauthorD},
    colorlinks=false, % links are activated as colror boxes instead of color text
    bookmarksnumbered, % use section numbers with bookmarks
    pdfstartview=XYZ % start with zoom=100% instead of full screen; especially useful if working with a big screen :-)
  ]{hyperref}
  \pdfcompresslevel=9
  \usepackage[pdftex]{graphicx}
  \usepackage[figure,table]{hypcap}
\else % compiling with latex
  \usepackage[dvips]{epsfig,graphicx}
  \usepackage[dvips,
    colorlinks=false, % no color links
    bookmarksnumbered, % use section numbers with bookmarks
    pdfstartview=XYZ % start with zoom=100% instead of full screen
  ]{hyperref}
  % hyperrefs are active in the pdf file after conversion
  \usepackage[figure,table]{hypcap}
\fi

\title{\papertitle}

%-------------SINGLE-AUTHOR HEADER STARTS (uncomment below if your paper has a single author)-----------------------
%\paperauthorA, \sthanks{This work was supported by the XYZ Foundation}}
%{\href{http://dafx09.como.polimi.it}{Dept. of Electronic and Information,} \\ Politecnico di Milano \\ Como, Italy\\
%{\tt \href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
%}
%-----------------------------------SINGLE-AUTHOR HEADER ENDS------------------------------------------------------

%---------------TWO-AUTHOR HEADER STARTS (uncomment below if your paper has two authors)-----------------------
%\twoaffiliations{\paperauthorA, \sthanks{This work was supported by the XYZ Foundation}}
%{\href{http://dafx09.como.polimi.it}{Dept. of Electronic and Information,} \\ Politecnico di Milano \\ Como, Italy\\
%{\tt \href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
%}
%{\paperauthorB,\sthanks{This guy is a very good fellow}}
%{\href{http://dafx09.como.polimi.it}{Reading Group, Dept.~of Reading Sciences} \\ Univ.~of Universe, Sun \\ {\tt %\href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
%}
%}
%-------------------------------------TWO-AUTHOR HEADER ENDS------------------------------------------------------

%---------------THREE-AUTHOR HEADER STARTS (uncomment below if your paper has three authors)-----------------------
%\threeaffiliations{\paperauthorA, \sthanks{This work was supported by the XYZ Foundation}}
%{\href{http://dafx09.como.polimi.it}{Dept. of Electronic and Information,} \\ Politecnico di Milano \\ Como, Italy\\
%{\tt \href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
%}
%{\paperauthorB,\sthanks{This guy is a very good fellow}}
%{\href{http://dafx09.como.polimi.it}{Reading Group, Dept.~of Reading Sciences} \\ Univ.~of Universe, Sun \\ {\tt %\href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
%}
%{\paperauthorC,\sthanks{She is a member of the Wheel Association}}
%{\href{http://dafx09.como.polimi.it}{Spinning Group, Dept.~of Turning Sciences} \\ Univ.~of Planets, Mars \\ {\tt %\href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
%}
%}
%-------------------------------------THREE-AUTHOR HEADER ENDS------------------------------------------------------

%----------------FOUR-AUTHOR HEADER STARTS (uncomment below if your paper has four authors)-----------------------
\fouraffiliations{
\paperauthorA, \sthanks{This work was supported by the XYZ Foundation}}
{\href{http://dafx09.como.polimi.it}{ISPG and CASE Lab,} \\ Politecnico di Milano \\ Como, Italy\\
{\tt \href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
}
{\paperauthorB,\sthanks{This guy is a very good fellow}}
{\href{http://www.acoustics.hut.fi/dafx08/}{Helsinki University of Technology} \\ Espoo, Finland \\ {\tt \href{mailto:dafx-08@acoustics.hut.fi}{dafx-08@acoustics.hut.fi}}
}
{\paperauthorC,\sthanks{She is a member of the Wheel Association}}
{\href{http://dafx09.como.polimi.it}{Spinning Group, Dept.~of Turning Sciences} \\ Univ.~of Planets, Mars \\ {\tt \href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
}
{\paperauthorD,\sthanks{Yes, senior}}
{\href{http://dafx09.como.polimi.it}{Unknown Group, Dept.~of Volatile Sciences} \\ Univ.~of Nowhere, Somewhere \\ {\tt \href{mailto:dafx09@como.polimi.it}{dafx09@como.polimi.it}}
}
%-------------------------------------FOUR-AUTHOR HEADER ENDS------------------------------------------------------

\begin{document}
% more pdf-tex settings:
\ifpdf % used graphic file format for pdflatex
  \DeclareGraphicsExtensions{.png,.jpg,.pdf}
\else  % used graphic file format for latex
  \DeclareGraphicsExtensions{.eps}
\fi

\maketitle

\begin{abstract}
This is the template file for the proceedings of the 12$^{\text{th}}$ International Conference on Digital Audio Effects (DAFx-09).
This template has been generated from WASPAA'99 templates and aims at producing conference proceedings in electronic form.
The format is essentially the one used for ICASSP conferences.
Please use either this \LaTeX{} or the accompanying Word formats when preparing your submission.
The templates are available in electronic form on the following website:
\\ \href{http://dafx09.como.polimi.it}{http://dafx09.como.polimi.it}. Thanks!

\end{abstract}

\section{Introduction}

\end{document}
% -----------------------------------------------
% Template for ISMIR 2010
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[small,compact]{titlesec}
\usepackage[left=1.9cm,top=1.9cm,right=1.9cm,nohead,nofoot]{geometry}
\usepackage{graphicx}
\usepackage{url}

\title{CSC 586E - Data Mining}
\author{Steven Ness - V00657393}
\date{27 May 2012}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents

%
\section{Abstract}

The Orchive is a vast collection of over 20,000 hours of audio
recordings from the OrcaLab research facility located off the northern
tip of Vancouver Island.  It contains a continuous archive of orca
vocalizations from the 1980 to the present time.  This archive of data
is very large, and the sound files that it contains occupy
approximately 10TB of disk space.

In this work we design a series of Machine Learning systems to examine
and classify various aspects of this archive.  We first look at
classifying different recordings based on how much noise is present in
each recording.  There is often considerable boat noise in the
recordings, and in many cases this makes analysis difficult or
unfeasible.  By identifying recordings with less boat noise, we can
narrow the amount of data that is required to be analyzed by humans
and machine learning systems.

The second task in this work is to develop machine learning systems to
pull out individual orca calls from recordings.  These systems should
discriminate between background noise, orca calls and the voice notes
that are present in most of the tapes in the orchive.  Once we have
identified clips containing orca vocalizations, the third task in this
work is to classify these calls based on a previously existing call
catalog, first developed by John Ford.



\section{Sources of Data}\label{sec:sources_of_data}

The source of audio data for this project comes from The Orchive
(http://orchive.cs.uvic.ca), a large archive containing over 20,000
hours of recordings from the OrcaLab research station.  OrcaLab is a
land based research station on Hanson Island that has been in
continuous operation since 1980, and the scientists who run it,
Dr. Paul Spong and Helena Symonds have more sporadic recordings from
this location that go back to 1969.  It was designed as a land based
research station in order to reduce the impact of the researchers on
the orcas under study, as the noise and disturbance from boats impacts
the orcas in observable but currently unquantified ways.  Our lab here
at the University of Victoria has been collaborating with OrcaLab for
approximately 6 years, and in this collaboration we are in the process
of digitizing the collection of analog audio tapes that are the
primary recording medium at Orcalab, designing and maintaining a
website that makes these recordings available to the world, and
performing data mining on these recordings.

The vocalizations of orcas are recorded through a series of
hydrophones, microphones that are designed specifically to work under
the water.  A few of the hydrophones are hardwired, but in order to
have greater geographical reach, most of the hydrophones operate
remotely, transmitting their recordings via VHF radio to the main lab.
These signals are then fed into a 6-channel mixing console.  The
researchers continuously adjust the levels of the recordings both to
increase the volume of the hydrophone located closest to the orcas,
and also to decrease the volume of any hydrophones that are close to
sources of boat noise.

Nonetheless, many of the recordings in the Orchive have substantial
boat noise in them that makes identifying orca calls both difficult
and tiring for a human researcher.  One important data mining task is
to identify the most clear recordings in the Orchive so that datasets
can be provided to biologists and other scientists interested in
studying these recordings.

\section{Pre-processing of data}\label{sec:preprocessing_of_data}

In previous work by our lab, we have been digitizing the large
collection of audio tapes and, in collaboration with concurrent
efforts by OrcaLab at stations in Alert Bay and on Hanson Island, we
have digitized 21,862 recordings.  Of these 14,862 are available on
the Orchive web interface (http://orchive.cs.uvic.ca).

To collect the training data for this experiment, it was important to
get a representative sample of these types of sounds from across the
entire dataset.  Labeling each recording by hand would have taken
considerable time for one individual, so it was decided to label each
100th recording in the archive, a task that required labeling
approximately 14,862/100 = 149 different recordings.  The procedure
that was followed was to examine each 100th recording, and attempt to
find an orca vocalization in this recording.  Many times, the first
recording analyzed contained an orca vocalization, in which case, it
was labeled.  A variable number of other vocalizations around it were
also labeled.  In addition, a sample of background noise, before,
after, and if possible, during the recording was also labeled.  If an
orca vocalization was not found, the next recording was examined, and
this continued until an orca vocalization was found.  In one case this
required looking forward 12 recordings from the desired recording.  In
the same manner, voice notes were also labeled, and a section of
background noise before the voice note was also labeled.  A small
section of annotated audio from the Orchive is shown in Figure
\ref{fig:dm_orchive}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{dm_orchive}
\label{fig:dm_orchive}
\caption{A small annotated section of audio from the Orchive}
\end{figure}

Initial results of classifying recordings into orca, background or
voice note had disappointing results, with only approximately 77\%
accuracy.  In this application, accuracy needs to be considerably
higher than this so that the number of orca vocalizations
misclassified as background are as low as possible so that the
annotations produced have as few false positives as possible. 

The problem that was identified was that many of the vocalizations
identified in the previous step were of distant orcas, or of orca
vocalizations hidden in boat noise.  In order to generate a suitable
training set of audible orca vocalizations, a web-based game using a
citizen scientist metaphor was developed, called the orcaGame
(http://orcagame.sness.net).  A screenshot of this game is shown in
Figure \ref{fig:dm_orcagame1}.  In this game, the user is shown a clip
at the top of the screen, and can classify this call as ``boat
noise'', ``orca'', ``distant orca'', ``voice note'' or ``unknown''.
This website was designed to be easily used by citizen scientists,
that is, persons from the general public with an interest in orca
vocalizations.  As part of a CSC 105 class here at UVIC, we obtained
Human Research and Ethics Approval to test this interface on a
population primarily first and second year undergraduate students.  Of
these, 20 individuals participated and labeled 1012 clips.  Of these,
the ones that were labeled as ``orca'' by a majority of participants
were used as input to a machine learning classifier.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{dm_orcagame1}
\label{fig:dm_orcagame1}
\caption{A screenshot of a version of the orcaGame interface that
  allows a user to classify a clip as boat noise, orca, distant orca,
  voice note or unknown.}
\end{figure}

Many of the recordings in the orchive are affected by considerable
noise.  There are a wide variety of sources of noise, including ocean
waves and rain, but by far the loudest source of noise is that from
boats with engines.  Eventually, we would like to detect all
occurrences of orca vocalizations in the Orchive, but for many of the
scientific goals of researchers, like determining the amount of
cultural drift of orca vocalizations over a span of years, a subset of
quiet, easily analyzed recordings would suffice.  To generate this
dataset, we used the orcaGame interface and identified 200 clips of
audio of approximately 1 second in length, with 100 being silent
recordings, and 100 being noisy recordings.  A spectrogram of a noisy
recording is shown in Figure \ref{fig:dm_noisy} and a spectrogram of a
silent recording is shown in Figure \ref{fig:dm_silent}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{dm_noisy}
\label{fig:dm_noisy}
\caption{A spectrogram of a noisy section of audio from the orchive}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{dm_silent}
\label{fig:dm_silent}
\caption{A spectrogram of a recording without much boat noise and
  containing orca calls.}
\end{figure}

Once we have identified interesting recordings from the Orchive which
contain acceptable levels of background noise, we now wish to segment
these recordings into sections with different types of audio being
labeled.  There are many distinct kinds of audio information in these
files including the stereotyped pulsed vocalizations, echolocation and
whistles of orcas, the sound that is made by the orcas when they rub
on the unique ``rubbing beaches'' found in the Robson Bight ecological
reserve, humpback whale vocalizations, and the sounds made by other
underwater cetaceans and mammals.  In addition, on most recordings
there is also a short voice note, where the researchers note the date
and time, the tape number, their name, and occasionally the identities
of the orcas that are currently in the area.  For this study, we have
chosen to examine the stereotyped pulsed calls of orcas, voice notes,
and all other sources of sound, which we label as ``background''.


\subsection{Audio Feature Extraction}

In order to classify the recordings in the Orchive, we first need to
extract audio features from the raw audio.  There are many types of
audio features that could be used, these can either describe
properties of the waveform, such as the number of zero crossings per
unit time, or spectral features which are usually based on the data
calculated by a Fast Fourier Transform (FFT).

While one could use the raw FFT of an audio waveform, this
representation is expensive in terms of space used and contains
substantial redundancy, so it is often beneficial to reduce the
dimensionality of this data.  A technique that is used is to model the
human peripheral auditory system, and a common algorithm that is used
is that of Mel-Frequency Cepstral Coefficients (MFCC)
\cite{Logan00melfrequency}.  The MFCC models the human cochlea and
outputs a user adjustable number of parameters, which is usually set
at 13, 20 or 40.  For this work we used a MFCC of size 13, which has
been shown to perform well in other Machine Hearing tasks.  The audio
feature extraction was generated by the Marsyas \cite{tzanetakis2000}
Machine Hearing \cite{Lyon11} framework.

The input to the MFCC is a FFT spectrum, and the underlying FFT
algorithm can use different window sizes and hop sizes.  In addition,
it is often useful to look at a number of frames of the MFCC at once,
and take the mean and standard deviation of these frames.  This is
called the ``memory size'', and a graphical representation of these is
shown in Figure \ref{fig:dm_ws_hs_mem}.

\begin{figure}[t]
\centering
\includegraphics[]{dm_ws_hs_mem}
\label{fig:dm_ws_hs_mem}
\caption{A graphical representation of window size, hop size and
  memory size in the Audio Feature Extraction algorithm that has been
  used.}
\end{figure}

In this work we use MFCC features, and also use other spectral
descriptors.  The first of these is the Centroid of a spectrum, which
is defined as the center of gravity of the observation vector.  The
second feature we used is the Rolloff, which defined as the frequency
for which the sum of magnitudes of its lower frequencies are equal to
percentage of the sum of magnitudes of its higher frequencies.  The
third spectral measure we use is the Flux of the spectrum which is
defined as the norm of the difference vector between two successive
magnitude/power spectra.

In addition, we use the number of zero crossings of the waveform per
unit time.  This gives a rough description of the noisiness of the
signal.

\section{Data Mining}\label{sec:data_mining}

There are three main areas that we investigate in this work.  The
first is to determine which of the approximately 22,000 recordings in
the Orchive contain acceptably low levels of boat noise.  The second
is to pull out individual clips of orca vocalizations from these
quieter recordings.  The third task is to classify these individual
vocalizations into call classes according to classes previously
identified by whale researchers.

In this work we subdivide our work into three sections, in the first,
we classify whole recordings into ``silent'' and ``noisy'', in the
second we segment the audio recording into ``background'', ``orca'',
and ``voice note''.  In the third we classify small clips of known
orca vocalizations into different call types, as identified in a
pre-existing orca call catalog.

\subsection{Silent and Noisy recordings}

To classify whole recordings into the classes silent and noisy, we
first collected a set of 100 examples of silent recordings, and a set
of 100 examples of noisy recordings.  We then extracted audio features
from these recordings and output these as a .arff file.  This file was
then processed with the SMO SVM classifier in Weka to generate the
classification accuracy numbers shown in the next section.  To
classify recordings, we extracted audio features from each clip of an
audio file which were output as a .arff file and used a C++ program
that used libSVM to classify each 1 second clip of the audio file.

\subsection{Orca, Voice Note, Background}

To segment recordings into Orca, Voice Note and Background, we used a
C++ program that combined an audio feature extraction engine with an
SVM classifier. We trained this classifier on different amounts of
data and evaluated the performance of each of the resulting Support
Vector Machines.  The C++ program classified each frame of audio
features into a class, and then performed a neighborhood voting
scheme, where a 1 second window of multiple 20ms frames were used, and
the class that had the most number of frames that were classified as
that class was output.  For example, if 80 frames were classified as
orca, 15 as voice and 5 as background, the classifier would output
``orca 80'' for that 1 second section of audio.

\subsection{Orca call classification}

In order to classify orca calls according to a pre-defined call
catalog, 325 clips containing orca vocalizations were classified by
hand into six classes of common calls, which were ``N1'', ``N3'',
``N4'', ``N7'', ``N9'' and ``N47''.  A single audio feature vector for
each clip was calculated which contained the mean and standard
deviation for the audio features mentioned above.  This gave a feature
vector of size 68 which was output in .arff format.  These files were
then used as input to Weka and three different classifiers were used,
including J48, Naive Bayes and the SMO SVM classifier.

\section{Results}\label{sec:results}

\subsection{Audio Feature Extraction}

We did a parameter search over window size, hop size and memory size.
The results of this are shown in Table \ref{table:afe}.  From this
table we can see that the optimal setting of window size and hop size
is 2048 and 1024 and the optimal window size is 40 frames.  While it
would be possible to go to large window sizes, in our experience this
blurs out the frequency to an unacceptable level in other Machine
Hearing tasks and makes segmentation of the audio problematic.

\begin{table}
\centering
\caption{Audio Feature Extraction}
\begin{tabular}{ccccc} 
\hline
Window Size & Hop Size & Memory & \% correct & \# data points \\  \hline
\\ 
1024 & 512  & 1  & 83.5 & 282899 \\
1024 & 512  & 10 & 87.7 & 282899 \\
1024 & 512  & 40 & 92.5 & 282899 \\
2048 & 1024 & 1  & 84.1 & 143049 \\
2048 & 1024 & 10 & 91.3 & 143049 \\
2048 & 1024 & 40 & 93.7 & 143049 \\
\\ \hline
\end{tabular}
\label{table:afe}
\end{table}


\subsection{Silent and Noisy recordings}

As training data for this task, we labeled 100 instances of audio as
``silent'' and 100 instances of ``not-silent'' and extracted a variety
of audio features from these files, including MFCC coefficients,
general spectral descriptors such as the centroid frequency, Rolloff
frequency and flux, and the number of zero crossings per window.

For this experiment, we extracted a single summary vector for each
audio clip containing 68 elements, which were the mean and standard
deviation for each of the audio features given above.  We generated
audio features for these 200 instances and classified them using the
SMO SVM classifier in Weka.  From this we obtained a classification
accuracy of 82.5\%.

To provide a summary of the recordings in the Orchive, 6 clips of 1
second in duration were extracted from each recording.  This gave a
total of 125,347 clips, totaling 2089 hours of audio.  We extracted
the same features as those extracted from the training data above and
output these vectors as a .arff file.  These vectors were then used as
training data for an SVM classifier implemented in C++ and using
libSVM \cite{cc01} as the classifier. We took each of the 125,347
clips of audio and classified each one of them into the classes
``noisy'' and ``silent''.  Of these, 87,657 were classified as noisy
and 37,690 were classified as quiet.  The quiet recordings would
provide for a good dataset for researchers to examine first, and could
then move on to progressively more difficult recordings as their
research project required more data.

\subsection{Orca, Voice Note, Background}

As an initial study, a total number of 3265 samples of data labeled
with ``orca'', ``voice'' and ``background'' was used to train a Naive
Bayes classifier in Weka with default parameters.  This gave a
classification accuracy of 68.3\%.  The same data was also used to
train a SVM using the SMO classifier in Weka with default parameters
which gave a classification accuracy of 77.6\%.  The confusion matrix
is shown below.

\begin{verbatim}
    a    b    c   <-- classified as
  886  298    2 |    a = b
  376 1516    3 |    b = o
   34   17  133 |    c = v
\end{verbatim}

It can be seen from this confusion matrix that background and orca
vocalizations are labeled as each other quite often.  In the Orchive,
the majority of the audio data is of background recordings, and an
important task is for the machine learning system to distinguish
background recordings from orca vocalizations.  If our classifier
gives a large number of false positives, it will be difficult for
researchers to use this data to find orca vocalizations.

Upon further examination it became clear that the problem was that the
large amount of background noise in the recordings was difficult for
our chosen audio features.  It was then decided to instead train a
classifier to identify clearly identifiable orca vocalizations, and
then extract these clear vocalizations from the archive for further
processing.  From the results of the previous section, where we
identified a large number of recordings with silence in them, this
limited amount of data would still represent a large dataset to study.

To improve classification accuracy, a set of 725 audible orca recordings
was chosen using the orcaGame interface.  534 clear voice recordings
were also collected, and 1539 background recordings were collected.
We then randomized selected subsets of 10, 50 and 500 recordings from
each of these collections and trained a machine learning classifier on
each of these collections.  To test these classifers, we used a
separate set of recordings, and used a neighbourhood voting method,
where each 20ms section of audio was classified by a SVM classifier,
and then the results of 1 second of these were collected.  The scheme
we used is described in the Data Mining section above.

The results from this are shown in Table \ref{table:obv}.  From this
we can see that the classification performance for classifying orca,
background and voice is approximately 85\%-89\%.  We are currently
running this classifier on all recordings in the Orchive, but due to
it's slow speed, are looking for ways to downsample this data and
provide a performance boost.


\begin{table}
\centering
\caption{Orca / Background / Voice}
\begin{tabular}{cccccc} 
\hline
\# Training & Time & Time to classify & \% correct & \% background & \% voice  \\  
samples & to train & classify 1 sec & orca & background & voice \\ \hline
\\ 
150   & 525    & 0.1 sec  & 64.1 \%  & 46.7 \%  & 91.7 \% \\
300   & 2103   & 0.5 sec  & 79.6 \%  & 66.0 \%  & 93.5 \% \\
1500  & 29025  &   2 sec  & 84.6 \%  & 85.3 \%  & 88.9 \% \\
\\ \hline
\end{tabular}
\label{table:obv}
\end{table}


\subsection{Orca call classification}

Using the Orchive and orcaGame web interfaces, we created a collection
of 319 calls of 6 classes, these included the common calls ``N1'',
``N3'', ``N4'', ``N7'', ``N9'' and ``N47''.  Audio features for each
20ms audio frame of these files were generated, these included the
MFCC coefficients, Centroid, Rolloff, Flux and Zero crossings.  The
mean and standard deviation for each of these features were then
calculated and were output as a .arff file.  

These were then classified with Weka using the J48 tree classifier,
which gave an accuracy of 58\%.  We next tried the Naive Bayes
classifier which gave an accuracy of 64.6\%.  The SMO SVM classifier
produced the best results, giving an accuracy of 75.9\%.

The confusion matrix for the SVM classifier is shown below.  From this
we can see that most of the classes are classified correctly.  One
interesting point is that there is some confusion between the N7 and
N9 calls, and it can be noted that these calls are often confused with
each other by humans as well.

\begin{verbatim}
 a  b  c  d  e  f   <-- classified as
 30  0  0  1  0  4 |  a = N1
  1 53  2  0  0  0 |  b = N3
  0  1 80  9  1  2 |  c = N4
  0  0 32  0  0  1 |  d = N47
  0  2  1  0 15 13 |  e = N7
  3  0  2  0  2 64 |  f = N9
\end{verbatim}


\section{Conclusion}\label{sec:conclusion}

In this work we first annotated a large number of clips from the
Orchive, and in doing so, doubled the number of annotations in the
Orchive that had been generated by 29 scientists over 4 years.  This
set of annotations labeled clips as orca, background or voiceover.

Because of the amount of boat noise in a large number of these clips,
early results with a SVM classifier gave the disappointing result of
approximately 77\% accuracy.  We then used the orcaGame citizen
science mini-game to refine the classification of these clips into
distant orcas and more clear orca calls, and also used this interface
to classify clips into the classes noisy and silent.

These noisy and silent clips were used to train a machine learning
classifier that was then used to classify all the recordings in the
orchive into silent and noisy, which gave a total of 37,690 silent
clips that would be useful for early work on call classification.

We then used the clear orca calls, background calls and voice notes to
train a machine learning classifier to segment recordings and pull out
clips of isolated orca vocalizations.  With 1500 training clips we
were able to get an accuracy of 84.6\% in classifying orca calls and
approximately the same accuracy with background and voice clips.

Finally, we classified a sample of orca vocalizations by hand into a
set of 6 call classes, and using an SVM classifier were able to get a
classification accuracy of 75.9\%.

In conclusion, this project was quite successful, and also more
importantly, has laid the groundwork for an ongoing study in which we
are classifying all the audio in the entire Orchive.



\bibliographystyle{plain}
\bibliography{datamining2012sness}


\end{document}
% -----------------------------------------------
% Template for ISMIR 2010
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[small,compact]{titlesec}
\usepackage[left=1.9cm,top=1.9cm,right=1.9cm,nohead,nofoot]{geometry}
\usepackage{graphicx}
\usepackage{url}

\title{CSC 586E - Term Paper}
\author{Steven Ness - V00657393}
\date{5 April 2012}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents

%
\section{Introduction}

In the paper ``Monitoring and Mining Insect Sounds in Visual
Space''\cite{hao12}, Hao et al. describe a novel method for data
mining large databases of insect sounds.  Their method is completely
automated, and does not require human experts to label data, as is the
case in most other systems of this type.  They perform classification
of sounds based on the spectrogram, a visual representation of a sound
that gives a frequency versus time plot of a sound.  They use a recent
distance measure called the Campana-Keogh (CK) measure to compare the
textures of two images.  

In this report, I will discuss my reading of this paper, first giving
a critical summary of the paper, going through each of the major
sections describing and commenting on the ideas and quality of the
research.  I will then discuss what I liked about the paper and what I
didn't like, and will finish with a short conclusion.


\section{Critical Summary}

\subsection{Introduction and Motivation}

The paper first describes the importance of monitoring and measuring
of the sounds of animals and insects.  In the natural habitat, the
sounds of animals can be used to determine how many animals are in a
location and what species these animals are, and can help scientists
measure the biodiversity of a location, and how this biodiversity
changes over time.  There is recent interest in this field of study
\cite{wimmer2010} \cite{seuer2008} as a method for determining the
biodiversity changes in regions over time.  The authors then say that
an automated system for detecting insects would also be useful from a
commercial point of view,

The paper then goes on to say that in laboratory settings, it would
also be advantageous to have an automated system that could segment
and classify recordings.  Currently this task often requires
researchers to annotate hundreds of recordings by hand, which is a
very time intensive task.  As a scientist who has done many hours of
hand annotating of large sound archives, I can attest to the
difficulty and time-consuming nature of this task.

The authors then say that there are many problems with current methods
of detecting, segmenting and classifying the sounds and vocalizations
of animals.  Current methods require researchers to manually annotate
recordings and to generate a large corpus of data to be used by
classification algorithms.  These algorithms often have a large number
of tunable parameters, some of which can be automatically determined
from the large amount of training data, and some which have to be
manually explored by the researchers.  In addition, many of these
algorithms are computationally expensive and cannot be deployed in the
field.  I have found these facts to be true in my own research, and I
appreciate the fact that the authors spend time discussing the
problems faced by researchers studying bioacoustics.

\subsection{Method}

The method that this paper proposes is to classify animal sounds in
the visual space by examining the texture of the spectrogram of the
sounds, and finding the smallest acoustic fingerprint that is
representative of the species.  The texture of an image is a concept
commonly used in the field of image processing and describes the
quantitative relationship of light and dark patterns in an image.

The images that this paper proposes to examine are spectrograms of an
audio file.  A spectrogram shows the time-frequency evolution of a
sound, commonly calculated by a Fast Fourier Transform (FFT).  There
is a long tradition involving the manual inspection of spectrograms to
analyse the vocalizations of organisms, one among many of these is the
Orca call catalog from Ford \cite{ford87}.  In these approaches, a
human manually inspects and classifies spectrograms.

I found that this is an interesting and novel way of approaching the
problem of classifying and annotating the sounds made by organisms.
Most other approaches use single time slices of spectral data, which
are sometimes averaged over time.  The idea of looking at sounds in
the visual space in fact means that instead of looking at a single
time point, the time evolution of sounds is instead investigated.
While in the sub-field of symbolic Music Information Retrieval (MIR),
there has been some research into the time structure of sound, in most
audio-based approaches to MIR, the time evolution of music is
explicitly ignored.  This is likely because this would dramatically
complicate the analysis of sound, and in most of the problems that
have been investigated by researchers, such as genre detection in
songs, this additional complexity does not significantly improve
results.  However, when looking at bioacoustics, the time evolution of
signals can be very important.

\subsection{Campana-Keogh distance measure}

There are many methods in the literature for computing the similarity
of two images by their textures that they have.  Some of these include
wavelets, Fourier transforms and Gabor filters.  The authors describe
one problem of these methods is that they often have many tunable
parameters, and that the exact values of these parameters can have
considerable impact on their classification performance.

In this paper they use a previously described distance measure called
the Campana-Keogh (CK) measure \cite{campana2010} which uses the
concept that two images are similar if one image can be used to
compress the other image.  The CK measure uses the MPEG-1 algorithm to
calculate the distance between two images and its formula is shown
below:

	\[ dist = \frac{mpegSize(x,y) + mpegSize(y,x)} {mpegSize(x,x) + mpegSize(y,y)} - 1 \]

This surprisingly simple measure has been shown to work well in a
variety of contexts, including the comparison of images of moths, wood
grains, nematodes and tire tracks\cite{campana2010}.

I find this part of the paper to be very interesting and novel.  On
first reading, I found it difficult to belive that such a simple
scheme would work, but on further reflection, the idea that two sounds
are similar if one can be used to compress the other could be an
excellent approach to the study of these signals.


\subsection{Notation and Sound Fingerprints}

The authors then go on to describe the notation used in their paper.
I found this section challenging to understand at first, and it was
only by repeated readings of this section, and indeed the whole paper,
that I was able to understand what they were trying to convey.  Once I
understood it, I found this section very valuable, but it would have
been useful if they described how each of these notations fit into the
paper as a whole.

In this section, they first define a sound sequence as a continuous
sequence of real valued data.  They then go on to define a spectrogram
as a visual spectral representation of an acoustic signal.  They
define a sliding window as a local subsection of a sound sequence, and
define a subsequence and a way to measure distances between
subsequences.

They then move on to more interesting definitions, first of the
entropy of a sound sequence dataset:

	\[ E(D) = -p(X)log(p(X)) - p(Y)log(p(Y)) \]

The information gain for a given splitting strategy is then defined as

	\[ Gain = E(D) - E'(D) \]

Where $E(D)$ and $E'(D)$ are the entropy before and after $D$ is
partitioned into $D_1$ and $D_2$.

	\[ E(D) = f(D_1)E(D_1) + f(D_2)E(D_2) \] 

Where $f(D_1)$ and $f(D_2)$ are the fraction of the objects that are
in $D_1$ and $D_2$, respectively.

They will use these definitions later in the paper to help speed up
the time used by their brute force algorithm.  I find this one of the
most interesting parts of this paper, the use of information entropy
and information gain to speed up their algorithm, which makes this
algorithm practical to use on large datasets.  Without it, the CK
measure on its own would be very expensive to run on all audio frames
in a large data mining experiment.

They then point out that given a linear ordering of annotated objects
in $D$, there exist at most $|D|-1$ distinct splitting points that
divide the ordered objects into two distinct sets.  Finally, they
define a sound fingerprint for a species as the subsequence from P
along with its best splitting point that produces the largest
information gain when compared with the non-matching sequences U.

They then go on to provide an figure that explains in rough detail
what their distance measure looks like.  This is shown here in Figure
\ref{fig:campana_figure2}.  I found these examples to be unclear at
first, but on deep examination and by staring at it for quite some
time, I figured out what they were trying to say.  In these examples,
they show some sound fingerprints with arrows pointing to a line below
them.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{campana_figure2}
\label{fig:campana_figure2}
\caption{A reproduction of the main figure from the original paper
  explaining their methodology for finding a candidate sound
  fingerprint.}
\end{figure}

The central point that I missed on the first reading was that this
number line is just showing a specific comparison of the distance of a
number of sound sequence fingerprints to a given fingerprint, which is
shown at the 0 point on the number line.  The ticks on the top of the
figure show the distance to the elements in P and the ticks on the
bottom of the figure show the distance to the elements in U.  The
dividing line is just one of $|D|-1$ distinct possible splitting
points, and for this splitting point, 4 elements in P are correctly
classified as P and 1 is in correctly classified as U.  All 4 elements
in U are correctly classified as not being from the set P.  By moving
this splitting point along the number line, different numbers of
instances get classified as P or U, and give different entropy
numbers.  For the figure above, the entropy of the unsorted data is:

	\[ [-(5/9)log(5/9) - -(4/9)log(4/9)] = 0.991 \]

And the entropy with the given split point is:

	\[ (4/9)[-(4/4)log(4/4) -(0/4)log(0/4)]\ * \]
	\[ (5/9)[-(4/5)log(4/5) -(1/5)log(1/5)] = 0.401 \]

Which gives an information gain of:

	\[ 0.991 - 0.401 = 0.590 \]

They then go on to investigate and describe the notion of sound
fingerprints.  This is also a very interesting part of this paper, and
in this section they develop a formal methodology of finding a sound
fingerprint of a species using techniques similar to those used in
genomics with discrete text strings.  They note that in this
particular problem we are interested in real value streams of numbers,
rather than discrete text symbols.

They first set out a constructed set of sequences that describe
observations in P:

	\[ Ma = \{ rrbbcxcfbb, rrbbfcxc, rrbbrrbbcxcbcxcf \} \]

And another set that describe sequences in U:

	\[ \neg Ma = {rfcbc, crrbbrcb, rcbbxc, rbcxrf, ..., rcc} \} \]

With these sequences, they then go on to describe methods to find the
repeated string $cxc$ that is present in $Ma$ and absent in $\neg Ma$.
They say that when using real data, the problem is considerably more
complex than this, as the data can be contaminated with noise, some of
the examples in P could be mislabeled, or the samples could be of low
quality.

\subsection{Methodology}

The authors then give a very detailed methodology section that
includes not just equations, but also the algorithms that they have
implemented.  I found this section useful and valuable, in that it
gave clear equations for how their method worked, and also presented
an implementation of these equations in an easy to understand series
of algorithms.

They first describe the Brute-Force algorithm, which basically just
searches over all possible combinations of subsequences for P and U,
and computes the distance between all these subsequences.  The
equation for this is:

	\[ \sum^{L_{max}}_{l=L_{min}} \sum_{S_i \in { P }} (M_i - l + 1) \]

While this algorithm guarantees that the best subsequence will be
found, it is quite expensive in terms of computer time, and even for
their toy dataset of 10 sound files of the insect they are looking for
and 10 sound files of other sounds, would take 1,377,800 calls to the
CK distance measure, which is the most time intensive part of the
whole process.

To reduce the computation time, they first investigate Admissible
Entropy Pruning, in which they note that they can easily compute the
upper bound of an ordering, and if this upper bound is less than the
best-so-far information gain of any previously determined ordering,
they can abandon the current search and move on to the next candidate.
For their toy problem, this only reduces the total number of
comparisons, but in a data mining context, where huge databases are
searched, they say this could prune the total number of calculations
by 95\%.  This Entropy Pruning algorithm is one of the key insights of
this paper.  Without it, their CK distance measure would likely be too
expensive to use on real datasets, and by using Entropy Pruning, they
are able to speed up their algorithm by a large factor.

They then further investigate ways to speed up their algorithm by
introducing a proxy for the CK distance measure using euclidean
distances.  They note that their algorithm orders candidate solutions,
and that the entropy pruning algorithm can eliminate poor solutions if
their upper information gain bound is less than the current best
solution.  They then go on to say that if they could generate better
solutions early in the search process, they could quickly eliminate
unpromising solutions.  This is however as they note, a
chicken-and-egg problem, as they cannot know what are these better
solutions before they find them.  In order to overcome this, they
introduce a Euclidean distance measure, and show via a graph that the
Euclidean distance is a good proxy for the CK distance, and is much
less expensive to compute.  This graph is shown in Figure
\ref{fig:campana_distance}.

\begin{figure}[t]
\centering
\includegraphics[width=70mm]{campana_distance}
\label{fig:campana_distance}
\caption{A graph showing the relationship between Euclidean and CK distance. }
\end{figure}
 
They then go on to describe a series of experiments in which they show
quite convincingly that their methodology works well on their toy set
of data, and give links to a website that gives more experimental
results on different datasets.  They follow this with a brief section
of their conclusions and further work.

\section{Things I liked most}

The thing that I liked most about this paper was the use of the CK
distance measure to compare spectrograms.  While at first it seems
like a strange idea to use the MPEG-1 algorithm to compare
spectrograms, the more I thought about it and read about it, it is a
very interesting approach, and one that I am currently investigating
in my own research.  It uses the idea that two images are similar if
one can be used to compress the other, an idea with substantial
grounding in the literature.  Further, it uses the MPEG-1 algorithm,
one of the most highly optimized pieces of code around, and which has
been implemented in hardware.  For doing real time detection in
streams of audio, especially in equipment located in the field, this
could dramatically lower the computational and power requirements of
such systems.

The second thing I liked the most about this paper was the firm
theoretical grounding they placed their algorithms upon, using
information entropy as a way to measure different orderings of
sequences.  They used this theoretical grounding to great effect later
in the paper, giving examples of a savings of 95\% of the
computational effort required by their algorithm.

\section{Things I didn't like}

The main thing I didn't like about this paper was the diagrams and
explanation of their algorithm in graphical form.  It took me
considerable time to understand their diagram, which turned out to be
quite simple when understood.  The main problem was that this diagram
wasn't fully explained in the text, and it was only after reading
their algorithm in detail that the concepts the figure described
became clear.  In order to improve this, they should have had a longer
exposition in the text about what the symbols diagram meant, and
should have spent more time on making this diagram clear.  The symbols
above the P elements and below the U elements were superfluous and
were not defined in the text.  The spectrograms were hard to see in
this diagram, and it was unclear why some of them were more similar
and less similar to each other.

The other thing that I didn't like in this paper were the diagrams for
the information gain versus the number of calls to the CK distance
measure.  There was a lack of information in this figures, and it was
unclear to me at first why the ``brute force search terminates'' line
was so far to the right of the diagram.  A better way to do this would
perhaps to have the bottom axis be scaled logarithmically, or to have
a more carefully selected dataset that showed their point more
clearly.

Sadly, the lack of thought behind these figures, and their poor
execution, actually hampered my understanding of the paper, rather
than enhancing it.  However, these concepts are difficult to
understand, and it is often a great challenge to describe them
visually.  Fortunately, the authors described their algorithms well in
the medium of text, and overall the paper communicated well the concepts
the authors were trying to convey.

\section{Conclusion}

Overall, I found this to be a very good paper, and the ideas of the CK
distance measure and the grounding of the ordering algorithm in
information theory were good contributions to the field.  I am
actually working on testing the algorithms in this paper on our large
orca call archive.  I greatly enjoyed reading this paper, and found
that I gained good insights into information entropy and the visual
mining of large datasets.

\bibliographystyle{plain}
\bibliography{datamining2012termpapersness}


\end{document}
% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\begin{document}

%\date{}

\title{\Large \bf Classification of Orca Vocalizations using various Distributed Systems : A Comparison }

\author{
{\rm Steven R. Ness}\\
University of Victoria
}

\maketitle

\subsection*{Abstract}

Given the vastness of this archive, there are certainly many different
tasks that bioacousticians and other scientists might want to do with
it, but one primary obstacle is that the Orchive is currently mostly
unlabelled.  After 4 years, there are approximately 12,000 annotations
in the Orchive, which represents only a few hours of actual annotated
time.  Even with many scientists working on labelling this data, it
would take many person-years to label the entire Orchive.  By using a
combination of expert users and Machine Learning software, it is
possible to train Machine Learning classifiers on expert labelled data
and from there to predict the location of orca vocalizations.   In
order to further add to the training data for the Machine Learning
classifiers, we have also built simple iPad and web based casual games
with a serious intent, which we call serious casual games.  Casual
games are those like the well known Solitare, which take a short
amount of time to play and do not require all the users attention.  By
allowing these citizen scientists to assist us, we are able to
collect larger amounts of data, and to sort through data we have
generated quickly.  In fact, one of the main users of this game
software has been ourselves, we find the game interface a quick and
effective way to label training data.  

With these interfaces, we are able to label substantial amounts of
data, but are still far away from labelling the entire 20,000 hour
archive.  In order to do this, researchers have commonly used a
combination of the techniques of Audio Feature Extraction and Machine
Learning to classify audio.  In this paper we will use a combination
of Audio Feature Extraction and Machine Learning to identify sections
of the Orchive where there are clear orca vocalizations that are
distinct and not overlapping with the vocalizations of other orcas and
are relatively free of boat noise. These extracted vocalizations will
then be made available to the scientific community and will also be
used by our lab in future work for answering other scientific
questions about this species.  A system diagram of how this process of
Machine Learning, Expert Interfaces and Serious Casual games works,
please refer to Figure \ref{fig:systemDiagram}.

\begin{figure}
\centering
\includegraphics[width=80mm]{systemDiagram}
\caption{System Diagram} 
\label{fig:systemDiagram} 
\end{figure} 

In order to do this, we will compare a number of different solutions
for doing distributed audio feature extraction and Machine Learning
and will compare not only their performance in classifying audio, but
also our experience with using these tools.

For my evaluation, we will generate two different types of results.
The first type will be the output of the audio feature extraction and
Machine Learning task. For this task, we will look at the distribution
of orca vocalizations in the dataset, and will train Machine Learning
classifiers that can distinguish regions of the Orchive that have
individual orca vocalizations.

\section{Related work}

A different methodology of doing audio feature extraction was explored
in the paper ``Distributed audio feature extraction for music''
\cite{bray05}.  In this paper the authors use a dataflow architecture
for distributed audio feature extraction on a set of networked
computers.  A dataflow architecture allows users to connect small
processing algorithms in a graph like network and to then use this
constructed network to transform input data into a desired type of
output data.  A simple example of a Marsyas network would take audio
samples from a file on disk, multiply them by a number and output them
to another file on disk, which would have the effect of increasing the
volume (loudness) of the file.  However, most of the commonly used
algorithms in Marsyas are complex feature extraction and machine
learning algorithms.  Marsyas was an existing software platform that
used this dataflow architecture, and this paper explored the use of
dataflow architectures on networked computers.  One of the important
contributions of this paper was that the optimal vector size of data
to send from one computer to the other was influenced by the network
settings of their ethernet routers, but in their case a 256 size
vector provided the best speed.  Results are also presented to show
that by using a Collection Partitioning Adaptive algorithm the authors
were able to get the best utilization of their cluster.  This
algorithm splits up the files to be processed and adaptively sends
more jobs to the computers that have completed their past jobs
quickest.  The authors also note that it is very simple to port an
existing Marsyas network to this distributed framework, and there is
indeed current efforts ongoing to do this in an automated fashion.

One serious drawback of this framework is the large amount of disk and
network traffic that is generated by the producer nodes on these
system, the authors point out that the maximum number of worker
computers that could be served by one dispatcher computer was only
four.  This makes sense because of the large amount of data generated
in many of the intermediate steps in the audio feature extraction
pipeline.  The authors propose a system of hierarchical dispatchers
and workers to solve this problem.

For many interesting tasks in realtime computer generated music and
analysis of music in performance situations this type of system would
be very useful, for example, one dispatcher could communicate with a
number of workers each of run a different type of Machine Learning
algorithm and where the results are then collated.  However, for the
present task of analyzing huge collections of audio that is already on
disk in a batch fashion, the architecture in distributed Marsyas
likely would need further work to be of much use.

A paper highly relevant to our own, yet using very different
underlying audio features and Machine Learning algorithms is ``Sound
retrieval and ranking using sparse auditory representations''
\cite{lyon10} by Dick Lyon and his colleagues at Google.  In this
paper, the authors describe a system designed to help them to create
systems to understand sounds, and they use both a novel audio feature
representation based on models of human hearing combined with a novel
Machine Learning algorithm previously used for image retrieval named
PAMIR (Passive Aggressive Model for Image Retrieval).  In this system,
the data was represented as sparse vectors, which is quite different
from the dense feature vectors commonly used in DSP.  Sparse vectors
have mostly zero values, with just a few non-zero values, and dense
vectors have numbers in almost all their elements.  Sparse vectors are
commonly found in Machine Learning systems that work with text.

In their system, they compare the results of the typical features that
are used in Machine Hearing, the Mel-Frequency Cepstral Coefficients
\cite{Logan00melfrequency} (MFCC) with those of their novel algorithm,
one based on models of the human peripheral auditory system which used
a filterbank called the Pole-Zero Filterbank Cascade (PZFC) and used
this data to make a series of Stabilized Auditory Images (SAIs).
While on one hand the MFCC system outputs a one dimensional vector for
each time slice, producing an image for the sound file when these time
slices are stacked, the SAI system on the other hand generates a two
dimensional vector at each time slice, which can be represented as an
image which can be stacked to form a movie.  The authors describe a
system whereby these images are divided into a number of regions, and
the section of the images that are in these regions are then clustered
based on their similarity using vector quantization to generate a
dictionary using an training set of audio.  This dictionary is then
used to do vector quantization of test audio, and these features are
used as input to the PAMIR system.

PAMIR is a new algorithm for doing mappings from a huge sparse feature
set to a very large query-term set, and was traditionally used in
image recognition \cite{chechik10} to generate tags for images on
Google Image search.  In this context, PAMIR is used to tag SAI images
in order to add tag annotations to sound files.  The authors show
impressive results, with an average precision of 35\% and best
precision at top-1 of 73\%.

This paper is directly relevant to my current paper, and in fact there
are a number of the algorithms in this paper that we want to implement
in my own OpenMIR system.  However, it should be noted that while
these algorithms, specifically the modeling of the auditory cortex and
the SAI model, do show a substantial increase in performance over
traditional techniques, they do come with a substantial performance
hit, and are typically much slower than traditional algorithms since
they often must do an $O(n^2)$ step to calculate the SAI image while
traditional FFT techniques only must do an $O(n(log(n)))$ step.  In
any case, the method of using vector quantization, SAI images and
advanced Machine Learning techniques such as PAMIR shows great promise
in the future in my work.

In ``Game-powered Machine Learning'' \cite{barrington12}, Barrington,
Turnbull and Lanckiet describe a system that combines Games With A
Purpose (GWAP) with Machine Learning in a framework that they call
``Active Machine Learning''.  In this framework, the user is presented
with a set of plausible answers by a Machine Learning system, these
plausible answers are chosen to be as close to the actual answer as
possible, which helps the system train itself on difficult data most
effectively, and also makes the game more fun and challenging to play,
since the answers are so close to each other.  This kind of system
directly inspired the work we describe earlier with GWAP, and how we
are developing a system to let users help us to classify orca calls,
and by using active learning we can help to make this game fun and
challenging.

In ``Overview of OMEN''\cite{McEnnis2006}, McEnnis, McKay and Fujinaga
describe their system called OMEN (On-demand Metadata Extraction
Network), a system that shares many commonalities with the OpenMIR
system we describe in this paper.  The main issue their system aims to
overcome is copyright problems that researchers encounter when they
want to legally extract audio features from songs.  In the strictest
legal sense, each researcher must purchase their own version of a song
in order to do any kind of audio science on it.  This becomes
prohibitive in the age of the Million Song Dataset.  In the OMEN
system, a coalition of libraries creates a network of systems that
host the raw audio data of the song, but only send back to the
researchers the specific audio features that they are interested in.
Another interesting aspect of this system is that they propose to use
the unused computing cycles of library browsing computers to be a grid
computing resource for doing their audio feature extraction.

One big difference is that their system is primarily based on Java,
where my system uses Java, but is primarily based on Python, C++ and
Javascript.  In many ways, the Java ecosystem is enticing for building
server applications, and several times in this project we considered
doing feature extraction in jMIR, the Java Music Information Retrieval
library that the OMEN system happens to use.  However, there are
considerable barriers to getting a productive Java development and
production system installed and maintained.

Their system has a Master Node computer that coordinates all tasks,
Library Nodes that coordinate tasks for a single library and Worker
Nodes that perform feature extraction.  In their system, Library Nodes
store the raw data and distribute it to the Worker Nodes.  One
possible sub-optimal case would be if a single Library Node got
overwhelmed with requests, in this case, it would be challenging to
get other Library Nodes to serve requests to new Worker Nodes.  The
MapReduce framework allows us to sidestep this issue, and by helping
with data locality, allows us to reduce the amount of data sent over
the network.

\section{Solution}

In order to analyze the audio in large online archives efficiently, a
scenario involving distributed computing must be used.  There are many
different types of systems that perform Machine Learning and many of
these can distribute their work to different machines.  Some of these
systems are better suited for this task than others, and in this paper
we investigate a few combinations of these tools.

One of the possible solutions to the problem of the segmentation of
audio into orca vocalizations and other sounds is to use a Machine
Learning supervised classifier system.  These systems require hand
labelled training data, in our particular case, audio would be
labelled as either orca or background.  This training data is then
used to train a classifier system that can classify future instances
based on features of the training data.  There are a wide variety of
classification algorithms that are commonly used, some of the most
popular are Support Vector Machines (SVMs), Neural Nets, Decision
Trees and Logistic Regression.  In our past work we have found good
success in using SVMs, and have shown that SVMs typically outperform
many other Machine Learning algorithms.  However, there is current
work in this area showing the benefits of many alternate algorithms
for different problem domains.

There are two steps to the problem of the classification of audio
signals.  The first is that of audio feature extraction, and the
second is that of classification with Machine Learning systems.  Sound
is a pressure wave in the air, and the variations in pressure over
time can be stored in a computer as real number values.  This raw
waveform data contains all the information of the signal, but because
of it's extremely high dimensionality, is difficult to analyze with
machine learning systems.  To take this raw data and to put it in a
form more conducive for analysis, we generate a large number of higher
level features, including spectral features \cite{marsyas}, pitch
information \cite{cheveigne02}, chromatic scale information
\cite{marsyas} and other features based on models of the auditory
cortex \cite{lyon82}.  One can also model the statistical behaviour of
these properties as they change over time, an avenue that has been
shown to be fruitful \cite{marsyas}.  In our system we investigated
the use of a number of audio features, the results of which are
discussed in the Evaluation section, but in summary we found that
using all the standard audio features was most useful, however using
pitch information degraded performance of the classifier.  This is a
surprising result and one that we are investigating further.

The second step is to take these audio features, train and test a
Machine Learning classifer on them, and then run this Machine Learning
classifier over all the data.  In many cases it is beneficial to
pre-calculate the audio features for the data, as once as good
parameters can be determined for the audio feature extraction engine,
such as the window size, hop size for the spectrum calculation and the
memory size for the windowed statistical calculations, these features
can be saved and reused in multiple Machine Learning experiments with
different parameters.  One example of this is the recent release of
the Echonest audio features for the Million Song Dataset
\cite{bertinmahieux11}.  This collection of audio features for one
million popular songs took a considerable amount of time to process,
and one approach is to save these audio features and running different
machine learning algorithms or different parameter sets of these
algorithms on them.  Another point that should be made here is that
the form of the features that are extracted in this step can have a
direct relation to the performance of the subsequent Machine Learning
step.  Different algorithms perform better with different forms of
data, and matching the right features to the right algorithm is both
an art and a science.

It should be noted that both of these steps, the audio feature
extraction and the Machine Learning could be carried out in the same
executable.  There are a number of advantages to this as well as some
disadvantages.  One major advantage is that the intermediate audio
feature data does not have to be saved on disk.  This intermediate
data can be very large in size, and can be many times larger in size
than the original audio data, depending on the type of compression
used for the original audio.  In many cases in the real world, it is
less expensive to just recalculate the audio features each time that a
new Machine Learning run is carried out rather than saving them on
disk.  This also can help to overcome any versioning problems, where
different versions of audio features are combined together in the
training, testing or predicting stages.  It will be a balance between
these two poles for production systems to manage.

For this work, Marsyas was used for doing Audio Feature Extraction.
There are other Music Information Retrieval toolkits for extracting
audio features, but the audio features output by Marsyas are typically
amongst the most robust and high performing in the literature
\cite{marsyas}.  We have implemented this feature extraction using
native filesystem operations loading data off of NFS or a local disk
depending on the cluster situation.

For the Machine Learning half of the project, a variety of Machine
Learning systems and algorithms were used and comparison of them in is
provided in the evaluation section.  Results have been produced for a
Logistic Regression system using a Stochastic Gradient Descent
algorithm on Mahout, a distributed Machine Learning system implemented
as a Java library on top of Hadoop, an open source clone of the Google
File System (GFS) and Google MapReduce system.  These results are
compared to a Logistic Regression system using a quasi Newton solver
as implemented in Weka, a popular system for Machine Learning and show
timing and classification results of this system run as a grid style
parallel job on Westgrid.  These results for Logistic Regression are
then compared to three different implementations of a Support Vector
Machine (SVM) classifier.  The first of these is simply the SVM mode
in Weka but run in a grid-style distributed manner.  The second of
these is a parallel version of the SVM algorithm, PSVM
\cite{chang07psvm}.  The third of these is a hybrid audio feature
extraction and SVM developed in the Marsyas framework.  These three
different systems will be compared and contrasted in the Evaluation
section.

I propose to use the Westgrid system for running the huge audio
feature extraction task. This uses a very simple queuing system known
as Torque, which is the latest evolution of the ancient PBS (Parallel
Batch System) parallel job distribution system. PBS allows scientific
users to schedule large jobs to run on computers, and performs well
for certain scientific tasks where a single, usually large, program is
run on a variety of different datasets, and the results are then saved
to disk and later analyzed by a scientist. This type of computing was
traditionally known as Grid Computing.  While this system has certain
benefits for running specific scientific tasks, for other tasks, the
methodology of having separate computers running mostly independently,
and all coordination of tasks being the responsibility of the
programmer with an tool such as MPI quickly becomes difficult. The
MapReduce paradigm helps in the coordination of large parallel tasks,
and the Hadoop system allows programmers to efficiently write
distributed programs that run on huge numbers of computers and allows
for failures of individual computers. In this project we will use the
Mahout Machine Learning framework on top of a Hadoop installation to
process the audio feature vectors output by Marsyas and to gain
insights into the vocalization data in the Orchive. The primary task I
will investigate in this project will be identifying regions of the
Orchive with distinct orca vocalizations that are nearby to
hydrophones and isolated from other orca vocalizations.

Mahout is a Machine Learning framework that interacts closely with
Hadoop to both store the data and also run the Machine Learning
algorithms.  Hadoop is an open source port of the GFS
\cite{ghemawat03} and MapReduce \cite{dean08} infrastructure and
allows users to run large jobs that follow the map reduce paradigm.
In this scheme, a parallel job is split into two phases, a map phase
and a reduce phase.  In the map phase, key value pairs are generated
by some given algorithm from an input file, this same process occurs
in parallel on many other nodes, and the data that is used by a given
map is determined by the locality of data on that node.  These key
value pairs are then sorted and shuffled to a set of reduce nodes,
which take the collections of key value pairs and do an operation on
them that in some way combines, or reduces, the data.  These map and
reduce functions can be thought of as similar to the map and reduce
functions in functional programming, however, it should be noted that
this is a loose similarity due to the lack of higher order functions
in the current MapReduce implementation.  Many algorithms have been
ported to use this MapReduce type system, some of the most natural are
those that process large collections of text for building inverted
indexes or other tasks for searches, which is not surprising noting
the provenance of the MapReduce algorithm.  For other tasks, such as
those common in audio feature extraction, the reduce phase is simply
an identity reduce, where the map outputs are copied verbatim to the
reduce outputs.  It is easier and more natural to make some algorithms
more efficient in a MapReduce context than others.

The Mahout Machine Learning framework has implemented many different
Machine Learning algorithms in it's framework, including clustering,
recommendation and classification algorithms.  In our current work we
are more interested in classification than either recommendation or
clustering, and Mahout boasts support for the following classification
algorithms: Logistic Regression (SGD), Bayesian, Support Vector
Machines (SVM), Perceptron and Winnow, Neural Network, Random Forests,
Restricted Boltzmann Machines, Online Passive Aggressive, Boosting and
Hidden Markov Models.  However, after investigation, it appears most
of these algorithms are in a definite alpha state, and require
patching of the main source tree with external files.  The two most
well supported classification algorithms in Mahout are the Logistic
Regression classifier, using a Stochastic Gradient Descent (SGD)
engine, and a Naive Bayesian classifier.  Upon extensive
investigation, the Bayesian classifier makes many internal assumptions
of the input being of large bodies of written text and was not
suitable for our use case.  However, the Logistic Regression
classifier was well suited to our data and performed well in our
tests.

One advantage of using the Mahout Machine Learning framework was that
it stored its input and output data on HDFS, the Hadoop Distributed
Filesystem.  When working with the huge amounts of data that are
generated by audio feature extraction and Machine Learning
experiments, managing and transferring experimental results from the
grid servers to the production web servers is often a time consuming
and error prone procedure.  After our experience with Hadoop, we have
integrated it into web application and use the WebHDFS system to serve
all experimental results to users.  These users interact with a web
application that displays some data obtained from the production web
server but also displays the results of experiments as data served
live from a WebHDFS system that is being populated by the experimental
results from live requests from scientists.


\section{Evaluation}

Our evaluation section consists of three parts, in the first we
optimize basic audio feature extraction parameters, in the second we
use different distributed Machine Learning systems to generate
classification results, and in the third we give a report on our
experiences with these different platforms for feature extraction and
Machine Learning.

\subsection{Audio Feature Extraction}

Our first challenge when dealing with the Orchive is the huge size of
the data.  The raw uncompressed .wav files that were digitized from
tape take approximately 12TB on disk at the time of this analysis,
with more files being added all the time.  In order to determine if we
could use a compressed format for doing the audio feature extraction,
we compared the results of using a SVM machine to classify the frames
of audio.  For the original .wav file we got a classification accuracy
of 94.5\%, while a highly compressed 32kbs MP3 gave 93.6\% accuracy.
The disk quota on the Westgrid system is set by default at 1TB, so by
sacrificing a small amount of accuracy, it was possible to reduce the
disk space used from 12TB to 199GB, a 60x savings in space.

In order to test the different distributed audio classification
systems we first generated a set of training and testing data.  In a
previous paper \cite{ness2008}, we were able to obtain a
classification performance of 82\% when using a SVM classifier on hand
labelled data.  While this performance was adequate when used on a
small recording, when run on the entire Orchive, this performance
would lead to way too many false positives.  For this paper, we looked
in more detail at the training data, and found that while the
annotation boundaries were close to the start and end of the
vocalization, there was a small amount of silence before and after the
vocalization.  Using Audacity, we trimmed out all the silences of a 10
second region of audio of orca vocalizations, and did the same to a 10
second region of voice notes.  For the background data, we took one
hundred 0.1 second regions from random background annotations and
joined them together with the Linux audio utility program sox.  The
results for this can be found in the first line of table
\ref{table:handTrimmed} and had over 99.73\% of the instances
classified correctly.  This large jump in performance was unexpected
but easily understood, because if feature vectors of silence are
labelled as orca, this will cause issues for the classifier.  We then
took a 4 minute region of orca calls and voice notes and removed all
the silences from both of them.  We then did a preliminary test where
we reduced computation time by downsampling the feature vectors at a
rate of 100:1, this is shown in the next line of Table
\ref{table:handTrimmed}, and followed this by another test at 10:1
downsampling and one with no downsampling.  These tests gave
classification performance of 95.7\% to 97.7\%.  We then repeated the
analysis in our previous paper by using non-hand-trimmed audio, the
results are shown in the third section of the table.  We found results
from 88.1\% to 93.0\% for 100:1 and no downsampling.  The difference
of this result from the previous paper is likely due to the exact
training and testing sets used in the two papers.  In the current
paper, we are classifying only nearby and isolated orca vocalizations
and not the distant or noisy vocalizations, which was a small tweak to
the experimental setup that has also helped us to not only achieve
better classification performance but to deliver a science goal more
pertinent to the bioacoustics researchers who could use the Orchive.

\begin{table}
\begin{tabular}{|l|c|l|l|r|r|}
\hline
 trim & time (min)  & ds & features & \# & \% corr.  \\
\hline
 x & 10 sec  &  1   & all   &    2586  &    99.73  \\
\hline
 x & 4       & 100  & all   &     606  &    95.71  \\
 x & 4       & 10   & all   &    6060  &    97.19  \\ 
 x & 4       & 1    & all   &   60596  &    97.72  \\
 x & 4       & 100  & mfcc  &     606  &    94.88  \\
 x & 4       & 10   & mfcc  &    6060  &    96.03  \\
 x & 4       & 100  & yin   &     606  &    xx.xxxx  \\
 x & 4       & 10   & yin   &    6060  &    xx.xxxx  \\
\hline
   & 4       & 100  & all   &     621  &    88.08  \\
   & 4       & 10   & all   &    6202  &    92.26  \\
   & 4       & 1    & all   &   62023  &    93.01  \\
   & 4       & 100  & mfcc  &     621  &    87.76  \\
   & 4       & 10   & mfcc  &    6202  &    91.52  \\
\hline
\end{tabular}
\caption{Classification results with hand trimmed orca vocalizations
  using bextract to generate audio features and the SVM classifier in
  Weka to do a 10-fold crossvalidation of these features.}
\label{table:handTrimmed}
\end{table}

The next task that we worked on was to determine the optimal settings
for the audio extraction algorithm.  While there are a number of other
audio extraction frameworks in existence like jMIR \cite{mckayphd},
SmIrK \cite{wang07} and AIMC \cite{waltersphd}, the Marsyas framework
implements most, if not all, of the most popular audio feature
extraction algorithms, and presents them as output from the
``bextract'' program as .arff files, which are the input to the Weka
suite of Machine Learning programs.  For all experiments here, we use
the audio features calculated by Marsyas, however in the future it
would be desirable to integrate other audio processing frameworks
within OpenMIR.

The features output by bextract include the number of zero crossings
per unit time, three spectral descriptors (centroid, rolloff, flux),
Mel-Frequency Cepstral Coefficients (MFCC) and chroma information
based on the western equally tempered scale.  The mean and standard
deviation of each of these over a window is then calculated and forms
the output.  The first experiment we did was to determine if the MFCC
coefficients contained enough information to do the classification on
their own, the results for this are shown in Table
\ref{table:handTrimmed} in the columns with ``mfcc'' in the
``features'' column.  For the hand trimmed example at a downsampling
of 10:1, the performance goes from 97.2\% to 96.0\% which is a small
but meaningful difference, especially when one considers how many more
false positives one would get when looking at the entire Orchive.  We
also tried adding the Yin pitch estimator as another feature in the
feature vector output by bextract.  Surprisingly, as one can see in
the previous table, the addition of this information actually degraded
performance of the classifier.  We are currently investigating how to
better incorporate pitch tracking information in our classifiers.

The next set of parameters that needed to be optimized were the Window
Size and Hop Size of the Digital Signal Processing (DSP) algorithms
that take the input audio and calculate spectral information from
them, the fundamental basis for which is the Fast Fourier Transform
(FFT) algorithm.  One other important input to the bextract feature
extraction algorithm is the length of time over which to calculate the
statistical properties of the features, this is known in bextract as
the ``memory'' and corresponds to the number of frames of features
that are accumulated.  The results for this are shown in Table
\ref{table:dspParams}.  From this we can see that as we go to longer
window sizes, the classification performance increases, and as we go
to longer accumulation window sizes, the performance also increases.

For our experiments we decided that a good sweet spot was a hop size
size of 1024 and a memory of 40 frames, which gave a classification
performance of 99.7\%.  80 frames of audio at a sampling rate of 44100
samples/sec and a hop size of 1024 would result in a feature length of
1.86 seconds, while 40 frames would only give 0.93 seconds of audio
per feature.  Given that orca vocalizations are usually between 0.5
and 3 seconds long, we chose a window size of 2048, a hop size of 1024
with an accumulator memory of 40 frames.  We did extensive experiments
with other values of hopsize, window size and memory that were
consistent with these results that are too long to report here.

\begin{table}
\begin{tabular}{|r|r|r|r|r|r|}
\hline
 winsize  &  hopsize  &  memory  &  total &   correct  \\
\hline
     256  &      128  &      20  &        13767   &    97.11  \\
     512  &      256  &      20  &        11647   &    96.11  \\
    1024  &      512  &      20  &         5923   &    97.74  \\
    2048  &     1024  &      20  &         2995   &    98.84  \\
\hline
     256  &      128  &      40  &        23311   &    96.18  \\
     512  &      256  &      40  &        11829   &    97.61  \\
    1024  &      512  &      40  &         5991   &    98.86  \\
    2048  &     1024  &      40  &         3022   &    99.74  \\
\hline
     256  &      128  &      80  &        23628   &    97.48  \\
     512  &      256  &      80  &        11963   &    98.71  \\
    1024  &      512  &      80  &         6044   &    99.74  \\
    2048  &     1024  &      80  &         3027   &    99.90  \\
\hline
\end{tabular}
\caption{DSP parameters}
\label{table:dspParams}
\end{table}

\subsection{Machine Learning}

The main distributed Machine Learning framework we used was Mahout,
which as we previous described is a Java framework for Machine
Learning that operates on top of Hadoop.  We investigated a number of
its algorithms and found that the Logistic Regression algorithm was
the only classification algorithm that was both suitable for our
problem and incorporated into the main source code trunk of the code.
Our first experiment was just to test the two parameters suggested for
tuning this algorithm are shown in Table
\ref{table:logisticRegressionTests}, and from these brief experiments
it appears the default parameters work best on this data.

\begin{table}
\begin{tabular}{|r|r|l|}
\hline
 Passes  & Rate  & Percent Correct                                         \\
\hline
    100  &    50  &  85.5  \\
   1000  &    50  &  85.5  \\
    100  &     5  &  83.0  \\
    100  &   500  &  85.5  \\
\hline
\end{tabular}
\caption{Logistic regression tests with different parameters.}
\label{table:logisticRegressionTests}
\end{table}

We then took a randomly selected 13GB portion of the 500GB of total
(one of the original 37 shards from feature extraction) audio features
calculated in the previous section and ran a comparison of the Mahout
Logistic Regression algorithm against a simple parallel
implementation using a script that runs Weka jobs on multiple
computers using the Torque/PBS system on the Westgrid cluster.

For this we had our choice of a number of systems on which to run a
Mahout/Hadoop cluster on and investigated all of them.  The first was
Emulab, which is a fine platform and is very useful for a number of
use cases, but for this case of obtaining 10-20 computers and storing
substantial amounts of long lived data on them did not fit the Emulab
model of running distributed programs on different types of emulated
networks on short lived borrowed computers.  While it would have been
possible to use Emulab, we were hoping for a longer term solution.
Planetlab allows for longer leases, but the large memory and CPU
requirements of our program made this less feasible as well.  We
attempted to obtain machines on the GeniCloud at HP, but were unable
to obtain the machines in the end.  The best solution we felt was to
use the six GreenGeni nodes at UVIC and setup a Hadoop cluster on
these.  We attempted to do this for quite some time, but ran into a
number of odd port issues.  We suspect the problem are concurrently
running Swift and Disco installs on these machines that might be
utilizing these ephemerally used ports.  The problem could also be due
to a misconfiguration of the networking hardware.  Another solution
would be to use the Amazon EC2 cluster service, or even their Elastic
MapReduce service, a service that is easy to setup and use, however,
costs for it can add up quickly.  For the results in this paper we use
a mini 2-node Hadoop cluster that was setup in our lab in a controlled
setting, this was simple to install and get results from.  In the near
future we hope to setup a larger Hadoop cluster and to rerun these
jobs with more processors.

For all of our analysis here, we took one of the 37 splits of the
original data analyzed by bextract.  This file was 13GB in size and
contained 22486467 lines.  For the following experiments we took the
first $n$ lines of this file where $n$ started at 10 and increased to
10,000,000 in powers of 10.

For the first experimental condition, the Logistic Regression
Classifier as implemented in Mahout was used, and Hadoop was used as
the underlying system below Mahout.  The Logistic Regression
classifier as implemented by default in Mahout can only take two
classes as input, so for all these experiments we used only orca and
background as the training data.  The timing results of Logistic
Regression in Mahout are shown in Table
\ref{table:machineLearningTiming1}.  As we can see, the system is very
fast, even classifying a million instances only takes one minute on a
small two node cluster.  As the number of instances increases, the
time increases quickly, it would be interesting to see this with a
larger cluster of 10 or more nodes.

For the second experimental condition, we used the Weka Machine
Learning package and ran it with it's Logistic Regression engine.
This was run on the Hermes cluster at Westgrid, and for these tests,
only 10 nodes were run at one time, although scaling up is as simple
as splitting the input file into more chunks and starting more jobs.
Weka is a Java program, and thus incurs a certain startup time when
creating and setting up the JVM and other resources.  This is
reflected in the results in Table \ref{table:machineLearningTiming2}
where up to 100000 instances the run times are all around 6 seconds.
The third experimental condition was to use Weka again, but this time
with its SVM engine.  Interestingly, the time that the SVM took to run
was about the same as the Logistic Regression, even though SVM is
typically be a considerably better classifier than Logistic
Regression.

In the final experiment, PSVM was run on the Checkers cluster at
Westgrid.  It was unable to be run on the Hermes and Nestor clusters
due to the use of MPICH2 by PSVM and the use of the original MPI
version 1.0 on the clusters.  This program takes advantage of the MPI
message passing library to communicate between a number of different
computers, and for the experiments shown below, we used 4 processors.
It should be noted that at least on the Checkers cluster, it is
difficult to reserve a block of 5 computers at once, much more
difficult than starting 5 individual jobs.  Depending on the cluster
you have access to, this may or may not be a problem.


\begin{table}
\begin{tabular}{|l|r|r|r|}
\hline
 system  &  \# proc.  &  num instances  &  time (sec)  \\
\hline
 Mahout  &     2          &            100  &       0.55  \\
 Mahout  &     2          &           1000  &       0.89  \\
 Mahout  &     2          &          10000  &       1.45  \\
 Mahout  &     2          &         100000  &       6.69  \\
 Mahout  &     2          &        1000000  &      57.90  \\
 Mahout  &     2          &       10000000  &     566.35  \\
 Mahout  &     2          &       22486467  &     566.35  \\
\hline
 PSVM     &           10  &            100  &     2  \\
 PSVM     &           10  &           1000  &     1  \\
 PSVM     &           10  &          10000  &     1  \\
 PSVM     &           10  &         100000  &     50  \\
 PSVM     &           10  &        1000000  &     635  \\
 PSVM     &           10  &       10000000  &       \\
 PSVM     &           10  &       22486467  &       \\
\hline
\end{tabular}
\caption{Timing results for all Machine Learning Algorithms}
\label{table:machineLearningTiming1}
\end{table}

\begin{table}
\begin{tabular}{|l|r|r|r|}
\hline
 system  &  \# proc.  &  num instances  &  time (sec)  \\
\hline
 Weka : LogReg    &           10  &            100  &        6.03  \\
 Weka : LogReg    &           10  &           1000  &        6.05  \\
 Weka : LogReg    &           10  &          10000  &        6.09  \\
 Weka : LogReg    &           10  &         100000  &        7.55  \\
 Weka : LogReg    &           10  &        1000000  &       31.24  \\
 Weka : LogReg    &           10  &       10000000  &      183.74  \\
 Weka : LogReg    &           10  &       22486467  &      233.99  \\
\hline
 Weka : SVM     &           10  &            100  &        5.38  \\
 Weka : SVM     &           10  &           1000  &        5.35  \\
 Weka : SVM     &           10  &          10000  &        5.15  \\
 Weka : SVM     &           10  &         100000  &        7.53  \\
 Weka : SVM     &           10  &        1000000  &       28.66  \\
 Weka : SVM     &           10  &       10000000  &      182.61  \\
 Weka : SVM     &           10  &       22486467  &      	  \\
\hline
\end{tabular}
\caption{Timing results for all Machine Learning Algorithms}
\label{table:machineLearningTiming2}
\end{table}


\subsection{Experience Report}

Probably the most difficult part of this project was the install, use
and development of Mahout.  In order to even install Mahout, it was
necessary to install Hadoop, a process that was very rewarding
although equally challenging with a vertical learning curve.  It
turned out the problem with our first trial install of Hadoop was that
some of it's reserved ports were in use by another experimental
distributed filesystem, Swift.  There were other issues we
encountered, but when these were worked out, future installs were
straightforward.  Once Hadoop was installed, we downloaded Mahout in
the recommended way, from the trunk of the Subversion repository,
which spoke to the early development status of the project.  We had
considerable problems when trying to get the example programs to run,
it turned out that a combination of issues with Mahout, Hadoop, the
Java install on our Ubuntu servers and Unix environment issues were
the source of the problems.  These issues only took about three weeks
to solve, and as they were one-time issues, they were not onerous.  In
a real production environment, dedicated Site Reliability Engineers
would ensure the availablity of the basic Hadoop framework and
possibly also Mahout as well, and the user would only need to use
these powerful tools.

We first investigated the SVM classifier in Mahout, but discovered it
was research software of alpha quality and required the patching of
the main Mahout system, this was disappointing, as one of the reasons
we chose Mahout was because of its supposed wide coverage of Machine
Learning algorithms.  While for doing Machine Learning on text Mahout
has many well developed algorithms, for doing classification of dense
vectors like those commonly found in audio applications, it's built in
app support is limited.  However, as we discovered, Mahout is best
used as a Java library and the programs that are provided for doing
Machine Learning from the command line expose only the most basic
functionality of the toolkit.

We did considerable investigation into using Mahout to develop Java
programs, and used the Mahout Java API to write programs that did
various utility functions and modified a program to do Logistic
Regression.  We found that Mahout is a powerful toolkit to use in Java
and contains support for many of the important data structures found
in Machine Learning such as dense vectors and sparse vectors, many of
the algorithms for doing Machine Learning, and many algorithms for
testing models some of which include n-fold crossvalidation, ROC
curves and confusion matrices.  Mahout uses a MapReduce paradigm for
running its algorithms, and by default runs these algorithms on
Hadoop.  This makes certain aspects of writing Machine Learning
programs easier, but imposes a steep learning curve in order to use
these features.  All in all, we would definitely recommend Mahout to
other researchers, however, we would caution them about the difficult
first install and vertical learning curve of the API.


\section{Future work}

There remains considerable work to be done in regards to the Orchive
as a whole.  The first is to find more examples of clear, nearby,
isolated orca vocalizations and background noise.  There are 12,000
annotations, about half of which are orca vocalizations.  A rough
estimate would be that about 1 in 20 of these would be a loud,
isolated vocalization, so one task we are working on is developing
tools to quickly let us go through the existing annotations and to
make training sets for Machine Learning algorithms.  In combination
with this is the task to better trim the ends of orca calls to
eliminate the silences that confound our Machine Learning algorithms.
We are developing HTML5 based tools to help us to manually do this,
and are also refining existing and developing new algorithms to
automatically do this.

The most exciting directly applicable future work to the current work
involves doing large scale clustering and vector quantization (VQ) on
the loud, isolated, orca vocalizations found in this study.  The VQ
algorithm will give us as output an alphabet with an arbitrary sized
dictionary.  With this one dimensional quantized and clustered
representation of the audio, we plan to use tools and techniques from
Bioinformatics \cite{sarkar2002} and Symbolic Aggregate Approximation
\cite{lin07} to extract information from this archive at a large scale.
Although this project concentrated on classification, it is the
clustering capabilities of Mahout that would be useful for this task,
and would be even more appropriate for doing large scale clustering
than for doing classification.

\section{Conclusions}

We set out to do two things, first, to assign audio features to the
entire Orchive, and further to classify all the audio in it into the
classes of ``orca'' and ``background''.  We achieved this by using the
Westgrid cluster and the bextract and sfplugin programs that are part
of Marsyas.  For the bextract part it took approximately 3 hours of
wall clock time, run on 37 computers to calculate all the features of
the Orchive.  For the sfplugin part, which classified the audio, we
ran on 42 nodes and it took 15 hours to annotate the entire archive.
This is an amazing speedup in itself, we had always considered that
calculating the features would take such a long time that we would at
least have to cache the intermediate features, but this assumption now
needs to be challenged.

The second thing we set out to do was to compare a variety of
distributed computing systems for both their performance and their
ease of use.  The simple Weka system on PBS was shockingly good, and
was extremely simple to setup and use.  With Weka, one is able to do a
wide variety of algorithms and to compare their results.  

Mahout is an excellent system, with many powerful capabilities and
with deep integration into Hadoop.  In our tests, we found the most
difficult thing as a student was to get the resources to run a
reasonably large (10-20 node) Hadoop cluster.  As we described in the
report, it is no small feat to get the Hadoop cluster installed and
running, especially on unknown hardware, but once this is done, Mahout
is simple to use and allows the storage of results on HDFS, the use of
which is almost essential, given the quantity of final and
intermediate results that need to be stored for a typical experimental
setup.  

Bextract in it's hybrid mode is perhaps the most useful system of this
entire group.  Because it does not need to generate intermediate
feature files that take huge amounts of disk space and often need to
be transferred to other computers to do the Machine Learning, it can
lead to a huge savings in time.  It is almost like having a mini
cluster within your one job, one where you do the feature extraction
and Machine Learning in one process.  All of the final classification
results that we ran were with this hybrid system.

{\footnotesize \bibliographystyle{acm}
\bibliography{distrib2012termpaper}}

\end{document}







% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Wonderful : A Terrific Application and Fascinating Paper}

%for single author (just remove % characters)
\author{
{\rm Your N.\ Here}\\
Your Institution
\and
{\rm Second Name}\\
Second Institution
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract}
Your Abstract Text Goes Here.  Just a few facts.
Whet our appetites.

\section{Introduction}

A paragraph of text goes here.  Lots of text.  Plenty of interesting
text. \\

More fascinating text. Features\endnote{Remember to use endnotes, not footnotes!} galore, plethora of promises.\\

\section{This is Another Section}

Some embedded literal typset code might 
look like the following :

{\tt \small
\begin{verbatim}
int wrap_fact(ClientData clientData,
              Tcl_Interp *interp,
              int argc, char *argv[]) {
    int result;
    int arg0;
    if (argc != 2) {
        interp->result = "wrong # args";
        return TCL_ERROR;
    }
    arg0 = atoi(argv[1]);
    result = fact(arg0);
    sprintf(interp->result,"%d",result);
    return TCL_OK;
}
\end{verbatim}
}

Now we're going to cite somebody.  Watch for the cite tag.
Here it comes~\cite{Chaum1981,Diffie1976}.  The tilde character (\~{})
in the source means a non-breaking space.  This way, your reference will
always be attached to the word that preceded it, instead of going to the
next line.

\section{This Section has SubSections}
\subsection{First SubSection}

Here's a typical figure reference.  The figure is centered at the
top of the column.  It's scaled.  It's explicitly placed.  You'll
have to tweak the numbers to get what you want.\\

% you can also use the wonderful epsfig package...
\begin{figure}[t]
\begin{center}
\begin{picture}(300,150)(0,200)
\put(-15,-30){\special{psfile = fig1.ps hscale = 50 vscale = 50}}
\end{picture}\\
\end{center}
\caption{Wonderful Flowchart}
\end{figure}

This text came after the figure, so we'll casually refer to Figure 1
as we go on our merry way.

\subsection{New Subsection}

It can get tricky typesetting Tcl and C code in LaTeX because they share
a lot of mystical feelings about certain magic characters.  You
will have to do a lot of escaping to typeset curly braces and percent
signs, for example, like this:
``The {\tt \%module} directive
sets the name of the initialization function.  This is optional, but is
recommended if building a Tcl 7.5 module.
Everything inside the {\tt \%\{, \%\}}
block is copied directly into the output. allowing the inclusion of
header files and additional C code." \\

Sometimes you want to really call attention to a piece of text.  You
can center it in the column like this:
\begin{center}
{\tt \_1008e614\_Vector\_p}
\end{center}
and people will really notice it.\\

\noindent
The noindent at the start of this paragraph makes it clear that it's
a continuation of the preceding text, not a new para in its own right.


Now this is an ingenious way to get a forced space.
{\tt Real~$*$} and {\tt double~$*$} are equivalent. 

Now here is another way to call attention to a line of code, but instead
of centering it, we noindent and bold it.\\

\noindent
{\bf \tt size\_t : fread ptr size nobj stream } \\

And here we have made an indented para like a definition tag (dt)
in HTML.  You don't need a surrounding list macro pair.
\begin{itemize}
\item[]  {\tt fread} reads from {\tt stream} into the array {\tt ptr} at
most {\tt nobj} objects of size {\tt size}.   {\tt fread} returns
the number of objects read. 
\end{itemize}
This concludes the definitions tag.

\subsection{How to Build Your Paper}

You have to run {\tt latex} once to prepare your references for
munging.  Then run {\tt bibtex} to build your bibliography metadata.
Then run {\tt latex} twice to ensure all references have been resolved.
If your source file is called {\tt usenixTemplate.tex} and your {\tt
  bibtex} file is called {\tt usenixTemplate.bib}, here's what you do:
{\tt \small
\begin{verbatim}
latex usenixTemplate
bibtex usenixTemplate
latex usenixTemplate
latex usenixTemplate
\end{verbatim}
}


\subsection{Last SubSection}

Well, it's getting boring isn't it.  This is the last subsection
before we wrap it up.

\section{Acknowledgments}

A polite author always includes acknowledgments.  Thank everyone,
especially those who funded the work. 

\section{Availability}

It's great when this section says that MyWonderfulApp is free software, 
available via anonymous FTP from

\begin{center}
{\tt ftp.site.dom/pub/myname/Wonderful}\\
\end{center}

Also, it's even greater when you can write that information is also 
available on the Wonderful homepage at 

\begin{center}
{\tt http://www.site.dom/\~{}myname/SWIG}
\end{center}

Now we get serious and fill in those references.  Remember you will
have to run latex twice on the document in order to resolve those
cite tags you met earlier.  This is where they get resolved.
We've preserved some real ones in addition to the template-speak.
After the bibliography you are DONE.

{\footnotesize \bibliographystyle{acm}
\bibliography{distrib2012termpaper}}


\theendnotes

\end{document}







%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a template file for the global option of the SVJour class
%
% Copy it to a new file with a new name and use it as the basis
% for your article
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Choose either the first of the next two \documentclass lines for one
% column journals or the second for two column journals.
\documentclass[global,referee]{svjour}
%\documentclass[global,twocolumn,referee]{svjour}
% Remove option referee for final version
%
% Remove any % below to load the required packages
%\usepackage{latexsym}
\usepackage{graphics}
% etc
%
% Insert the name of "your" journal with the command below:
\journalname{myjournal}
%
\begin{document}
%
\title{Insert your title here}
\subtitle{Do you have a subtitle?\\ If so, write it here}
%\author{First author1\inst{1} \and Second author\inst{2}% etc
\author{Steven R. Ness\inst{1} \and Shawn Trail \inst{2} \and Peter Driessen \inst{3} \and George Tzanetakis \inst{4} \and W. Andrew Schloss\inst{5}
% \thanks is optional - remove next line if not needed
\thanks{\emph{Present address:} Insert the address here if needed}%
}                     % Do not remove
%
\offprints{}          % Insert a name or remove this line
%
\institute{Insert the first address here \and the second here}
%
\date{Received: date / Revised version: date}
% The correct dates will be entered by the editor
%
\maketitle
%
\begin{abstract}
Insert your abstract here.
\end{abstract}
%
%% \section{Introduction}
%% \label{intro}
%% Your text comes here. Separate text sections with
%% \section{Section title}
%% \label{sec:1}
%% \subsection{Subsection title}
%% \label{sec:2}
%% as required. Don't forget to give each section
%% and subsection a unique label (see Sect.~\ref{sec:1}).
%% \cite{tindale05}

\section{Introduction}

Auium (MISTIC).  In this group, we have been developing a wide range
of technologies (***list?- novel spatial DSP and digital synthesis,
electro-acoustic control interfacing, and Machine Learning and Music
Information Retrieval) to understand music, to create music and to
develop robots that can play acoustic instruments and their respective
electro-acoustic control interfaces.

Recently, the MISTIC group performed two instantiations of the fifth
MISTIC concert, a concert that featured Dr. Andrew Schloss' new piece
``sonicpair'', a duet between Dr. Schloss on Radiodrum and Joanna
Hood, a classically trained violinist, accompanied by robotic
percussion and real-time audio DSP using a microphone amplification
system.  For this work, Schloss created a piece that invokes
variations for the Notomoton, a newly developed novel robotic drum
instrument and several other modular robotic frame drums. The
Radiodrum is used to play the robotic percussion, and also manipulate
the audio from the microphone- which could be processing the viola or
Noto. On the viola, a variety of extended techniques were used,
including rubbing the bow gently on the strings, as well as, different
plucking and scratching techniques. Also performing at this concert
was Trimpin, the father of musical robots, along with Ajay Kapur,
Co-director of Calarts robot program, and composer/researchers Arne
Eigenfeld, Darren Miller and Steven Ness. The concert showcased a
broad variety of musical contexts in which robotic music can and will
inevitably be approached. ***Andy, would you like to offer a brief
synopsis of each composer's piece to illustrate diversity in this
context?

Another example of a musical context (for our current/proposed work)
stemming from a broader North American perspective is Pat Metheny‚Äôs
recent Orchestrion Project. Performing on a digitally extended solo
guitar, Metheny activates an entire stage full of acoustic instruments
that have been equipped with actuators. Initiated in 2007 and
premiered in 2010 the massive project then went on nearly a year long
world tour. Shawn Trail, a new interdisciplinary Ph.D. student with
MISTIC, has been on tour and in pre-production of the Orchestrion
Project with Metheny, working as control interface/robotics technician
trying to keep this massive technical set-up functioning properly
night after night of full-capacity use. This tour represented one of
the first times that a well known popular musician (17 time Grammy
winner, 32 nominations) featured music robots prominently in their
show. Trail recently also assisted Dr. Schloss as stage technician and
sound engineer at the Sonic Boom festival. Schloss was performing a
solo piece entitled``Variations for Notomoton'' composed by David
Jaffe for the Radio Drum and robotic percussion. We will use this
instance to contrast with the Orchestrion as both works similarly
feature a traditionally trained acoustic instrumentalist interfacing a
robotic ensemble using adaptations of the proprietary technics of
their respective instruments (Schloss- percussion, Metheny- Guitar),
yet in dramatically different contexts (andy- compositions designed
for true improv/pat- full thru-composed orchestrations with some
improvisation). Because both situations had very controlled and
systemized live technical support and preproduction they serve quite
well as parallels representing distinct, musically idiosyncratic,
expert musicians with finite realizations of very specific and
technically demanding (both musically and equipment) musical
performance scenarios- both using the traditional stage context- yet
dealing with spatial acoustic sound reinforcement differently as each
scenario required.

In this paper we will first discuss previous work related to this,
then we provide a broad overview of the system description of both the
Orchestrion and MISTIC concerts. We will then compare these two
similar but quite contrasting examples of the uses of robotics in
music. The MISTIC group focuses primarily in the field of Music
Information Retrieval \cite{orio06} and applies these technologies to
the production of music from a research and development
platform. Metheny's project, commercial by nature, was more focused on
a final, fully realized body of performance art. Both scenarios are
similar in that they rely heavily on novel electro-acoustic control
interface methods which we will discuss in detail. The Orchestrion has
provided us an additional intensive opportunity to observe what does
and doesn't work in a rigorous performance context. We also have
extensive experience with our own ensemble of instruments in live
performance (funded by SSHRC, NSERC, CCA) and have learned what works
and what doesn‚Äôt. We believe that new instruments have a legitimate
place [Kapur2007] with potential to become part of an embedded
conventional musical practice, not just a research curiosity. While
musical-robotics might seem niche and esoteric at this point
[Burtner2004], historic innovations such as monophonic to polyphonic
music, electrical amplification of the guitar, or computers in the
recording studio all brought skepticism, but eventually became
mainstay practices.

Proprioception In Robotic Music Instruments

At MISTIC we believe that the ability of a robotic instrument to
perceive at some level its own functioning is an interesting direction
to explore with strong potential. We refer to this ability as
‚Äúproprioception;‚Äù in its original definition, it refers to the ability
of an organism to perceive its own status. Our goal is to integrate
information from sensors that are already embedded in
hyperinstruments, along with acoustic analysis to enable
proprioception. For example we would like a robotic instrument to be
able to "know" how loud it is playing or to be able to detect whether
a mechanical part of it is malfunctioning. The combination of
hyperinstruments (digitally extended electro-acoustic instrument with
control-inferfacability), gesture sensors, and musical robots provide
many interesting possibilities for music performance as the detailed
control information obtained from the human performer can be used to
inform the automatically controlled robot. In the past we have
explored this combination in the context of North Indian music and
more specifically improvisation of a human sitar player with a robotic
percussion instrument. [Kapur2007] Proprioception in this context can
be achieved by merging sensors (hyperinstruments) and actuators
(robotic musical instruments) into a single physical device. The
direct sensors can provide both coarse information, such as whether a
particular solenoid actuator is stuck, as well as more detailed
nuanced information such as detecting that a robotic stick is striking
much harder using the same control input than the other sticks. This
information can also be used as a control feedback mechanism to adjust
the control input accordingly.

The system we propose performs a timbre classification of the incoming
audio automatically mapping solenoids correctly in real-time to the
note messages sent to the musically desired drum. We plan to use audio
signal processing and machine learning techniques to have robotic
musical instruments that "listen" to themselves. Our existing
experience utilizing direct sensors and indirect acquisition of
musical gestures through audio analysis [Tindale2011] will provide the
foundations of this research direction. A specific short term goal is
to devise a system that is not only diagnostic, but also is capable of
assisting the performer in subtle issues of velocity scaling that are
quite tedious to do by hand. We have implemented this in the Marsyas
\cite{Marsyas} dataflow architecture. For a performance, there would
be a number of different drums and a series of modular beaters. The
system would perform timbre analysis of the audio from a centrally
located microphone and would be able to calculate which beater to send
a message to make the desired musical sound. Broken beaters could be
detected by listening for silence during known moments a given
solenoid was triggered to play but didn't. By doing this, a performer
would not have to re-calibrate the solenoids during a performance, the
system would do it automatically.

Good velocity scaling is essential for a percussion instrument to give
a natural graduated response to subtle changes in gesture, e.g. a
slight increase in the strength (velocity) of a stroke should not
result in a sudden increase in the loudness of sound. Issues like
velocity seem quite pedestrian, or even trivial until one has grappled
with this problem with real instruments. Dynamics are a critical
aspect of musical performance, especially with percussion and string
instruments (such as in Andy/Pat's case), and are a major part of the
expressiveness available to the performer. The tasks to achieve good
velocity scaling are:

‚Ä¢ Constant sound from a single marimba bar (or drum) and actuator: We
‚Ä¢ repeatedly strike a marimba bar (or drum or string or other object)
‚Ä¢ with the same actuator, and monitor the sound (timbre) produced
‚Ä¢ (level, spectrum content, spectral envelope, dynamic envelope). We
‚Ä¢ then adjust the striking force to keep successive sounds as similar
‚Ä¢ as possible Constant sound from multiple objects: We repeatedly
‚Ä¢ strike a series of objects (e.g. different marimba pitches) and
‚Ä¢ monitor the sound (timbre) produced as above. We then adjust the
‚Ä¢ striking force to keep successive sounds as similar as possible. We
‚Ä¢ start with two objects, increasing the number of objects until
‚Ä¢ e.g. we use all of the marimba bars available.  Dynamic range
‚Ä¢ testing: We repeat the above tasks over a wide dynamic range. We
‚Ä¢ expect to observe larger variations in timbre at higher striking
‚Ä¢ forces, and thus the task of keeping the sound‚Äôs timbre constant for
‚Ä¢ a given fixed striking force will be more challenging at these
‚Ä¢ higher dynamic levels.

With self-aware musical robots in performance contexts, calibrating the velocity curves that control robots to the sonic output of the drums that the robots strike has always been a challenging endeavor.  One of the primary problems is that due to the physics of the drum beaters and membranes, small changes in the positioning of the beater can produce dramatic changes in the sound being produced.  Calibrating an array of multiple solenoids on different drums is difficult, and this problem is compounded by the fact that sometimes the beaters or the objects that are being struck move during a performance. 

***IMPORTANT TO DIFFERENTIATE- - Andy's excerpt on Japanese humanoid robots*** EXAMPLES OF WHAT WE DO LIKE AND WHAT WE DON'T!!! 
***Andy could you write?

Background

***Andy could you write?
- What motivates Andy as a composer and performer to pursue Radiodrum?
	- Spatial reach that it allows
	- 20 foot arms
	- Extend what humans can do
	- Do things that no human can do
	- Control interfaces - Radiodrum
	- Andy's composition and performance as a constant
	- avant garde - reference to lineage (?) 
	- could you describe your lineage versus that of Pat's which is jazz- ***metheny and andy tied by different traditions... backgrounds? ANDY can you comment?

Andy Bio: 
The Radio Drum:
The Notomoton:

At MISTIC we have also done work in the field of Computational
Ethnomusicology (****references- ***site peter biro, orchive), which
is generally described as the study of music with computational
modeling and simulation. By extending the application of Music
Information Retrieval (MIR) algorithms and systems to the archiving,
analysis, retrieval of and interactions with orally transmitted music
cultures we can design and build tools that help us organize,
understand and search large collections of music. Computational
Ethnomusicology encompasses a wide variety of ideas, algorithms,
tools, and systems that have been proposed to handle the increasingly
large and varied amounts of musical data available digitally. This
provides us with categorical access to various elements and points
throughout entire collections of field recordings, for instance. This
information also allows us to investigate the intrinsic qualities of
music related to a specific social function in order to generate novel
ways of interfacing robotic music instruments. A practical example of
this is found with the Gyil- the traditional xylophone of the Lobi
Nation from northwestern Ghana. A direct predecessor to the modern
conventional marimba, yet distinct, though, from other seemingly
similar West African xylophones (balaphone, marimba, etc) along with
other indigenous xylophone traditions because its technique, in which
a single musician sings, plays bass, comps chords, and improvises
simultaneously. The Gyil‚Äôs format of playing is related to the concept
of improvising used in jazz, and its repertoire is the most complex
and virtuosic of solo xylophone traditions. This important aspect of
the Gyil‚Äôs performance practice serves as a foundation for how we
might develop repertoire using digitally extended capabilities. The
Gyil's repertoire is quite foreign and abstract making it hard to
approach for many musicologists, which is one reason it remains under
researched. Through our system of MIR we can analyze the musical
content much more objectively and find patterns and distinct
structures and styles of playing much faster and more precisely-
making the information more accessible for understanding and using to
influence the development of new music.

Pat commissioned his instruments from the innovators of the field that he specifically chose based on a preconceived conceptual vision of what he wanted. Once they were fabricated and installed in his studio he then pursued mastery over the ensemble. His work was more aligned with that of a soloist with an orchestra performing a through composition. MISTIC is more concerned with continually developing an ever evolving modular, custom, DIY, open source, yet systematic framework. In this sense our motivation is to develop the technologies in a self-sufficient capacity. Musically speaking, our research has followed closely distinct forms of indigenous music looking at forms of improvisation, theoretical content, social functions of specific musics, and instrumental technics proprietary to those culturally specific instruments. It is not arbitrary or coincidental that we have looked at the correlation to drumming music and social-cultural forms of music as an impetus for our methodology. In our previous research on new musical resources (funded by SSHRC, NSERC and CCA), we concentrated on innovations in virtual controllers like the radiodrum [Boie1989], or in enhanced electroacoustic percussion instruments like the E-Drumset [Tindale2010]. The sound generation was either oriented towards electronics (physical models of acoustic instruments played through loudspeakers) or acoustics (various drums struck by a robotic mechanism). To create new musical instruments, we need an interdisciplinary group of designers, electrical engineers, computer scientists, and craftsman capable of designing and building new electroacoustic instruments, but also performers who understand what works musically. Similarly, a Lobi Gyil master from Ghana would design and build his xylophone, and then pursue mastery over it as a performer [Chernoff1979]. As it takes many years to realize and master an extended instrument, our goal is to develop the music technology that we utilize self-sufficiently in order to gain and describe a comprehensive understanding of what comprises the tools we use to realize our art. Formalized theory is essential when implementing complex technological performance systems. The utilization of proprietary instrumental techniques for real-time control of the extended digital functions effectively bridges musical human gestures and complex computer interaction methods in a way that is intuitive to both the performer and audience. This is why percussion remains a pervasive model for control interfaces at MISTIC- because of the direct relationship to sound and gesture. Pursuing this work, however, requires the development of a system for analysis and the standardization of design paradigms for interactivity in musical instruments. 

- Robots listening to themselves

Although, as we demonstrate, that it is possible to run this system on a single microphone based system using Audio analysis and MachineLearning techniques, in some particularly loud surroundings for example, a music concert, it may become difficult to correctly map solenoid activations to sounds.  For this reason we also are
working on a system of optical pickups \cite{overholt05} that would directly monitor the sonic output of that drum, theoretically providing an ultra discreet signal with no sympathetic interference.  With this system it would be  possible to accurately map and calibrate a beater's relationship to its respective drum.  

Less important for the mapping and calibrating, but a very important concept musically to the MISTIC group is the idea of Localized Sound Reinforcement (LSR) \cite{eargle04}.  Because many drums, strings and idiophones make sounds too quiet to be heard on stage, it is important to project their sound through speakers.  Typically, the amplified sounds from all the drums is projected through two speakers in a stereo configuration, or more in a multichannel audio system. However, the spatial non-proximity of the sound and the object producing the sound invokes a mild form of cognitive dissonance \cite{festinger57} where the mind tries to reconcile two different pieces of information.  In LSR, there is a speaker attached to each drum, and the amplified sound appears to come directly from instrument itself. By placing the amplification at the visual and primary auditory source of the drum amplified sound is still projected from its source rather than arbitrarily from loud speakers arranged accordingly around a venue.

- MIR applications to this (Marsyas)

- Acoustic instruments: correlate the necessity of mastery that an instrumentalist must have to be successful and associate with the same technical virtuosity required for relevant technological innovations

***Just to expand on my previous choice of title. Coping strategies generalizes nicely to describe various approaches: ignore (if it fails
live with it), mask/hide (trigger samples), redundancy, etc (you probably can think of some others).

We can then motivate self-awareness/embodiment/proprioception as an unexplored coping strategy and maybe provide a proof of concept experiment.	

\section{Comparison}

- MISTIC concert setup

Andy- Radiodrum, Macbook- Max/MSP 4 audio outputs, Noto., 5 Solonoids/modular beaters, 5 Frame drums arranged spatially in a cluster. The 2 lowest drums mic'd with Senn. to reinforce low transients that are lost without amplication, and a 414 on the Noto for processing, Elka footpedal to control Max Mapping Logic.

The robots were controlled using a Max/MSP \cite{puckette02} patch that took input from the radiodrum and an octave of foot pedals and sent OSC \cite{osc} messages to the drum controllers that were controlled by the new NotomotoN system developed by Dr. Ajay Kapur at CalArts \cite{ajay}. These drums had a series of solenoids attached to them, with four solenoids of differing design attached to each head.  For the piece that Andy composed, he needed a wider sonic palette and removed several of the solenoids and then attached them onto microphone stands which he then placed to beat frame drums. Calibrating this setup was time consuming and difficult. Minimal thought was given to the sound reinforcement and so the sound design of the piece suffered. However, simple but very effective approach was used to deal with the loss of lower transients from the two largest drums. By placing *senn. a proper mic for the occasion, and oriented the two drums staggered- left/rightish respectively and panning the signal on the board in rough approximation, one could emulate the reinforced sound originating from the general area of the source drum. Since the other drums were higher frequencies they cut through the room noise and retained their spatial characteristic, thus no further reinforcement was needed beyond the Max audio which balanced nicely in stereo configuration. 

%% - Use of NotomotoN (Kapur reference)
%% 	- Single drum with a number of beaters on the head to allow for
%% 		 rolls, flams and other such techniques
%% 	- Multiple kinds of solenoids for doing different kinds of
%% 		things.  Some are fast, some can hit hard- ajay actually said that wasn't the reason, it was more just because he had those already and used them to save on cost.
%% 	- Tuning the performance to the type of solenoid
%% 	- Drum has a USB plug and appears as a device in Max
%% 	- We found this sonically and artistically limiting so we removed
%% 		the solenoids and put them on mic stands and hit frame drums

Problems- one stereo bug, other? drums arranged around room, people would continually bump, had to constantly realign beaters, watching them fervently- sometimes getting so askew that they would no longer reach the intended drum surface and therefore not sound, crippling the performance. Often local sound engineers won't know how to deal with our unique situation- a dedicated sound engineer responsible for recreating the artist's intention is essential. We saw this in Vancouver and Open Space dealing with the provided house sound and accompanying sound engineer on duty. Open Space improved a bit because we had the previous concert the night before and more prepared for what was previously unexpected, but both suffered from lack of technician as crew member. For Andy's solo show it was a tandem production/performance duo and worked much more cohesively. Pat's tour could not have happened without a dedicated production crew, each crew member had specific roles and duties. Pat has had the same sound technician for more than 30 years, David Oakes. This type of relationship facilitates a symbiotic relationship between the art and technology.

Musical robot band:

Examples of automated, programmable musical instrument ensembles appear as early as 1200's. Described as fountains and musical automata, the flow of water alternated from one large tank to another at hourly or half-hourly intervals. This operation was achieved through the innovative use of hydraulic switching.[1] Created by al-Jazari, this musical automaton was was a boat with four automatic musicians that floated on a lake to entertain guests at royal drinking parties. It had a programmable drum machine with pegs (cams) that bump into little levers that operated the percussion. The drummer could be made to play different rhythms and different drum patterns if the pegs were moved around, performing more than fifty facial and body actions during each musical selection.

Ab≈´ al-'Iz Ibn IsmƒÅ'ƒ´l ibn al-RazƒÅz al-Jazarƒ´ (1136‚Äì1206) was an Kurd polymath: a scholar, inventor, mechanical engineer, craftsman, artist, mathematician and astronomer from Al-Jazira, Mesopotamia, who lived during the Islamic Golden Age (Middle Ages). He is best known for writing the Kit√°b f√≠ ma'rifat al-hiyal al-handasiyya (Book of Knowledge of Ingenious Mechanical Devices) in 1206, where he described fifty mechanical devices along with instructions on how to construct them.

While Metheny wasn't looking as far back as the 13th century A.D., he was specifically influenced by a specific icon of a past era, which was an engineering feat in itself. First and foremost was the Player Piano- he grandfather... he remembers playing with it, fascination, which led to the broader development of Orchestrions (explain)- hence the name of the project. Video

Pat was an early champion of the Synclavier System on stage. Pat developed one of the first guitar interfaces for synth control before MIDI existed. His style is so idiosyncratic and has been so influential over the years that, in one way, his sound has become iconic, almost period referential because he has chosen to retain very specific select guitar tones, while limiting the processing and sound design to be that of a finely engineered acoustic specimen with subtle, yet pervasive technological enhancement- much like Andy has chosen to master the Radiodrum, an unknown and unprecedented realm of exploration and development that has required many years of practice much the same way an acoustic instrument would. In using that as their developed language they then approach the bots as extensions of their very specific palate. It fully represents the coupling of past and future and makes for an exciting time, being able to witness the evolution of traditions in a very tangible form.

Regarding his own Orchestrion: Pat describes being able to create illusion of movement in time and space by constantly re-contextualizing the guitars relative timing in front of behind the master clock and because of inherent latency- both in getting the MIDI info to the bots and then acoustically in the space considering the orientation of each bot to each listener he is able to create a kind of swing specific to his taste. knowing the music so well, and how the bots perform it- he can predict the bots in way humans can't. Being able to overdub with yourself in a custom designed performance interface scenario designed as an extension of what is already happing pre-tech/interface so that it is a natural, organic, logical extension- it becomes a fingerprint with fingerprint a perfect match and much more innate. unconscious expression. able to execute what want instantly.

- How did Pat do it, how did we do it	
	- big budget / smaller budget
	- lots of time for development / not much time for development 
	- thousands of people at concert / 100-200 people for concert	
	- large concert halls / small art gallery and academic recital hall
	- robot drums were amplified / robot drums were minimally amplified
	- robot drums all on stage / robot drums in the audience as well as on stage
	- theatrical aspects / primarily sonic aspects

nightly setup and teardown- spell out the schedule- go get tour schedule- where? rigors- traveling roadshow

Mapping scenario:

-instruments get instructions from a variety of sources

Hexaphonic Guitar Pickup
-one to one mapping (notes he plays get played by bot- can control multiple instrument simultaneously)
-polyphonic

created very specific triggering environments for very specific musical pieces

pieces with improvisations-
ornet
live
first tune- one loop- using moog

M4L- (or Max) remapping notes from guitar to groups of percussion- selectable with foot controls

foot pedals have subsequent mapping logic running in Max

INSTRUMENTs:

Angeli! Picasso!

Rag West: (pneumatic)

Percussion Box- fathead
acoustic guitar- direct
ebass- direct

Interface

Bots- actuator/sound situation:

disklavier x 2 (1 baby grand- LDC, SDC, 1 upright)

lemur: AKG's- older C451's

piccolo snare drum- 2 solonoids, drumsticks. one mic-? small dia- dynamic

finger cymbals- single mic (dynamic or small diaphragm condenser) embedded natural metronome
hi hat- two solonoids mounted opposite to play double notes on closed hi-hat. 1 mic

floor tom- 2 beaters- mallet/stick- 1 akg

conga 1- 2 beaters- mallet/stick
conga 2- 2 beaters- mallet/stick
-both with small dynamic mics(senn. akg? sure?)

Bottles x 2= Ribbons? fatheads x2 each

cymbal x 3

30" bass drum- 2 solonoids. senn.

rack tom- mallet/stick, plate- beater, snare brush stick with tip- senn.

cymbal- w sticks, wood block, plate- one mic

18' bd- AT akg bd mic? one beater

marimba
glockenspeil- one overhead senn.
vibraphone
Guitarbot x 2

percussion

metal plate- 1solonoid, shaker box (rotary motor), shaker cylinder (rotary motor)- senn. x1 general
shaker group (rotary motor), bells (rotary motor), cymbal w 2 sticks/solonoids- senn x1

accordian- lavalier mic

- 100 inputs on board- audio?
- 2 hour concert- pieces? times?- 8-10 pieces

***- different control interfaces (parameters, triggers)
pedals- how? arduino mega- copy group 499- talk about max patch, arduino code- example of why t his worked well- changed behavior of pedal on spot- led's, max logic... original versus final used... better if led's controlled by max

- 3000-4000 people at each concret- amount of shows

- just pat and the orchestrion

- solenoids, motors and pneumatics

- drums, zylophones, wind, string, idiophones

- Eric Singer from LEMUR built robots

- Rag West

- Control scenarios:

	pitch to MIDI hexaphonic guitar pick-up embedded into his hollow body electric. 
	3 custom footpedals (photo- selecting bots), developed by Trail, using a Max patch for 
	switching logic.- compositionally embedded logic- 
	Moog Taurus Pedal (tempo, looping, transport controls)- overuse- cumbersome
	
Roland Synth

House sound- wedges, monitors, bose, subs
yamaha board- rack with computer- monitors through computer to house back to stage

- How to mic the setup
- Differences and similarities
- DIY aspects- list for both
- commercial- list for both- fireface vs. pat- problems
- control scenarios

Problems:

Most of the robots in both the Metheny and MISTIC concerts use solenoids to produce sound through striking an object.  Although diverse and useful\cite{kapur07}, solenoids have a variety of advantages but also have some drawbacks as well.  Many larger or older solenoids produce a click when activated. Some performers use this to great artistic effect \cite{kapur06}, however it is still a fact that must be attended to since it isn't always desired or appropriate.
	
	- why we need to look other places for robotic acoustic sound generation- linear conversion problem. 
	- velocity range
	
Other probs with solonoids

GuitarBot (photo):

The GuitarBot is a novel robotic string instrument. There are currently only 3 in existance. Developed by Eric Singer, the initial prototype incorporates electro-acoustic pickups. The other two were commissioned by Pat Metheny for the Orchestrion Tour, and have optical pickups. GB is comprised of four vertical strings tuned like a cello. The pitch of each string is established by a separate mechanism that runs along its length ‚Äòpinching‚Äô the specified pitch. The mechanism is driven by a belt and is controlled by MIDI Pitch CC‚Äôs. The string is excited by a rotary mechanism that spins at the bottom of each string using the four picks attached to it. The plucks are controlled by rhythmic sequencing in MIDI Rhythm CC‚Äôs. The instrument is intonated using Max by defining the spot on the string that represents the lowest note to be sounded. Each string is two octaves. The highest pitch of that particular sting is defined by then manually moving the ‚Äòpitch mechanism‚Äô until one finds the tonic of that string. The rest of the pitches between are established by the firmware on the instrument then running an interpolation based on the chromatic scale between the highest and lowest note of each string respectively. This is an extremely problematic process and is never accurate due to organic variations in placement inevitably present each time the mechanism selects a new pitch, even the slightest variation will result in the desired pitch being out of tune with other instruments (such as xylophones) that have fixed tunings.

HOW PAT SOLVED and IMPROVEMENTS TO BE MADE WITH MIR:

known intermittent short somewhere in the optical pickup of the low a string of guitar bot 2, and the fact that the tuning was never precise, it was  presumed that the guitarbots could not be left to their own fate on stage and therefore could not be trusted to faithfully reproduce the written melodies in tune. to complicate things Further other complications regarding consistency were created by the marimba mallet shafts bending therefor recontextualizing the preprogrammed velocity automations in the sequencing. Shifting of beaters on other drums, the pneumatic instruments sometimes getting ‚Äústuck‚Äù without indication of malfunctions, and problems with control interfacing greatly compounded the pressure put on the ‚Äòperformer of the bots‚Äô.

samples, sequencing, studio stems, to support bots irregularities without compromising fidelity.

MIR solutions: smart real-time self-tuning of guitar bot, beater self-realignment/calibration, 

- IT SHOULD INTERPOLATE THE SOUNDS ITSELF AND TUNE ITSELF
- LISTEN TO EACH NOTE AS IT IS PLAYING AND DYNAMICALLY ADAPT ITSELF

- DISTANCE OF SOLENOID IN DRUMS
	- PROBLEMS THAT THIS CAUSED IN PERFORMANCE, BOTH FOR PAT'S SETUP AND FOR OUR SETUP
	- OCCURRED IN PRACTICE IN BOTH CONCERTS/CONTEXTS

- MARIMBA BARS WOULD BEND
	- PRESET VELOCITY
	- WOULDN'T EXECUTE THE NOTES
	- PREPROGRAMMED VELOCITY CURVE NO LONGER RELEVANT (?)
	- BAR IN DIFFERENT RELATION TO MALLET (?)
	
because these were prototypes we had no system to remember where the original position was so the mallet	 																								   instruments were wild most of the time with unusable audio- samples were used to compensate for this. no previous long term- nightly gigging- only big one-off shows, etc. with much prep/breakdown time needed. had/need to be systimized.
	
HOW TO MOVE FORWARD

	- motors for actuators to move themselves to correct positions
	- OPTICAL PICKUPS- both audio and control data- discreet signal- new multichannel DSP potential   
	
	Other types of robots: 
		- kalimba model
		- string model  	 
		- peumatics
		- motors- types- where is that quote about pancake motors?                                                                                      
	
\section{Experimental}

For our experiments, we built a system that did an automatic mapping based on timbre classification. This was straightforward to do using existing Marsyas feature extraction and classification modules. These modules allow complex features to be calculated based on input audio, some of these include Fast Fourier Transform (FFT), Mel-Frequency Cepstral Coefficients (MFCC), Correlogram based approaches \cite{slaney93}

Basically there are (number) different drums each with a corresponding robotic beater. The goal is for the system to recognize which beater is beating which drum (including a silent drum for broken beaters) without having to explicitly specify the mapping using a microphone.

2) Automatic velocity calibration: Given a desired velocity acoustic response that can be obtained either by the actual robot or by a human performer re-adjust the current velocity response to match it. This could be done with a simple regression from energy or loudness or could incorporate multiple features to include timbral effects.

CRITICAL: For the experimental setup we will need a minimum of two beaters and two drums for the first experiment. For the second ideally we would like something that has a good dynamic range....

- MARSYAS AS A REALTIME SYSTEM FOR DEVELOPING SELF-AWARE MUSIC ROBOTS
	
\section{Conclusions}

\section{Future Work}

\section{Acknowledgments}

%
%
% BibTeX users please use
\bibliographystyle{acm}
\bibliography{eurasip2011gtzan}
%

\end{document}

% end of file template.tex

\documentclass[12pt]{article}
%\include{macros/style}
\usepackage{graphicx}  
%\include{macros/use_packages}
\usepackage{url} 


\begin{document}

% Front Matter
%\input frontmatter/fm

\newpage
\tableofcontents
\newpage
\listoffigures	
\newpage
\listoftables
\newpage

\section{Summary}

Traditional approaches to the problem of music genre classification
typically use Fast Fourier Transform (FFT) based methods combined with
Machine Learning Algorithms.  These FFT based methods have the
drawback that they only superficially model the human auditory system.
By using models that are more physiologically based, we hypothesize
that it will be possible to gain increased understanding and
classification performance in music genre classification tasks.  To
this end we have applied the technique of Stabilized Auditory Images
to this task, and to create a program that we will enter in the yearly
Music Information Retrieval Evaluation eXchange (MIREX 2010)
evaluation.  We present results that show that in certain tasks our
methods can outperform highly tuned FFT based approaches.


\section{Glossary}

AIM-C - A MIR framework written by Tom Walters from Cambridge (now at
Google) that performs many tasks, one of which is the calculation of
SAI images.\\

Cochlea - Part of the ear that transforms audio energy into nerve
impulses.\\

FFT - Fast Fourier Transform - A method that takes an input sequence
of audio and calculates it's spectrum, amongst many other things.\\

Flux - In this work, is the norm of the difference vector
between two successive magnitude/power spectra.  \\

MFCC coefficients - A way to transform a standard spectrum into
one that more closely approximates how the human ear perceives sound.
\\

Music Information Retrieval - also known by the
acronym MIR, a new field of study where one applies tools from areas
such as Digital Signal Processing, Audio Feature Extraction and
Machine Learning to help people understand and retrieve information
from music or audio.  \\

Rolloff - A measure of the steepness of falloff in an audio
spectrum \\

SAI - Stabilized Auditory Image - A 2D representation of sound with
both frequency and pitch axes. \\

Spectral Centroid - A measure of the ``center of mass'' of a spectrum.
\\

\section{Introduction}

The problem of music genre classification is becoming increasingly an
important topic of study as devices like the Google Android and Apple
iPod make music distribution ubiquitous.  With millions of songs in
the world instantly available, discovering new music that a given
listener will enjoy becomes more challenging.  Techniques from the
field of Music Information Retrieval are directly applicable to this
problem.  However, most of these techniques use an FFT based approach,
which only superficially approximates the way the human auditory system
works\cite{bregman90}.

In our system we have taken inspiration from the human auditory system
in order to come up with a rich set of audio features that are
intended to more closely model the audio features that we use to
listen and process music.  To this end we use the Stabilized Auditory
Image (SAI) \cite{lyon1990} \cite{patterson2000}, which combines
several different concepts, some of which directly model auditory
physiology and psychoacoustics, and some which are based more on a
general model of human auditory perception.  A single example frame of
a Stabilized Auditory Image is shown in Figure ~\ref{fig:sai}

\begin{figure}[here]
\includegraphics[width=150mm]{sai}
\caption{Stabilized Auditory Image \label{fig:sai}}
\end{figure}

This SAI image representation generates a 2D image of each section of
samples from an audio file.  We then reduce this large amount of
information in two steps, first by cutting the image into overlapping
boxes and finding row and column residuals of these boxes, and then by
vector quantizing the resulting high dimensionality vector.  The
resulting spare vector is then histogrammed across the audio file, and
this histogram is then used as input to an SVM \cite{yh05} classifier\cite{chapelle2006}.


\section{Discussion}

In our experiments, we generate an SAI image using a series of modules
which start by taking a stream of audio and run a series of filters
and modules on this data.  These modules filter the audio using a
Pole-Zero Filter Cascade, find strobe points in this audio, and
generate SAI images.  These images are then cut into boxes and are
transformed into a high dimensional feature dense feature vector
\cite{rehn2009} which is vector quantized to give a sparse high
dimensional feature vector.  This sparse high dimensional vector is
then used as input to a Machine Learning classifier, in our case a
Support Vector Machine (SVM) based classifier.  This whole process is
shown in diagrammatic form in Figure ~\ref{fig:flowchart}

\begin{figure}[htb]
\centering
\includegraphics[width=50mm]{flowchart}
\caption{Stabilized Auditory Image \label{fig:flowchart}}
\end{figure}

\subsection{Pole-Zero Filter Cascade}

We first process the audio with a Pole-Zero filter cascade (PZFC)
\cite{lyon1990}, a model inspired by the dynamics of the human cochlea.
When given an input audio signal, the PZFC produces a bank of a large
number of filters, in our case we used 95 such filters.  These filters
are bandpass-filtered and produce halfwave-rectified output signals.
These signals simulate the output of inner hair cells along the length
of the cochlea.  For this experiment we used the same parameters as
described in \cite{lyon2010}, which were tuned to classifying sound
effects.  We did not do any tuning of this system to the problems of
genre, mood or song classification, this would be a fruitful area of
further research.

\subsection{Image Stabilization}

The output of the PZFC filterbank is then subjected to a process of
strobe finding, where large peaks in the PZFC signal are found.  The
temporal locations of these peaks are then used to initiate a process
of temporal integration whereby the Stabilized Auditory Image is
generated.  These strobe points ``stabilize'' the signal in a manner
analogous the trigger mechanism in an oscilloscope.  When these strobe
points are found, a modified form of autocorrelation, known as Strobed
Temporal Integration, which is like a sparse version of
autocorrelation where only the strobe points are autocorrelated.  This
has the advantage of being considerably less computationally expensive
than full autocorrelation.

\subsection{Box Cutting}
We then divide each image into a number of overlapping boxes using the
same process described in \cite{lyon2010}.  We start with rectangles
of size 16x32 and tile the SAI image with these rectangles.  Each of
these rectangles is added to the set of rectangles to be used for
vector quantization.  We then double the height of the rectangle up to
the largest size that fits in an SAI frame.  Each of these doublings
is added to the set of rectangles.  We then double the width of each
rectangle up to the width of the SAI frame and add these rectangles to
the SAI frame.  The output of this step is a set of 44 overlapping
rectangles.  The process of boxcutting is shown in Figure
~\ref{fig:boxcutting}.


\begin{figure}[here]
\centering
\includegraphics[width=120mm]{boxcutting}
\caption{Stabilized Auditory Image \label{fig:boxcutting}}
\end{figure}

In order to reduce the dimensionality of these rectangles, we then
take their row and column residuals and join them together into a
single vector.  We then take all these vectors and concatenate them
into a single vector.

\subsection{Vector Quantization}
The resulting dense vector from the previous step is then converted to
a sparse representation by Vector Quantization.  For this we first
preprocessed a collection of 1000 music files from 10 genres by
performing the PZFC and SAI.  This gave a very large set of all SAI
frames sampled at all strobe locations for each file.  We then took
this set of SAI images and performed box cutting on them followed by
the calculation of row and column residuals.  All the vectors from
each of the boxes were then compared to each other and a dictionary of
200 entries was generated for each by using a nearest neighbour
algorithm.

This process required the processing of huge amounts of data.  For
each frame of audio data, an entire SAI frame was created, these
frames were then stacked in time to create a three-dimensional matrix
of values.  This process was repeated for each song in the 1000 song
test corpus.  Each image was divided into boxes, and for each of these
boxes we had to perform vector quantization across all frames and
across all songs.  It was only using the massive computing
infrastructure at Google that this process was possible to be
completed in a reasonable amount of time.

This resulting dictionary for all boxes was then used in the MIREX
experiment to convert the dense features from the box cutting step
into a set of sparse features, where each box was represented by a
vector of 200 elements, with only one element being non-zero.  The
sparse vectors for each box were then histogrammed over the entire
song to produce a sparse histogram for each song.

\subsection{Machine Learning}

For this system, we used the Support Vector Machine machine learning
system from libSVM which is included in the Marsyas\cite{Marsyas} framework.
Standard Marsyas SVM parameters were used in order to classify the
sparse, histogrammed representation of each song.  It should be noted
that SVM is not the ideal algorithm for doing classification on such a
sparse representation, and if time permitted, we would have instead
used the PAMIR Machine Learning algorithm as described in
\cite{lyon2010}.  This algorithm has been shown to outperform SVM on
this task, both in terms of execution speed and quality of results.

\subsection{Test Driven Development}

There was a large amount of code that was required to be written in a
short amount of time, and in order to convince ourselves that errors
had not been made in the process of porting this code from AIM-C to
Marsyas\cite{tzanetakis02}, it was decided that the Test Driven
Development\cite{tdd} (TDD) methodology would be a good fit.

In TDD, test code is first written, and then production code is
written to make the tests pass.  This is a very different philosophy
from regular software development or even more modern forms of
software development where tests are written after the code is
written, often by a second developer.  In TDD, the main coder writes
the tests, and writes the code to make the tests pass.  In this case,
we already had code that we knew worked, so we were able to use the
output of this code as a prototype around which to build our test
classes.

Our development methodology was to first run the original code and
collect the results.  We then wrote test code that incorporated these
results and would run inside the Marsyas framework.  We then would
port the original code to Marsyas, and work until we could get byte
for byte identical output from the two systems.  In a number of cases
this methodology proved very useful, as there were subtle differences
in philosophy between the two systems, some of which caused very
subtle differences in output that would have compromised the accuracy
of the system.


\subsection{MIREX 2010}

All of these algorithms were then ported to the Marsyas Music
Information Retrieval framework from AIM-C, and extensive tests were
written as described above.  These algorithms were submitted to the
MIREX 2010 competition as C++ code, which was then run by the
organizers on blind data.  As of this date, only results for two of
the four Train/Test tasks have been released.  One of these is for the
task of classifying Classical Composers, and the other is for
classifying the mood of a piece of Music.  There were 40 groups
participating in this evaluation, the most ever for MIREX, which gives
some indication about how this classification task is increasingly
important in the real world.  Below I present the results for the best
entry, the average of all entries, our entry, and the other entry for
the Marsyas system.  It is instructive to compare our result to that
of the standard Marsyas system because in large part we would like to
compare the SAI audio feature to the standard MFCC features, and since
both of these systems use the SVM classifier, we partially negate the
influence of the machine learning part of the problem.

For the Classical Composer task the results are shown in table
~\ref{table:classical} and for the Mood classification task, results are
shown in table ~\ref{table:mood}

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|} \hline
Algorithm & Classification Accuracy \\\hline
SAI/VQ & 0.4987 \\
Marsyas MFCC & 0.4430 \\
Best & 0.6526 \\
Average & 0.455 \\ \hline

\end{tabular}
\caption{Classical Composer Train/Test Classification task}
\label{table:classical}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|} \hline
Algorithm & Classification Accuracy \\\hline
SAI/VQ &  0.4861 \\
Marsyas MFCC & 0.5750\\
Best &  0.6417 \\
Average &  0.49 \\ \hline

\end{tabular}
\caption{Music Mood Train/Test Classification task}
\label{table:mood}
\end{table}

From these results we can see that in the Classical Composer task we
outperformed the traditional Marsyas system which has been tuned over
the course of a number of years to perform well.  This gives us the
indication that the use of these SAI features has promise.  However,
we underperform the best algorithm, which means that there is work to
be done in terms of testing different machine learning algorithms that
would be better suited to this type of data.  However, in a more
detailed analysis of the results, which is shown in
~\ref{fig:perclass}, it is evident that each of the algorithms has a
wide range of performance on different classes.  This graph shows that
the most well predicted in our SAI/VQ classifier overlap significantly
with those from the highest scoring classification engines.

\begin{figure}[here]
\includegraphics[width=150mm]{perclass}
\caption{Per Class Results for Classical Composer \label{fig:perclass}}
\end{figure}

In the Mood task, we underperform both Marsyas and the leading
algorithm.  This is interesting and might speak to the fact that we
did not tune the parameters of this algorithm for the task of music
classification, but instead used the parameters that worked best for
the classification of sound effects.  Music mood might be a feature
that has spectral aspects that evolve over longer time periods than
other features.  For this reason, it would be important to search for
other parameters in the SAI algorithm that would perform well for
other tasks in Music Information Retrieval.


For these results, due to time constraints, we only used the SVM
classifier on the SAI histograms.  This has been shown in
\cite{lyon2010} to be an inferior classifier for this type of sparse,
high-dimensional data than the PAMIR algorithm.  In the future, we
would like to add the PAMIR algorithm to Marsyas and to try these
experiments using this new classifier.  It was observed that the MIR
community is increasingly becoming focused on advanced Machine
Learning techniques, and it is clear that it will be critical to both
try different Machine Learning algorithms on these audio features as
well as to perform wider sweeps of parameters for these classifiers.
Both of these will be important in increasing the performance of these
novel audio features.



\section{Conclusions}

In this project, a large amount of code was ported from the
AIM-C system to Marsyas and was evaluated in the MIREX 2010
competition.  These features are novel and hold great promise in the
field of MIR for the classification of music as well as other tasks,
including pitch determination.  Some of the results obtained were
better than that of a highly tuned MIR system on blind data.  In this
task we were able to expose the MIR community to these new audio
features.  These new audio features have been shown in other work
\cite{lyon2010} to outperform spectral features, such as MFCC, on
certain types of audio, and by evaluating these features with Machine
Learning algorithms more suited for these high dimensional, sparse
features, we have great hope that we will obtain even better results
in future MIREX evaluations.

\section{Recommendations}

The next steps in this research are clear.  First of all, the PAMIR
system must be tried on the current SAI intervalgrams using the set of
data we have already used.  The SVM algorithm is not well suited to
this type of data, and PAMIR should be able to give an increase in
classification accuracy, and a huge speedup in terms of time.
Secondly, more parameters of the PZFC filters and SAI intervalgrams
should be tried to find settings that work better on this task.
Third, larger sets of training data must be found and used in order to
evaluate this system.  The current 1000 song training set has
performed well in the past, but by using a much larger corpus of data,
we will be able to more finely tune this system.


\section{References}

\bibliographystyle{IEEEtranS}
\bibliography{google2010}




\end{document}
% Created 2008-11-04 Tue 14:33
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{morefloats}
\usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}

\title{Co-op report}
\author{Steven Ness}
\date{10 Oct 2011}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents


\section{Introduction}

During the summer of 2011, I interned in the Machine Hearing group at
Google at their main campus in Mountain View, California.  I worked as
an intern under the mentorship of Dick Lyon, the leader of the Machine
Hearing group.  The Machine Hearing group is part of Google Research,
a part of Google that does academic research on topics such as Machine
Vision, Machine Learning and Natural Language Processing.  In our
office were such notable researchers as Peter Norvig, the author of
the standard textbook in Machine Learning, who I was privileged to
interact with on a few occasions.

The Machine Hearing team at Google is involved with a wide variety of
projects involving sound and music, and is also active in the academic
community, regularly publishing papers and book chapters.  As a
result of my internship at Google last summer, I published a book
chapter \cite{sness10} with Dick Lyon and Tom Walters on the work that
I had done with them, and other work completed previously in their
group. This book chapter was on Auditory Sparse Coding, a new approach
to analyzing audio using models of the human auditory system and
advanced machine learning techniques that incorporate sparsity into
the way they solve problems.

The project that I was given this summer involved developing new
Digital Signal Processing based models of the human auditory
periphery, including the cochlea and olivary complexes.  My specific
project was to develop a model of the Medial Superior Olive, a neural
processing region in the auditory peripheral system that determines
the location of sounds based on the difference in time that a sound
arrives at both ears.  In my project I was tasked with developing
these models, and then developing visualizers to display these various
kinds of data.

However, the week that I started my summer internship, Google released
Google Music, a new cloud based music service that allows you to
upload all your music to the cloud, and play it from any internet
connected device, including laptops, tablets and mobile phones.  Our
small group was directly responsible for the automated music
playlisting feature in Google Music, this feature allows one to pick a
seed track and will then generate a playlist of similar songs.
Because of this, I took on an additional two projects, the first was
to add new audio feature extractors to their mobile client.  The
second project was even more exciting, and involved calculating and
visualizing different audio features of music using a web based
interface that would be used internally at Google to debug and enhance
their pipeline.

In addition, the first summer I was at Google, I got the latest
Marsyas audio feature extraction framework into the Google codebase.
Marsyas \cite{tzanetakis00} is a framework developed by my supervisor,
Dr. George Tzanetakis, here at the University of Victoria, and is one
of the leading frameworks for audio feature extraction and machine
learning.  Because of my familiarity with it, I did a number of small
projects involving this, including fixing bugs that prevented us from
using the Python bindings to Marsyas.



\section{Background}

The field of Music Information Retrieval (MIR) or Machine Hearing
\cite{Marsyas} attempts to teach computers to understand music.  There
are a wide variety of tasks in this field that range from music
playlisting to analyzing Gregorian chants and Bach fugues.  Both in my
group at UVIC and at Google, we are specifically interested in
approaches that take audio as input.  From this audio we calculate a
variety of features, including the low, medium and high frequency
content and how these frequencies evolve over time.  We then use
advanced machine learning algorithms including Support Vector Machines
and Deep Belief Networks \cite{bengio2007}.  

\begin{figure}[t]
\begin{center}
\includegraphics[width=50mm]{images/spectrogram}
\caption{
A spectrogram of the human voice reciting a short passage of text.} 
\label{fig:spectrogram} 
\end{center} 
\end{figure} 

Most current approaches in MIR use spectral based approaches that use
the Fast Fourier Algorithm to decompose a signal into a set of
sinusoid's with a specific frequency and phase.  An example spectrogram
of a human voice is shown in Figure \ref{fig:spectrogram}.  These approaches are
computationally efficient and fast, and give us a good understanding
of many features of music.  Using these features, the field of MIR has
had many successes, in the field of genre recognition, for example, we
often can achieve a classification accuracy of 80\%.  In the past 5
years, most of the advances in MIR have come from applying more and
more advanced machine learning algorithms to this problem.  It now
appears that we have reached a plateau where the performance of these
systems is not increasing, and it is felt that by using better audio
features, performance can be again improved.


FFT based approaches are fundamentally different from how our ear
actually hears sound.  FFT approaches take a window of data and
decompose this window into different sinusoids with a period and
phase.  The choice of window size involves a tradeoff between time
resolution and frequency resolution, and in order to increase time
resolution, frequency resolution must be decreased.  For certain
sounds produced in music, like those of sustained notes, this model
works well, but for many others, such as drum hits or the pulse
resonance phenomenon found in human voices, this model has
limitations.

The models of the human auditory system (Figure \ref{fig:humanear}
developed both by Dick Lyon \cite{slaney93} and other researchers
around the world have a fundamentally different approach in which
audio is filtered by a filterbank cascade that models features of the
human cochlea, the outputs of these filters are then processed by a
mechanism that is modeled on higher levels in the auditory periphery
that take this filterbank audio and generate two dimensional movie
frames that contain both frequency and an autocorrelation axis.  These
frames contain the fine timing information that is utilized by the
human hearing system to separate, localize and identify sounds.


\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/humanear}
\caption{
The human ear and parts of the human auditory periphery.  The
cochlea, the main object of our DSP modelling, is shown in purple.} 
\label{fig:humanear} 
\end{center} 
\end{figure} 

We are interested in developing these models to help us with different
machine learning tasks in music, including classifying songs into
genres, finding the most similar songs to a seed song and categorizing
sounds with tags.

\section{Methods and Results}

For my main project, I ported a new model of the human cochlea called
the Cascade of Asymmetric Filters and Resonators (CARFAC).  This model
is the latest in a series of ever more accurate and efficient models
of the human auditory system \cite{lyon82}.  Last summer I was
involved with investigating the performance of the Pole Zero Filter
Cascade (PZFC) model \cite{lyon10} in a variety of audio tasks, where
it performed very well.  This PZFC model is shown schematically in
Figure \ref{fig:dsppzfc}

\begin{figure}[t]
\begin{center}
\includegraphics[width=50mm]{images/dsppzfc}
\caption{
A schematic version of the Pole Zero Filter Cascade, a model of the
human cochlea and parts of the auditory periphery, developed by Dick Lyon.} 
\label{fig:dsppzfc} 
\end{center} 
\end{figure} 

The CARFAC model includes a more advanced treatment of the fast acting
compression that the Outer Hair Cells in the cochlea perform to allow
the ear to hear both very loud and very soft sounds, a technique
formally referred to as Automatic Gain Control (AGC).  This model is
more advanced than the PZFC model, and this additional complexity can
be seen in Figure \ref{fig:dspcarfac}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=50mm]{images/dspcarfac}
\caption{
The Cascade of Asymmetric Filters and Resonators (CARFAC) model.  This
model is similar to the PZFC model, but includes an Automatic Gain
Control (AGC) stage, that models the action of the Outer Hair Cells in
the human cochlea.} 
\label{fig:dspcarfac} 
\end{center} 
\end{figure} 


The code for the CARFAC model was written in MATLAB, and it was my
task to take this code, port it to C++, verify that it worked
identically to the original code and then to optimize it to run as
quickly as possible.  For this, I used a software development
methodology called Test Driven Development \cite{fraser03} (TDD).  In
TDD, one inverts the normal software development process in that one
first writes the tests, and then the minimum code to make these tests
pass.  This is an ideal development strategy to use in this case,
since we have a working reference implementation of the algorithm in
MATLAB.  Using this strategy, I was able to port this MATLAB code
first to Python and then to C++.  The C++ code was added to the
Marsyas \cite{Marsyas} framework and was open-sourced during my time
at Google, and is now available to be used by the community.

The process of porting the CARFAC model was straightforward but time
consuming.  After I had ported this model to C++, we then went back to
MATLAB to develop a model of binaural hearing using the output of the
CARFAC filter cascade.  We used the Stabilized Auditory Image (SAI) model
proposed by Patterson \cite{patterson92}, which works well for the
pulse resonance sounds created by many types of animals, from fish to
insects to the human voice.  In Figure \ref{fig:pulseresonance} the
sounds from various animals are shown, an in each one, the same
phenomenon is seen, where a fast impulse, or pulse is created and is
then resonated through the vocal tract or other sound producing organ
in the creature.

\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/nap}
\caption{ The Neural Activity Pattern (NAP) of the CARFAC model.  In
  this figure, the vertical axis corresponds to cochlear place, with
  points closer to the bottom axis corresponding to lower frequencies
  and the horizontal axis corresponding to time}
\label{fig:nap} 
\end{center} 
\end{figure} 

\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/pulseresonance}
\caption{
A diagram of the waveforms produced by a variety of different animals,
showing the pulse and resonance structure of these vocalizations.  In
each, a short pulse is generated which then reverberates through the
vocal tract or sound producing organ of the organism.} 
\label{fig:pulseresonance} 
\end{center} 
\end{figure} 

This model finds trigger points in the input audio signal and
stabilizes the train of peaks from the CARFAC filterbank cascade into
a two dimensional image.  The output of the CARFAC filterbank is shown
in Figure \ref{fig:nap}, in this figure, the vertical axis corresponds
to cochlear place, with points closer to the bottom axis corresponding
to lower frequencies and the horizontal axis corresponding to time.
This plot can also be referred to as a Neural Activity Profile (NAP).
These peaks flow by rapidly, at the rate of pulses from the organism
in question, and in order to view them, one should align subsequent
peaks to each other.  There are many approaches to doing this, and the
approach commonly used is to find trigger points in the audio, that
is, points that correspond to pulses in the output of the vocal tract.
One trigger detection algorithm is shown schematically in Figure
\ref{fig:triggerpoints}.  In this figure the solid black line in the
center corresponds to the audio signal, and the red dots signify
trigger points.  The solid black line at the top of the figure
represents the current threshold value of the algorithm, and when this
threshold crosses the line representing the audio, a new trigger point
is generated.

\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/triggerpoints}
\caption{
In this figure the solid black line in the
center corresponds to the audio signal, and the red dots signify
trigger points.  The solid black line at the top of the figure
represents the current threshold value of the algorithm, and when this
threshold crosses the line representing the audio, a new trigger point
is generated.} 
\label{fig:triggerpoints} 
\end{center} 
\end{figure} 

\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/ihcohc}
\caption{
A schematic view of the connections between the cochlea and the higher
levels of the auditory periphary, including connections between the
olivary complexes to each other and back to their respective cochleas.} 
\label{fig:ihcohc} 
\end{center} 
\end{figure} 

In the Medial Superior Olive (MSO) in the human auditory system
\ref{fig:ihcohc}, a series of computations is executed that allows
for the computation of Interaural Time Difference, or ITD.  These time
differences are then processed by higher regions of the brain to
localize sounds in the azimuthal (or right-left) plane.  It has been
proposed that the MSO accomplishes this task by finding trigger points
in one ear and then using these trigger points to analyze the sound
coming from the opposite ear.  The binaural NAP patterns are shown
in Figure \ref{fig:binauralnap} and the strobes generated from this
are shown in Figure \ref{fig:binauralstrobes}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/binauralnap}
\caption{
binaural nap caption} 
\label{fig:binauralnap} 
\end{center} 
\end{figure} 


\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/binauralstrobes}
\caption{
binaural strobes caption} 
\label{fig:binauralstrobes} 
\end{center} 
\end{figure} 

If a sound was closer to the right hand side of the head, it would
first enter the right ear, trigger points from this ear would then be
transmitted across the brain to the other ear, which would then
integrate the audio in that channel.  One can then determine the place
on the azimuth plane by looking at the stabilized image and noting the
position of the central SAI axis.  I developed a realtime visualizer
of these images, and an example signal delayed 1ms in the right ear
as compared to the left, is shown in Figure \ref{fig:marcarfac}.
In this figure the entire width is 4ms, and one can clearly see that
the stereotypical SAI chevron is moved 1ms to the left.

\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/marcarfac}
\caption{
mar carfac caption} 
\label{fig:marcarfac} 
\end{center} 
\end{figure} 

Near the end of my internship, I demoed this program to the entire
Machine Hearing and Machine Vision teams at Google and was able to
impress the audience with a real-time demo using binaural microphones,
a model of the human head (using a Google Android doll) and a hand
held audio source that very effectively demonstrated positive support
for our hypothesis of the localization of sound by the MSO.

I then developed a program using the Python programming language and
the Marsyas framework to visualize this data in three dimensions, an
approach that was hypothesized to give us additional information about
the time evolution of these Stabilized Auditory Images.  A view of one
example of a songbird visualized with this algorithm is shown in
Figure ~\ref{fig:3dsai}.


\begin{figure}[t]
\begin{center}
\includegraphics[width=100mm]{images/3dsai}
\caption{
3dsai caption} 
\label{fig:3dsai} 
\end{center} 
\end{figure} 


\section{Conclusions}

During my summer internship at Google, I developed a new model of
binaural hearing using the new CARFAC model of the human cochlea.
This model was developed in a combination of C++ and MATLAB and has
been released as open-source software to the community.  The
underlying CARFAC model can be used in a variety of different
applications, including sound source separation and music genre
recognition.  I also developed a realtime visualizer of the binaural
stabilized images in Marsyas.  This viewer has also been released as
open-source to the community and can be downloaded from the Marsyas
web page (\url{http://marsyas.info}).  This project was completed
successfully.

In addition, I also took on a number of other projects unrelated to
this one, mostly involving either the calculation of new audio
features, or the visualization of these features.  This work helped
our team to get a deeper understanding of the data and how their
algorithms processed this data.  One highlight of my trip was when
Dr. Samy Bengio, a leader in the field of Machine Learning looked at
one of the visualizations that I had produced of the output of a
vector quantization and histogramming algorithm that was supposed to
be sparse.  My visualization clearly showed that the data was not
sparse and provided good fodder for discussion in the group and
hopefully provided new avenues of investigation to further improve
the process of song playlisting.


\bibliographystyle{acm}
\bibliography{googlecoop2011}


\end{document}
% Created 2008-11-04 Tue 14:33
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{morefloats}
%% \usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}

\title{Co-op report : Chubby - Distributed Lock Service}
\author{Steven Ness}
\date{September 19th 2012}

\begin{document}

\maketitle

\clearpage

\section{Abstract}

Chubby is a distributed lock service widely used at Google to perform
such diverse tasks as electing a master from a group of machines,
distributing configuration files to collections of clients, and
replicated sharing of the roots of databases, such as BigTable.  It
has been in use at Google for quite some time, and was first described
to the world in a 2006 paper by Mike Burrows from Google.  It provides
a simple API that can be quickly integrated into programs written in a
variety of languages, including Java, Python and C++.  At it's core is
a distributed consensus algorithm, known as Paxos.  This paper will
describe the Chubby system, but will concentrate on the underlying
Paxos algorithm.

\clearpage
\setcounter{tocdepth}{3}
\tableofcontents

\clearpage
\listoftables

\clearpage
\listoffigures


\clearpage
\section{Introduction}

One of the primary challenges in developing distributed systems is the
need for groups of computers to synchronize their conceptions of
information to a shared resource.  With a single processor computer,
this is simple, because there is only one processor that can change
values stored in memory.  This becomes more challenging for a
multi-processor computer, and in this situation mechanisms such as
mutexes are used to lock values in memory so that only one processor
or thread can access them at the same time.  These multi-processor
systems depend on the fact that all the processors have access to a
shared clock and shared memory.

On geographically distributed computers, the problem of locking
becomes much more challenging.  The main difficulty that is
encountered is that there are a wide variety of different failures
that can occur, both in individual servers and in the network
connections between these servers.  There have been a variety of
different algorithms that have been proposed for dealing with these
failures, including the two-phase commit and three-phase commit, but
for both of these algorithms, there are particular failure states that
are unrecoverable, and must be resolved by a human.  The Paxos
algorithm, developed by Leslie Lamport, is a special case of the
three-phase commit algorithm that tolerates a wide variety of failure
cases and does not require operator intervention.  This algorithm will
be described in detail in the next section.


\section{Background}

There has been considerable research devoted to the problem of
obtaining consensus amongst a distributed set of agents that are
subject to failures.  An early paper, published in 1988 by Dwork
et. al. \cite{dwork88} examines partially synchronous systems, a
class of systems that lie between synchronous systems, where all
processors execute tasks one after another, and asynchronous systems,
where the timing of tasks between different systems is not
guaranteed.  In these partially synchronous systems, there exist
bounds on the time it takes for one computer to send a message to
another computer and the relative speeds of different processors, but
these bounds are not known a priori.  This paper investigates ways to
develop consistent systems with this behaviour.  It should be noted
that current computer networking architectures can be described as
partially synchronous systems, which makes the results of this paper
especially relevant.

Another paper published in 1988 by Oki and Liscov \cite{oki88}
describes the concept of ``viewstamped replication'', which describes
a technique for providing a highly available system using replication
of data between different computers.  In their system, a single master
node contains a master copy of data, this data is then replicated to a
series of backup nodes.  In order to ensure the fidelity of
information, a type of timestamp, known as a viewstamp is used to mark
each data transaction.  By examining the viewstamps of this data, the
master and backup nodes can keep themselves in sync with one another
and ensure that if the master node goes down, the backup nodes will
contain correct information.

In 1998, Leslie Lamport published a seminal paper, ``The part-time
parliament'' \cite{lamport98}, in which he describes the Paxos
algorithm.  In this paper, the parliament of a fictional Greek society
named Paxos, with inhabitants named Paxons is described.  In this
fictional parliament, the system by which legislature members make and
agree upon laws.  In this parliament, the members are described as
being busy with business matters, and are often called out of the
chamber, yet with the system Lamport describes, they are able to
fulfill the functions of a voting parliamentary system.  There are a
number of special cases that are described, these are related in the
form of a history of the parliament, where different failure cases are
described, and the methods by which the parliament solves these
problems are layed out, one by one.  The members of this parliament
correspond to computers in a distributed environment, and the
different failure cases describe different types of failures that can
occur between networked machines.  Lamport then goes on to rigorously
prove the effectiveness of this technique using the mathematics of set
theory.  Although seminal, this paper was initially rejected by the
journal it was sent to, and only after other similar papers were
published did Lamport again submit this paper.  The Paxos algorithm
described in this paper forms the foundation of the Chubby system
described in the paper by Google.


\subsection{Paxos Algorithm}

The Paxos algorithm, while simple at it's core, can be complex to
describe and understand.  In the algorithm, there are four different
types of agents, Clients, Proposers, Acceptors and Learners.  Clients
are agents that want to store new values for data.  They send these
requests to Proposers, which are machines that take requests from
clients and send them to a synod of Acceptors.  The Proposer can be
also thought of as the master node, and the Acceptors can be thought
of as the backup nodes.  The Acceptors must reach a majority consensus
on the data that is proposed, and once this majority is achieved, they
send their results to a set of Learners.  When Clients then want to
retrieve the data that has been proposed, they contact these Learner
agents to receive the data.

The basic Paxos algorithm in absence of any failures is shown in
Figure \ref{fig:basic_paxos}.  In this figure there are Client
machines that would like to store a new value of data.  They send this
data to a Proposer agent, which can also be thought of as the master
agent.  This Proposer agent then sends a proposal to a set of Acceptor
machines, also referred to as a synod.  This synod of Acceptors can be
thought of as the backup machines.  Once the Acceptors agree with a
majority, the learned value is sent back to the Proposer, and is also
sent to a set of Learners.  In Figure \ref{fig:acceptor_failure}, one
of the Acceptor agents fails.  Because there is still a majority of
two machines, the algorithm can proceed normally.  In Figure
\ref{fig:learner_failure} one of the two redundant Learner machines
fails.  In this case, there is a backup Learner machine that is able
to respond to Clients, and the algorithm is able to proceed normally.

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/basic_paxos}
\caption{ This figure shows the flow of information in the Paxos
  algorithm when all machines behave correctly.  Clients first send
  their data to Proposers, which send the data to a synod of
  Acceptors.  Once a majority decision has been made by the Acceptors,
the values are sent to the Learners. }
\label{fig:basic_paxos} 
\end{center} 
\end{figure} 


\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/acceptor_failure}
\caption{ In this diagram is shown the case of when an Acceptor
  machine fails.  In this case, there are still a majority of
  Acceptors (two Acceptors), which allows the algorithm to proceed.  }
\label{fig:acceptor_failure} 
\end{center} 
\end{figure} 


\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/learner_failure}
\caption{ Shown is the case in which one of the two redundant Learner
  machines fails.  In this case, the second Learner fails, but because
  of the redundant Learner, the algorithm is able to proceed. }
\label{fig:learner_failure} 
\end{center} 
\end{figure} 


\clearpage
\section{Methods and Results}

In the paper ``The Chubby lock service for loosely-coupled distributed
systems''\cite{burrows06}, Mike Burrows from Google describes the
Chubby system, a distributed lock service that uses the concepts from
the Paxos algorithm to create a global scale service that is widely
used at Google.  The author describes an already implemented and
running system and talks about the various issues that were
encountered when building this system.

The system that they describe uses a series of five Acceptor machines,
this was chosen as an optimal value because a set of three machines
means that if any two machines failed, the system would not be able to
progress in the storing of data items.  Seven machines was decided as
too wasteful of resources, and five was chosen as the optimal number
of machines.  Most of the Chubby cells, as synods of machines is
called in Google, are located within a single datacenter to provide
for high availability and reliability.  They also describe another set
of Chubby instances that are in a global cell, that is, machines are
distributed across datacenters.  This provides for higher availability
at the cost of lower performance.

One of the problems that was encountered in the building of this
system was the lack of aggressive caching.  The first iteration of this
system did not include caching, and it was noticed that in this case,
clients would repeatedly loop, looking for files that did not exist
yet.  They tried a number of solutions, including an exponential time
decay function for the delay between when applications could request
data.  They finally settled on a solution where a call to access the
database was made to be inexpensive.

Another problem that was encountered was the lack of quotas for
users.  The system was designed to store small amounts of infrequently
changing data, but it was found that some clients would ignore these
design requirements, and would store large amounts of data, or would
access files too frequently.  By adding a simple quota system where
only a maximum of 256 kilobytes could be stored in a file object,
this problem was largely alleviated.

The author of the Chubby paper shows a set of results obtained from
real world performance data within Google.  Some of this data is
reproduced here.  In Table \ref{table:results1} results for the time
for different failure cases to occur is shown.  The large time of 18
days between failures is significant, and the short time of 14 second
for this fail-over to fix itself is also quite important.  The few
fail overs and the short time for a fail over to repair itself is
important for a large scale system.  In Table \ref{table:results2} the
number of active and proxied clients is shown.  22,000 active clients
is an impressive number of clients, and when the additional 32,000
proxied clients are added, they show that large numbers of clients can
access data at the same time.  In Table \ref{table:results3} the large
number of 230,000 cached client files is a significant number of
clients.  There are 24,000 distinct clients that are cached.  These
are all in all truly global scales of numbers, and shows the efficacy
of the Chubby system.


\begin{table} 
\begin{center} 
\begin{tabular}{|c|c|} 
\hline 
Time since last fail-over & 18 days \\
Fail-over duration & 14s \\
\hline
\end{tabular} 
\end{center}
\caption{ The time since a master machine last failed over, and the
  duration of time that it took a failure case to resolve itself.  }
\label{table:results1}
\end{table} 

\begin{table} 
\begin{center} 
\begin{tabular}{|c|c|} 
\hline
Active clients (direct) & 22,000 \\
Additional Proxied Clients & 32,000 \\
\hline 
\end{tabular} 
\end{center}
\caption{Shown in this table are the number of active clients with a
  direct connection to the master node.  Also shown is the number of
  additional proxied clients that contain proxied connections to the
  master node.}
\label{table:results2}
\end{table} 

\begin{table} 
\begin{center} 
\begin{tabular}{|c|c|} 
\hline 
Files open & 12k \\
Client-is-caching file entries & 230k \\
Distinct clients cached & 24k \\
\hline
\end{tabular} 
\end{center}
\caption{This table shows the number of files that are open and the
  number of clients that have cached information about these files.}
\label{table:results3}
\end{table} 

\clearpage
\section{Conclusions}

The Chubby system is a global scale distributed lock service developed
by Google that uses the Paxos algorithm, previously described by a
number of authors, including Leslie Lamport.  The performance
characteristics of this system was shown by the authors of the Chubby
paper, and other information about the history of how the system was
built are presented.

The author describes how the Chubby system has become the backbone of
many internal Google systems, including being the primary name server
inside of Google.  Other systems inside of Google that use Chubby are
the GFS (Google File System) storage infrastructure, and the BigTable
distributed database system used at Google.

\clearpage

\bibliographystyle{acm}
\bibliography{googlecoop2012}


\end{document}
% Created 2008-11-04 Tue 14:33
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{morefloats}
\usepackage{hyperref}
\usepackage{graphics}
\usepackage{graphicx}

\title{Co-op report}
\author{Steven Ness}
\date{10 Oct 2011}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents


\section{Introduction}


During the summer of 2012, I did an internship in the Google News
group at Google in Mountain View, California, which is their main
campus.  I worked as a SWE (Software Engineer) intern under the
mentorship of Trevor Pering, an engineer on Google News.

Google News is a computerized news aggregator, that is, it takes news
stories from thousands of trusted sources, including newspapers,
online news sites and other news organizations.  It can be seen by
navigating to http://news.google.com.  On this page, you will see
several story clusters, that is, stories from different newspapers
that are clustered according to their content.  

My summer intern project was to measure and reduce the latency of
Google News, as perceived by users.  Latency is a measure that tells
how long it takes for a system to perform a task, and it has been
shown that sites with less latency have higher user satisfaction
ratings.  Google News is already extremely fast, especially in
locations with high-speed internet, with page load times on the order
of 1-2 seconds.  

\section{Background}

Google News is one of the largest properties of Google, and the
director of Google News, has announced public ally that Google News is
``sending a billion clicks to news organizations every
month.''\cite{billionclicks}.

This is a truly staggering number of visitors, and highlights the
importance of extreme diligence on the part of software engineers
along with a robust testing platform to ensure the reliability of the
platform.  The consequences if the site is down for even a few minutes
would be dramatic.

Google News was first launched in 2002 and was originally led by
Krishna Barat, a scientist and engineer at Google.  It is a news
aggregator, which means that it takes news stories from sources on the
web, and puts these stories into a format easily accessible by
readers.  It uses a clustering algorithm to cluster stories about the
same topic together, and presents users with the highest quality
versions of these stories.  It also allows users to see all the
sources for a story via a separate link.  It indexes many thousands of
news sources, the exact number and identity of these sources is
confidential, but a blog post from 2010
(http://googlenewsblog.blogspot.com/2009/12/same-protocol-more-options-for-news.html)
gave a number of over 25,000 publishers.  

These publishers are from a wide variety of countries and languages,
with over 60 regions and 28 languages being represented.  Each of the
editions for the different languages have different features, but the
overall layout of the site is similar for each language/region
combination.  There is a mobile version of Google News that displays
content in a manner more appropriate for the small screen size and
limited bandwidth of these devices.  The layout of Google News is
constantly changing, and a major update to the site occurred in 2011.

The codebase for Google News is very large and is written primarily in
Java, with much of the customer facing interface code being written in
Javascript.  It is well written and highly optimized code, and
consists of a backend and a frontend, the backend being responsible
for crawling and clustering stories while the frontend is responsible
for querying the backend servers and serving content to customers.
The code is highly confidential, and further description of the exact
way the system functions cannot be discussed in this report.

My intern project was in two phases.  The first phase was to implement
an API (Application Programming Interface) to allow performance
monitoring of various kinds of user interactions with the website.
The second phase was to reduce the user perceived latency of Google
News by developing a caching system for a large piece of CSS code that
is currently inclined in the Google News homepage.


\section{Methods and Results}

\subsection{Project 1 - Measuring Latency}

For my first project, I developed an API to allow developers to easily
send performance monitoring results to different performance
monitoring systems inside of Google.  One of these is an internal and
confidential system that is widely used inside of Google for
performance monitoring.  The other system is a new feature of Google
Analytics that allows users to upload timing data for arbitrary user
actions.  This system provides methods to allow users to upload this
data and to query it with an advanced web based interface.  A view of
the user timing interface of Google Analytics is shown in Figure
\ref{fig:latency-ga1}.  In the main pane of this interface is shown
the timing information for one type of user interaction named ``ms'',
and the data is shown for a range of 5 days. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-ga1}
\caption{ The user timing interface in Google Analytics.  A line graph
  of the user timing information for the ``ms'' variable over the span
  of 5 days is shown in the main pane.}
\label{fig:latency-ga1} 
\end{center} 
\end{figure} 

A user can also view a histogram of the timing information for a
single experiment, this is shown in Figure \ref{fig:latency-csi-ga2}.
In this figure, a histogram of timing results, with units in seconds
is shown.  The default interface shows a coarse view of the data and
by clicking on the plus marks on the left hand side of the interface,
each of these bins can be expanded, and the contents can be inspected
in more detail.

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-csi-ga2}
\caption{ A histogram of the timing data for a single experiment with
  units of seconds.  Each of the lines in the histogram corresponds to
  a bin of timing results, with the ranges for the bin shown on the
  far left.}
\label{fig:latency-csi-ga2} 
\end{center} 
\end{figure} 

The API for this project was written in Javascript, and an object
oriented API was developed to allow developers to instrument arbitrary
client actions and to determine the performance of each of these
actions.  The code is confidential and cannot be shown in this report,
but was reviewed by a number of members of the Google News frontend
team.  It was deployed into production in the second month of my
internship in an experimental mode, and currently runs on 1\% of the
queries to Google News.  Data from it is currently being used to
analyze performance of various user actions.

\subsection{Project 2 - Reducing Latency}

The second project that I worked on this summer took the majority of
my internship and was to try to reduce the latency of Google News, as
seen by users.  Google News is already extremely fast, with page loads
happening in the range of 1-4 seconds.  One commonly used metric for
investigating latency is the ``Above the Fold Time'' (AFT) which is
defined as the time it takes for the visible content to be displayed
to the user.  A graph of the mean AFT for Google News is shown in
Figure \ref{fig:latency-csi}.  This graph shows data for a two week
period of time, with the top graph showing the latency of the site,
and the bottom graph showing the measurement volume.  As one can see
from this graph, the latency is already quite low and consistent.
However, if one instead looks at the data for visitors viewing the
Tanzanian edition of the site (Figure \ref{fig:latency-csi-tz}), one
sees much higher and more variable latency.  The primary reason for
this is lower internet connection speeds for these users.

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-csi}
\caption{ The mean Above Fold Time latency for Google News.  The top
  graph shows the mean Above Fold Time and the bottom shows the
  measurement volume for this variable. }
\label{fig:latency-csi} 
\end{center} 
\end{figure} 

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-csi-tz}
\caption{ The mean Above Fold Time latency for the Tanzanian version
  of Google News.  The top graph shows the mean Above Fold Time and
  the bottom shows the measurement volume for this variable. }
\label{fig:latency-csi-tz} 
\end{center} 
\end{figure} 

One well known way to reduce the latency of a website is to cache as
many resources as possible.  Images are one of the most commonly
cached resources, but another common resource that benefits from
caching is CSS (Cascading Style Sheet) resources.  In the current
version of Google News, there is a large section of CSS that is
currently inclined in the webpage, and by making this a cacheable
resource a significant speedup for a certain segment of users could be
realized.  The current page size of Google News is around 513KB and
the CSS part of this is 133KB.  With compression, the main page is
86KB and the CSS is 24KB of this.  By serving this CSS as a cacheable
resource, the users web browser would load this resource the first time
the page is loaded, and on subsequent loads, the users web browser
would load this resource from cache.  If the total latency for the
site is dominated by bandwidth considerations, this caching should
reduce the latency for users.  This should be the case for users
outside North America and Europe, where connection speeds are slower.
It could also be the case for users on mobile devices or tablets,
which is a growing market for people consuming news.

This would be a simple problem if there was only one version of this
CSS resource.  In Google News however, the various different
internationalized versions of the site are primarily differentiated by
making changes to the CSS, via a system of conditionals.  There are
many different conditionals in the CSS, with a total of approximately
43 different conditionals being used in the current code, with about
20 of these changing for each different edition of the page.

The system that I developed includes a program that takes different
sets of these conditionals, and generates a different instance of CSS
for each of these sets of conditionals.  These CSS files are then
served via a static web server.  To ensure that users get the correct
CSS for their edition, a hash fingerprint of each of these CSS files
is calculated, and the filename that is stored in the static web server
is named based on this calculated hash.  

One of the issues that is commonly encountered when dealing with
cached files is that when the contents of the cached file change, the
content must be invalidated and the user's browser must download the
changed resource.  Because we are taking a hash of the contents of the
CSS, we can be guaranteed that the cached CSS files will never
change.  This can be exploited by the static web server, and metadata
can be sent to the user's browser to tell it that it never needs to
download a new version of the file.

Due to confidentiality issues, I cannot talk in more detail about my
implementation of this system.  However, it was reviewed in detail by
a number of Google engineers, and has been included in the production
version of Google News as a 1\% experiment.

We are currently in the process of collecting data for this system.
In the meantime, I have run this code on an internal system in Google
that allows testing of the latency of websites, this system is named
``Latency Lab''.

The first experiment I conducted used an unlimited bandwidth network.
For this experiment, I did one run with no caching of CSS, the results
for this can be seen in Figure \ref{fig:latency-nocachedcss-unlimited}
and another run that turned on the caching of CSS
\ref{fig:latency-cachedcss-unlimited}.  The middle section of these
two graphs shows the 10 different runs that were run, and the graph at
the bottom shows a histogram of the timing data.  From these two, one
can see that the timing results for these two jobs were very similar,
and are perhaps indistinguishable from one another.  This is to be
expected, because on the unlimited bandwidth networks inside of Google,
the time taken by transferring the data over the wire is extremely
small and does not dominate the total page load time.

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-nocachedcss-unlimited}
\caption{ Graph showing timing information for a situation with no
cached CSS on an unlimited bandwidth network.}
\label{fig:latency-nocachedcss-unlimited} 
\end{center} 
\end{figure} 

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-cachedcss-unlimited}
\caption{ Graph showing timing information for a situation with
cached CSS on an unlimited bandwidth network.}
\label{fig:latency-cachedcss-unlimited} 
\end{center} 
\end{figure} 

In order to model the case of users with a slower connection speed, a
traffic shaping network was used with the properties of a simulated
EDGE network.  The results with caching of CSS is shown in Figure
\ref{fig:latency-cachedcss-EDGE} and the results with no caching of
CSS is shown in Figure \ref{fig:latency-nocachedcss-EDGE}.  From these
results, one can see that the cached CSS version has less latency than
the version that does not cache CSS.

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-nocachedcss-EDGE}
\caption{ Graph showing timing information for a situation with no
cached CSS on a simulated EDGE network. }
\label{fig:latency-nocachedcss-EDGE} 
\end{center} 
\end{figure} 

\begin{figure}[t]
\begin{center}
\includegraphics[width=120mm]{images/latency-cachedcss-EDGE}
\caption{ Graph showing timing information for a situation with cached
  CSS on a simulated EDGE network.  }
\label{fig:latency-cachedcss-EDGE} 
\end{center} 
\end{figure} 

We are currently in the process of getting this code into a production
experiment, and are looking forward to collecting data from real users
to back up these results.

\section{Conclusions}

In this summer internship at Google, my project was to measure and
reduce the latency of Google News.  In the first part of my project, I
developed and launched an API that allows developers to measure
performance data from arbitrary users actions.  This code has been
pushed into production and is currently running at a 1\% level.  It is
supporting Google engineers working on new functionality for Google
News by allowing them to see the performance numbers of their new
features.

The second sub-project was to reduce the latency of Google News.  To
do this, we developed a system to allow in-lined CSS to be cached.
Preliminary numbers support our hypothesis that caching resources will
result in lower latency for users.  We are currently in the process of
getting this code into production, and hope to collect numbers to
further support our hypothesis.

\section{References}

\bibliographystyle{acm}
\bibliography{googlecoop2012}


\end{document}
% --------------------------------------------------------------------------
% Template for ICAD-2010 paper; to be used with:
%          icad2010.sty  - ICAD 2010 LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
%
% --------------------------------------------------------------------------

\documentclass{article}
\usepackage{icad2010,amsmath,epsfig,times,url}
\usepackage[justification=centering]{caption}

% Example definitions.
% --------------------
\def\defeqn{\stackrel{\triangle}{=}}
\newcommand{\symvec}[1]{{\mbox{\boldmath $#1$}}}
\newcommand{\symmat}[1]{{\mbox{\boldmath $#1$}}}

% Title.
% --------------------
%\title{Symphony of the Seasons - Multichannel Sonification of Phenological
% Data}
\title{Sonophenology : A tangible interface for sonification of \\
geo-spatial phenological data at multiple time-scales}

% *** IMPORTANT ***
% *** PLEASE LEAVE AUTHOR INFORMATION BLANK UNTIL FINAL CAMERA-READY SUBMISSION *** 

% IF ONE AUTHOR , uncomment this part
%\name{Jyri Huopaniemi} 
%\address{Nokia Research Center \\ 
%Speech and Audio Systems Laboratory \\ 
%P.O.Box 407, FIN-00045 Nokia Group, Finland \\ 
%{\tt jyri.huopaniemi@nokia.com}} 
%

%% % IF TWO AUTHORS, uncomment this part
%% \author{Paul Reimer\thanks{e-mail: pdreimer@engr.uvic.ca}\\
%%     \scriptsize University of Victoria
%% \and Alexandra Albu\thanks{e-mail:ed.grimley@aol.com}\\
%%     \scriptsize University of Victoria}

\name{
Steven R. Ness, Paul Reimer, Norman Krell,}
\nameb{Gabrielle Odowichuck, W. Andrew Schloss and George Tzanetakis}
\address{
University of Victoria \\                                                                                          
Department of Computer Science and School of Music \\
Victoria, BC, Canada.}
                 
%% \twoauthors{Steven R. Ness, Paul Reimer, Norman Krell, Gabrielle Odowichuck and George Tzanetakis}
%% { University of Victoria \\                                                                                          
%%  Department of Computer Science and School of Music \\
%%  Victoria, BC, Canada.}
%% {test2}
%% {test2a}
%% {Jyri Huopaniemi} {Nokia Research Center \\ Speech and  
%% Audio Systems Laboratory \\ P.O.Box 407, FIN-00045 Nokia Group,  Finland  \\ {\tt jyri.huopaniemi@nokia.com}} 
%% {Jyri Huopaniemi} {Nokia Research Center \\ Speech and  
%% Audio Systems Laboratory \\ P.O.Box 407, FIN-00045 Nokia Group,  Finland  \\ {\tt jyri.huopaniemi@nokia.com}} 
%% {Matti Karjalainen}  {Helsinki University of Technology \\ Acoustics Laboratory \\  
%% P.O.Box 3000, FIN-02015 HUT, Finland \\ {\tt matti.karjalainen@hut.fi}}  

%% {\name{Steven R. Ness} 
%% \address{University of Victoria \\
%% Department of Computer Science \\ ECS Room 512 Victoria, BC, Canada.}}


%% \name{Paul Reimer} 
%% \address{University of Victoria \\
%% Department of Computer Science \\ ECS Room 512 
%% Victoria, BC, Canada.}

%% \name{Norman Krell} 
%% \address{University of Victoria \\
%% Department of Computer Science \\ ECS Room 512 
%% Victoria, BC, Canada.}

%% \name{Gabrielle Odowichuck} 
%% \address{University of Victoria \\
%% Department of Computer Science \\ ECS Room 512 
%% Victoria, BC, Canada.}

%% \name{W. Andrew Schloss} 
%% \address{School of Music \\
%% University of Victoria
%% Victoria, BC, Canada.}

%% \name{George Tzanetakis} 
%% \address{University of Victoria \\
%% Department of Computer Science \\ ECS Room 512 
%% Victoria, BC, Canada.}


\begin{document}
\ninept
\maketitle

\begin{sloppy}

\begin{abstract}
Phenology is the study of periodic biological processes, such as when
plants flower and birds arrive in the spring.  In this paper we sonify
phenology data and control the sonification process through a tangible
interface consisting of a physical paper map and tracking of
fiducial markers. The designed interface enables one or more users to
concurrently specify point and range queries in both time and space
and receive immediate sonic feedback.  This system can be used to
study and explore the effects of climate change, both as tool to be
used by scientists, and as a way to educate members of the general
public.


\end{abstract}

\section{Introduction}

The study of the yearly timing of biological processes is called
phenology.  Examples of this include when a particular species of tree
first flowers in the year, when birds return from their migrations, or
when frogs first emerge after winter.  It comes from the Greek words
for ``to show or appear'' (phaino) and ``reasoning, or rational
thought'' (logos).  It is an ancient field of study and people have
recorded this type of information since the dawn of time. For example
farmers have kept records about the emergence of their crops from year
to year in order to help them determine the optimal time to plant
crops in a specific geographic location.

In just the last few years, internet enabled collaborative websites
have transformed the collection of phenology data from a discipline
where the individual scientist or farmer records their data onto paper
for primarily their own research or use, into one where large numbers
of amateurs from the general public can all collect and enter their
own phenological observations.  Sites such as the National Phenology
Network \footnote{http://usanpn.org} and Nature's
Calendar \footnote{http://www.naturescalendar.org.uk/} allow citizen
scientists to record their own observations about when certain natural
phenomenon happen.  This type of approach is commonly referred to as
Crowdsourcing.

Tangible interfaces are a new metaphor in Human-Computer interactions,
where instead of having the user interact with only the screen and
keyboard, the person interacts with physical objects in the real
world.  These types of interfaces can involve cameras, sensors,
motors, actuators and displays, and merge the real and virtual worlds
into a single unified user interface.

In this paper, we propose to sonify phenological data and let people
explore these datasets using a tangible interface.  Our proposed
system could be used with both historical phenological data and also
with the large quantities of crowdsourced phenological data that is
just now becoming available. There are several aspects of phenology
data that make it a particularly interesting candidate for control
through a tangible interface and sonification. Ideally a system 
for exploring phenology data should allow the specification of both
spatial and time range queries in addition to simple point queries i.e 
render the data from Tokyo and Osaka between 1985-1990. We design a
tangible interface based on tracking of fiducial markers that can be used
for specification of point and range queries in time and space over 
a printed map. Of particular interest is the relative timing of
different events such as flowering happens earlier in the South than
the North. Synchronicity and relatively timing are clearly conveyed in
our sonification. We are particularly interested in installation and 
public outreach environments therefore the sonification has also been 
designed to be aesthetically pleasing and not intrusive. 




\section{Related work}

In ``The Climate Symphony'', \cite{quinn01} the author presents a
sonification of 200,000 years of ice core data in an artistic
presentation that is a combination of sonification and story-based
narrative structure.  Eight sets of time series data of the relative
concentrations of a number of ions in this ice core were examined, and
using Principal Component Analysis, these time series were reduced to
three sets of time series data.  These time series data were sonified
with simple sine waves which were then amplitude modulated by the
amount of ice sheets coverage.  Interesting contributions of this
paper include the idea that because of the variety of different cycles
in global temperatures that are driven by climate forcing from the sun
(on time scales of 400,000, 100,000, 40,000 and 22,000 years) there
are natural periodicities to this data.  By using the natural ability
of humans to hear periodic structure in audio signals, this paper
demonstrates that this type of data is amenable to sonification.
Another important contribution of this paper is that it attempts to
create a system that will engage members of the general public by
providing an interesting and pleasant way to explore climate data.

In ``Broadcasting auditory weather reports - a pilot project'',
\cite{hermann03} a sonification system is described that generates a
sonified summary of a days worth of weather data which is then
broadcasted on a local radio station.  The data that is sonified
includes time markers, wind, rainfall, temperature, cloudiness,
humidity as well as discrete events such as thunder, hail and fog.
They then sonify a 24 hour period in a 12 second audio clip, and
comment on the different mappings of weather data to sound that they
tried.  One interesting contribution of this paper was that they found
it useful to explore the emotional content of music, and that the
authors tried to map pleasant weather events (like bright sunshine) to
musical phrases that evoked pleasant emotions, and less pleasant
weather conditions (such as rain) to more melancholy musical phrases.

Another related paper is ``Sonification of Daily Weather Records''
\cite{flowers01}, in which the authors describe a system that sonifies
the weather data from Lincoln, Nebraska.  In this paper, the authors
choose three different parameters to sonify, temperature, rainfall and
snowfall.  For the temperature, they take daily high and low
temperature measurements and convert these to MIDI notes.  Because of
the sizable difference between the high and low notes, this produces a
sonification with two independent melodic lines, which humans are able
to independently track as separate streams, as previous research by
Bregman has shown \cite{bregman90}.  They also propose mappings for
rainfall and snowfall, for these the authors use one, two and three
note sequences to encode different amounts of rainfall and snowfall,
for example, for rainfall events less than 0.05 inches, only a single
note is sounded, and for rainfall events over 0.5 inches, a sequence
of three consecutive notes are played.  They chose this mapping in
order to follow the metaphor that light rain makes only light plinks
and that heavier rain ``comes down harder''.

In ``Atmospherics/Weather works: A multi-channel storm sonification
project'' \cite{polli04}, the authors describe a system for sonifying
the meteorological data associated with weather storms using
multi-channel audio.  In this paper they present sonifications for two
storms, one of which was a typical strong hurricane, and one was an
extremely violent storm that was not predicted by existing
meteorological models.  They choose these two storms in order to test
if their sonification of storms could help meteorologists develop
insights into the differences between these storms.  Besides the very
interesting idea of using multi-channel audio to help users understand
the data better, they also present ideas for a variety of different
sonifications of weather patterns.  They first identified a number of
variables, including temperature, wind speed and humidity at a variety
of elevations, and then did a simple mapping of this data to pitches.
Another interesting idea that was employed in this paper was to
correlate each geographical point on the map to a speaker and then to
use loudness as an indication of wind speed.  The authors report that
this gave a dramatic spatialization effect to the data.

In the majority of existing system for sonifying scientific data the
result of the sonification process is a monolithic audio signal and
the amount of influence users have in the sonification process is
minimal or non-existent. In contrast in our system we have tried to
make the sonification process an interactive, exploratory
experience. Our design has been informed by several different research
topics: phenology, crowdsourcing, tangible interfaces,  and
sonification. In the following section we describe these different
topics and show how they relate to our work. The resulting system 
which we call Sonophenology integrates these different influences in a
coherent whole. 


\section{Background and Motivation}

\subsection{Phenology}

Phenology is the study of the timing of biological processes as they
occur during various times of the year.  The timing of biological
processes are intimately linked to the environment in which the
organisms exist, and one of the most important determiners of the
timing of seasonal changes is the average local temperature.  For
example, during a warm year, cherry blossoms will flower earlier than
they would during a year with a colder spring. 

Recently, phenological data has been used in a number of research
projects in climate change \cite{post99, penuelas02, walther02,
  menzel06}.  In these studies, a general conclusion has been reached
that changes in local and global temperature affect the timings of
phenological processes, and that these processes are exquisitely
precise measures of climate change.  Currently these results are
typically compared using using either statistical measures, such as
the ANOVA (Analysis of Variance) tests such as in Doi \cite{doi08} or
using visual representations of this data such as graphs that show
histograms of the timing of various events across years.

Data about winter temperatures have been recorded for the last 2000
years in China \cite{ge03}, and this data has been used to study
climate variations.  Another set of phenological data that has been
used to study climate change is that of Burgundy grapes in France
\cite{chuine04}.  In this study, spring and summer temperatures from
1370 to 2003 were studied, and using the data from the ripening of
this species of grape, it was possible to look at variations in
temperature over this time span.  These types of studies show that
phenology data can be used as a source of proxy data for studying the
climate.  Karl Linnaeus, the founder of modern taxonomy, studied
phenology extensively, and by making observations of the flowering of
18 different plant species across Sweden.  In his research, he came to
the conclusion that flowering plants are exquisitely sensitive weather
instruments.

\subsection{Crowdsourcing}

Crowdsourcing is a relatively new phenomenon that has been enabled by
the pervasive spread of the internet in society, and allows members of
the general public to help scientists collect or analyze data.  It is
a new type of collaboration where non-specialists help expert
scientists \cite{howe08_crowdsourcing} and has been used to great
advantage in a number of research programs
\cite{surowiecki05_crowdsourcing} \cite{bradham08_crowdsourcing}
\cite{travis08_crowdsourcing}.  Hong \cite{hong04_crowdsourcing}
presents results that show that a group of problem solvers with a
diverse background can outperform smaller groups of experts.

Whereas it used to be the case that obtaining phenological datasets
used to be a difficult and time consuming process, the advent of these
websites will mean that there will soon be huge archives of
phenological data.  One of these sites that has already started to
distribute data is the Nature
Watch \footnote{http://www.naturewatch.ca} website in Canada.  This
website has subprojects including IceWatch, PlantWatch, FrogWatch and
WormWatch that monitor the timing of various physical and biological
processes, including when ice is present, when plants emerge and bloom
and when worms and frogs emerge from hibernation.

With the advent of these new crowdsourced sites for the collection of
phenological data, the concept of phenology is becoming more well
known in the general community.  These websites have thousands of
observers located in many geographical regions, and with this data
becoming available, it can be anticipated that these citizen
scientists will want to observe the results of their observations.
Currently, results are usually presented in the form of a map with an
associated timeline which allows the user to go back and forth in time
to observe which plants are flowering at which places over time.

\subsection{Tangible Interfaces}

Tangible computing interfaces using tokens detected by computer vision
techniques, such as the reacTable proposed by Kaltenbrunner, Jorda,
and Geiger \cite{reactable_tei07} have been tailored specifically for
designing multimedia processing algorithms.  The shape, translation,
and rotation of tokens placed on a planar desktop surface controls
some aspect of a multimedia processing pipeline. Early versions of
these interfaces had an audio focus, to complement the visual process
of designing an audio processing interface (e.g. an musical
instrument). Tokens designed specifically for detection,
classification, and spatial location/orientation are known as fiducial
markers.

Fiducal marker detectors and trackers operate by identifying fiducials in a
video frame, based on information that is known a-priori. Several visual
properties can serve to identify a fiducial marker, (e.g. colour, geometry);
several popular, state-of-the-art detectors use fiducials designed and
identified by the topology of a hierarchy of shapes contained within the
fiducial design, as described by Costanza and Robinson in
\cite{regionadjancency_vvg03}.

Costanza, Shelley and Robinson\cite{dtouch_dafx03} describe the application of
this approach to detecting fiducial markers via the use of a region adjacency
graph (RAG) to encode a two-level topology (e.g. black and white) of binary
shapes into a tree structure representing that shape.  Several constraints are
imposed on marker designs by this choice of detector; markers must consist of
white shapes wholly surrounded by black shapes, which in turn may enclose
another level of black shapes. Detectors work on a binary-thresholded version of
the input image, which allows some variation in the detected colour
(e.g. off-white, near-black).  A region adjacency graph can be generated for any
number of levels, but in practice the number of levels is limited to three,
denoted in \cite{dtouch_dafx03} as root, branches and leaves.

Bencina et al. improve on the topological fiducial detector in
\cite{reactivision_cvpr05}, where the centroid of clusters of shapes contained
in a lower level of the topology are used to rapidly reject candidate fiducial
matches that do not conform to the structure of expected fiducials, and use this
centroid information to discriminate between different fiducials.

Using a fiducial detector based on marker topology presents a tradeoff between
marker complexity (and hence increasing size of the marker at the same level of
resolution), and number of possible markers represented by different topologies
of the same size.

Tangible interfaces based on positioning multiple fiducial markers placed on a
multitouch table or desktop surface have many advantages over a conventional
interface using a keyboard a mouse. These interfaces are a pure
direct-manipulation modality: the user can intuitively see the structure they
have created. Physical controls for parameters, visual representations of those
parameters, and visualizations of the output produced by each processing unit,
are located spatially nearby the fiducial token. The display plane and the
control/interface plane are often aligned, preventing confusion common from a
mouse/screen arrangement. Affordances are offered in multiple dimensions, for
each fiducial marker detected (i.e. marker id, position, and rotation). This
implies a simple marker printed on paper can yield more information than a
dedicated, wired, peripheral such as a computer mouse.  Indeed the costs of
fiducial tracking hardware are little more than the cost of a conventional
webcam and a printer for producing fiducial markers.

\pagebreak

Each marker can be positioned by a separate person, and so marker-based
interfaces lead easily to collaborative interfaces, since multiple people can
use the same desktop surface with a separate collection of markers, or become
more productive in assembling a single algorithm using multiple simultaneous
operators.

In addition to using fiducial markers as physical controls independent from
other markers, commonly a system of rules is designed to relate multiple
fiducial markers present on the same desktop. Possible interactions include
varying parameters of one or more markers, varying processing steps of one or
more markers, or establishing an application specific chain of markers which
interact in a pre-determined way.

We extend the concept of a tangible, fiducial marker-based interface used to
create an aesthetically pleasing, and usable environment for exploring
phenological data.

\subsection{Sonification}

Sonification can be described as the use of audio to convey
information.  In other words, scientific data is represented not as a
visualization, like a graph, but instead as a collection of sounds
that are played at different times.

The manner in which a given set of data is mapped to audio is a
challenging problem, there are an infinite number of ways to transform
data into a sonification\cite{fitch94}.  Many aspects of any sound can
be modified: we can perceive changes in amplitude, pitch, timbre,
directional, and temporal information.  Any of these auditory aspects,
or audio parameters, can be modified by a data set. The best choice
when selecting audio varies, depending on the content of a given set
of data.  The direction, or polarity, of the datasets that are being
compared can also affect the perception of a sonification.  For
example, temperature is often described aurally as a tone with
increasing pitch \cite{walker07}. The scale of the relationship
between a one-dimensional data set and the audio parameter modified by
that data must also be considered.  If we consider the temperature to
pitch example, we must consider how quickly will the pitch increase,
and whether the relationship will be linear or non-linear
\cite{walker00}, that is to say, one wants to preserve the ratios, not
the differences in frequency. The aesthetics of sonification are also
an important consideration.  The goal is to create a collection of
sounds that represents a dataset accurately, and is also pleasing to
listen to.


\section{System Description}



\subsection{Overview}

%% - Generate database from phenology data
%% - GIS system - Postgres with GIS
%% - Track fiducials
%% - Map fiducials to map locations
%% - Sonify data

\begin{figure}[htb]
\includegraphics[width=80mm]{flowChart}
\caption{A flowchart of the system organization of our system.  This
  system has at its core a Controller module that communicates with
  the phenology and GIS databases as well as the video camera and
  sonification engine.  It generates sonifications by tracking the
  positions of fiducials on a printed paper map.}
\label{fig:flowChart} 
\end{figure} 

Our system consists of a number of separate sub-components that
interact together to provide a tangible interface for the exploration
of geo-spatial phenological data.  The overall organization of this
system can be seen in Figure \ref{fig:flowChart}.


The phenological data sources that we obtained for this paper are
quite diverse, and contain various types of information that could be
used for sonification.  In this application, we constrained our
analysis to include only the species name, the latitude and longitude of the
observation and the date when this observation was taken.  Other data
that we are not using for this paper includes the type of observation,
for example, was the observation of the first bloom of the lilacs or
when they were in full bloom.  Many of the observations also include
comments from the observers.  These additional sources of data could
be used in the future to enhance the audilization and visualization in
our interface.  We first sanitize these data sources and read them
into our GIS-enabled database system.

The second section of our system is the fiducial tracking interface.
This uses a consumer grade webcam and tracks pre-printed fiducial
markers on a surface.  We also use fiducial markers to determine the
position and orientation of the physical map underneath the fiducial
markers.  We then create a mapping from the set of coordinates of the
fiducial markers to the physical latitude and longitude on the map.
When the user places fiducial markers on the map, this system then
takes the latitude and longitude of these points and queries the
database to obtain corresponding phenological data points.

The final step in this system then involves taking these phenological
data points, which include latitude, longitude, species and
observation date, and sonifying them.  

\subsection{Phenology - Japan lilac}

For this project, we are concentrating first on a set of observations
of the flowering of the common purple lilac \textit{Syringa vulgaris}
in Japan \cite{funakoshi00}.  Observations on the flowering of this
species were collected from 1996 until 2009.  Because of the large
difference in latitudes between the south and north of Japan, flowers
bloom earlier in regions in the south of Japan before they do in the
north of Japan.  These types of geographical differences are one
source in the variation of flowering times.  Another difference that
may be possible to observe is the effects of climate change on the
flowering times of these lilacs, however to truly see effects of
climate change, one must of course examine temperature records over
longer time spans, on time spans of centuries to millenia.  If average
temperatures increase over a period of years, one would expect that
the phenological processes that respond to temperature would tend to
move to earlier times in the year.

\subsection{Tangible interface}

%% - map of japan (or the place of interest)
%% - fiducial markers

\begin{figure}[htb]
\includegraphics[width=80mm]{apparatus}
\caption{Shown above is a picture of the fiducial tracking interface.
Above the computer monitor is a small consumer grade video camera,
which is pointed downwards in order to view the fiducial markers which
are placed on a printed paper map.}
\label{fig:apparatus} 
\end{figure} 

While it would be possible to develop a simple desktop or web-based
interface to explore a sonification of this data, a much more
intuitive and engaging interface could be a tangible interface, where
users interact with a physical interface.  We have chosen a fiducial
based tag tracking system previously used in the reacTable
\cite{reactable_tei07}.  A picture of this system is shown in Figure
\ref{fig:apparatus}.

This interface is inexpensive and easy to deploy, requiring only a
consumer-grade webcam, physical printed map and printed fiducial tags,
and could be easily deployed within a classroom setting.  With such a
system in a classroom, a teacher could teach students not just about
phenology, climate change and maps, but also about new systems for
physical interaction with computers.  By moving markers across the
map, the students experience a direct correlation with the location of
the marker on the map and the associated phenological data.

\begin{figure}[htb]
\includegraphics[width=80mm]{fiducialDiagram}
\caption{(a) Rotating the fiducials will change the sonification range. (b) Translating a pair of fiducials will sonify all data points found within the enclosed area.  }
\label{fig:fiducialDiagram} 
\end{figure} 

In order to generate the query of the GIS database that contains the
phenology data, we use two different user-interface metaphors.  The
first, simple method, is to simply use the center of the fiducial as
the latitude/longitude search point and to return all data points that
lie underneath that fiducial.  A more complex setup that we have also
implemented allows users to select a region of the map and a time
range for each region.  In this scheme, regions are created by placing
two fiducials on the map.  The first fiducial specifies the top left
corner of a bounding box and the second fiducial specifies the bottom
right corner.  To change the time range that is sonified, the system
calculates the relative rotation angle between the two fiducials and
maps this to a value of years.  This setup is demonstrated in Figure \ref{fig:fiducialDiagram}.

%% - step sequencer 
%%  - each fiducial is a different timbre (instrument)
%%  - each year is a different pitch
%%  - each date (from april 10th-july22nd) is a step in the step sequencer





\subsection{Sonifications}

There are a number of advantages to sonifying these phenological data
over using statistical tools and visual graphs.  One advantage is that
by using different timbres to represent the different sections of the
map that we are sonifying, we take advantage of the fact that humans
can distinguish different melodic streams that are rendered in
parallel by different timbres.  This could potentially allow a user to
follow many different lines of data at once.  This technique becomes
even more powerful because of the distributed geographical and
temporal nature of the phenological data, where flowers in the south
bloom earlier than flowers in the north.  These different melodic
lines start and swell at different times, and the combination of
different timbres with different start times of these timbres make it
even simpler for users to follow the progression of phenological
events.

Our primary sonification metaphor is that of a step sequencer, which
uses a fixed two-dimensional grid consisting of quantized steps, with
the horizontal axis representing time and different steps on the
vertical axis being different instruments, or different pitches of one
instrument.  In our system, the vertical, or pitch axis, corresponds
to different years, and the horizontal, or time axis, corresponds to
the timing of the phenological event in days since the start of the
year.  This system allows us to easily hear and compare changes in the
timings of different events over years by listening to the
organization of pitches.  If a phenological event occurs on the same
date each year, one would hear a chord of all the notes at the same
time.  If on the other hand, the date of a phenological event becomes
earlier each year, one would hear a descending arpeggio of notes.

The comparison of phenomena over various years is an essential part
of this system, as one a primary motivation of this project is to
provide a way for people to not just see but also hear and explore the
effects of climate change.  These different modalities of experience
might prove effective in the education of people about phenology and
climate change.


\begin{figure}[htb]
\includegraphics[width=80mm]{sonificationChart}
\caption{A graphical representation of 10 years of flowering data for
  the common lilac in three locations in Japan.  The three different
  locations are depicted by different shapes, a circle, a triangle and
  a star.  From this diagram, one can see that there are certain years
  (2002-2004) in which flowering occurred earlier than in other years.}
\label{fig:sonificationChart} 
\end{figure} 

One mapping that we have found to be useful is a step sequencer.  In
our system, one axis of the sequencer has pitches that correspond to
different years, where earlier years have lower pitches, and later
years have higher pitches.  On the other axis of the step sequencer we
have the day of the year.  We also then map each fiducial marker to a
different musical instrument or timbre.  With this mapping, if the
flowers in a specific region all flowered at the same time, then one
would hear the notes from all the years sounding at once.  On the
other hand, if the flowering dates occur at later times each year, one
would hear an arpeggio of notes with increasing pitches.  A graphical
view of three observation locations over a time period of 10 years is
shown in Figure \ref{fig:sonificationChart}.

Another mapping that we are exploring is to instead represent each
phenological observation as a distinct sonic event.  This type of
sonification produces a radically different soundscape which is more
textural and ambient.  One can imagine what this sonification sounds
like by thinking of the timing of blooming of plants in the spring.
One will often see one or two different plants of a species flower,
then as time goes on more plants will flower in almost an exponential
fashion until all the plants of the species have flowered.  If one
were to sonify each of these events as an impulse sound, then the
sonification of this data would sound something like the popping of
popcorn.  What is interesting in this method is that it allows us to
perceive the ``stochastic'' nature of the natural process, where each
event is not significant unto itself, but the aggregate events outline
a process that can be reflected in an auditory soundscape that reveals
subtle differences in the rate of change of a physical system.  Our
ears are very sensitive to subtle differences in stochastic signals
like colored (or filtered) noise.

When converting data into audio, there are a number of different
mappings that can be used.  The simplest would use a sinusoidal
oscillator and would linearly map input data into the frequency of
this oscillator.  One disadvantage of this mapping is that in the
human auditory system, the frequency to pitch ratio is not linear but
rather is logarithmic.  Because the human ear hears frequencies
logarithmically, a logarithmic mapping of data to frequency would more
accurately preserve the ratios of data points to each other.  There
are a potentially infinite number of mappings of data to pitch values,
the one that we chose for this application was to map data values onto
the equal tempered scale, as seen on the piano keyboard or MIDI note
values.  However, in our system, we anticipate that several values
could occur at one point in time, and if we were to simply map data
values to MIDI note values, it would be common to encounter
dissonances in simultaneously played notes.  To overcome this, one can
use different scales or chords instead of the chromatic scale.  In our
system, we mapped the 10 different year values to the pentatonic
scale.  We are also developing mappings using chords, for example the
notes of a C-sharp major 7th chord, or any other chord, could be used
to map each year to a pitch component of the chord. Then the
chronological order would determine the position of the year within
the chord - in our example chronological order follows pitch height.
One could use the same chord for all instruments with or without the
same keynote, however, one could also use different chords for
different instruments, which might have the advantage that it would
further improve distinguishability for people by different melodic
lines following the chords.

For most of our work in this paper we have used sampled sounds from
the RWC dataset \cite{goto03}.  However, we have also implemented a
synthetic instrument model in order to provide more and different
sonification parameters.  In doing this, we have implemented simple
sine sources, plucked strings as well as more advanced synthetic
models of physical instruments.  The advantage to using these types of
synthesized sounds is that it is possible to control different
parameters of the sound, for example, the brightness of a clarinet
sound, or attack speed of a trumpet, these parameters can then be
mapped to the data that is being sonified.  Using synthetic instrument
models, one could also generate timbres that are intermediate between
two instruments, for example, one could make a sound that was half-way
between a clarinet and saxophone.  This type of fine-grained control
is difficult to implement using pre-generated samples.  The main
disadvantage to using synthetic instruments is that the models are
often quite elaborate and are computationally expensive, which limits
the amount of simultaneously playing instruments.

%% Plucked string model 

%% - sine source
%% - synthetic instrument
%% - using different aspects that you can control for instruments map
%%   these to different data
%% - timbre changing over time - difference between first bloom and full
%%   bloom
%% - difficult to do this with samples
%% - chords - use chord structure for these
%% - each year has own note within the chord
%% - disharmonicity
%% - intermediate timbres
%% - most models are quite elaborate - maybe too much for the computer
%% - different instruments can play different chords


\section{Conclusions}

In this paper we have presented a system that takes geo-spatial
phenology data and allows users to interact with it using a tangible
interaction metaphor.  The dataset of the flowering dates of Japanese
lilacs was a useful dataset to explore with this system as it
contained data points of flowering dates that occurred at different
times and in different locations from the northernmost to the
southernmost areas in Japan.  

We have explored this dataset with our system.  We have observed a
number of interesting properties of the data and of the system.  One
interesting observation about this data is that in certain years the
flowering of trees occurs earlier, and in some years they occur later.
This is clearly heard in the sonification of this dataset because in
these years, the note that is played for the different instruments is
the same, and is repeated earlier in the cycle than those notes from
other years.  Another observation is that for the data points that
occur earlier in later years, a descending arpeggio is indeed heard.

With the inclusion of the tangible interaction interface, this system
is quite approachable for members of the general public, and in the
few number of interactions that these individuals have had with our
system, they find it both interesting and easy to use.  We are
currently considering doing user studies with this system, with the
goal of building a system to help educate students and the public
about climate change with an engaging interface.

In future work, we would like to develop a similar system to the one
described in this paper but for mobile devices, such as the iPhone.
This interface would allow people to interact with a computer
generated map of a region, for example, a map of Japan and would allow
people to explore the timing of various phenological events on their
own personal mobile device.  In conjunction with this, we are building
a web-enabled version of this app using a combination of Flash and
HTML5 technologies.  The advantage of these web based and iPhone based
applications is that they could have much wider penetration into the
general community, at the cost of a more limited interaction metaphor.

This system can also be used for other phenology datasets, and as
websites such as the National Phenology Network and Nature's Calendar
start releasing their crowdsourced data, we anticipate that there will
be a huge amount of phenological data that would be interesting to
sonify.  In addition, this system could also be used with other
geo-spatial datasets, for example, one could develop an interface to
allow scientists to sonify the amount and type of ground cover as
determined by satellite images.

Although this system was developed as a tool to be used in a single
location, in the future we would like to extend it to allow for remote
collaboration between scientists.  In this system, scientists in
different cities could each have their own map, camera and fiducial
markers.  The fiducial markers would be mapped to unique instruments,
so that for example, one scientist could use fiducial markers that
correspond to different timbres.  By exploiting the ability of humans
to do auditory stream recognition, each scientist could choose to
either focus on the sounds from the instruments that they are
controlling or could focus on sounds that are being generated by a
query from the fiducials of the other scientist.  This type of
multiple user interaction paradigm is often challenging when using
visual interfaces because of problems of occlusion, reach and grasp,
and could be more intuitive and easy to understand when using
sonification instead.

We have made a website\footnote{http://sonophenology.sness.net} that
presents visualizations and sonifications of the data used in this
paper, along with videos showing the system in action.

\section{Acknowledgements}

We would like to thank the National Phenology Website and
Dr. S. Funakoshi for making the Japanese lilac dataset available to
the scientific community.



% -------------------------------------------------------------------------
% Either list references using the bibliography style file IEEEtran.bst
\bibliographystyle{IEEEtran}
\bibliography{icad2010gtzan}
\end{sloppy}
\end{document}
% Template: LaTeX file for ICMC 2009 papers, with hyper-references
%
% derived from the DAFx-06 templates
%
% 1) Please compile using latex or pdflatex.
% 2) Please use figures in vectorial format (.pdf); .png or .jpg are working otherwise 
% 3) Please use the "papertitle" and "pdfauthor" commands defined below

%------------------------------------------------------------------------------------------
\documentclass[twoside,10pt]{article}
\usepackage{icmc2009,amssymb,amsmath}
\usepackage{subfigure}
%\setcounter{page}{1}

\usepackage{mathptmx} 

%____________________________________________________________
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
%==== set the title ====
\def\papertitle{Audioscapes: exploring surface interfaces for music exploration}
%\def\papertitle{}	%-- should be empty for the submission anyway!

%==== 1st submission: author name and affiliation are empty for anonymous submission ====
%\def\paperauthorA{} 
%\affiliation{}{}


%==== final submission: author name and affiliation ====
%---- uncomment 1 to 4 lines, for 1 to 4 authors
\def\paperauthorA{Steven R. Ness}
\def\paperauthorB{George Tzanetakis}

\affiliation{\paperauthorA, \paperauthorB}
  {University of Victoria\\ Department of Computer Science, Victoria, Canada \\{\tt {sness@sness.net,gtzan@cs.uvic.ca}}}

%%---- set correspnding affiliation data for...
%%-- 1 author
%\affiliation{\paperauthorA}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%%-- 2 authors with same affiliation
%\affiliation{\paperauthorA, \paperauthorB}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}
%\affiliation{\paperauthorA, \paperauthorB}
%  {University of Victoria\\ Department of Computer Science, Victoria, Canada \\ {\tt \href{gtzan@cs.uvic.ca}{mailto:gtzan@cs.uvic.ca}}}

%-- 2 authors with different affiliations
%\twoaffiliations{\paperauthorA}{School\\ Department}
%  {\paperauthorB}{Company\\ Address}

%%-- 3 authors with different affiliations
%\threeaffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}

%%-- 4 authors with different affiliations
%\fouraffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}
%  {\paperauthorD}{School C\\ Department Z}

%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ user defined variables  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%%-- if using .ps or .eps figure files, they will be converted on the fly
%%-- RMK: for faster LaTeX runs, use it only once after adding new \includegraphics[]{} cmds
%\usepackage{epstopdf}	 

%---- the hyperref package must be last to properly work
\usepackage[pdftex,
       pdftitle={\papertitle},
	pdfauthor={\paperauthorA},
	colorlinks=false,bookmarksnumbered,pdfstartview=XYZ]{hyperref}
%\pdfcompresslevel=9
\usepackage[pdftex]{graphicx}	% for compatible graphics with hyperref
\usepackage[figure,table]{hypcap}	% corrects the hyper-anchor of figures/tables

\title{\papertitle}

%------------------------------------------------------------------------------------------
\begin{document}

\DeclareGraphicsExtensions{.png,.jpg,.pdf} % used graphic file format for pdflatex
    
\maketitle

\begin{abstract}
  There is a growing interest in touch-based and gestural
  interfaces as alternatives to the dominant mouse, keyboard and 
  monitor interaction. Content and context-aware visualizations 
  of audio collections have been proposed as a more effective way 
  to interact with the increasing amounts of audio data available
  digitally. {\it Audioscapes} is a framework for prototyping and
  exploring how touch-based and gestural controllers can be used with
  state-of-the-art content and context-aware visualizations. By
  providing well-defined interfaces and conventions a variety of
  different audio collections, controllers and visualization methods 
  can be combined to create innovative ways of interacting with large
  audio collections. We describe the overall system architecture, the
  currently available components and specific case studies. 
\end{abstract}

\section{Introduction}\label{sec:introduction}

The size of digital audio collections has been steadily increasing due
to a combination of factors including digital music distribution,
increases in storage capacity, advances in audio compression and the
wide popularity of portable digital music players. Effective
interaction with these large audio collections poses significant
challenges to traditional user interfaces. Portable players and music
playlist management software typically allow users to select artists,
genres or individual tracks by essentially browsing long lists of
text. This mode of interaction, although adequate for small music
collections, becomes increasingly problematic as the collections
become larger. The emerging area of Music Information Retrieval (MIR)
deals with all aspects of managing, analyzing and organizing music in
digital formats. In the past ten years many MIR algorithms and user
interfaces have been proposed that can assist with the browsing and
navigation of large music collections.

%  In several cases these are content
% and context aware interfaces that rely on automatic analysis of the
% audio signal to extract high-level content information. Many of these
% interfaces are proof-of-concept prototypes that, although capable of
% demonstrating the potential of this approach, are not directly
% practical and usable. 

Recently there has been an increasing interest in alternatives to the
traditional mouse/keyboard human-computer interaction. Touch-based and
gestural interfaces have changed status from research curiosities to
being part of many mainstream consumer computing devices. 
% A closely
% related trend is the diversification of form factors beyond the
% traditional desktop/laptop design. Representative examples range from
% mobile phones to immersive displays and tabletop-surfaces.

% We believe that browsing and navigation of large audio and especially
% music collections is a domain that would significantly benefit from
% the use of interfaces that go beyond the traditional keyboard,mouse
% and monitor paradigm. Unfortunately, the large variety of different
% protocols, programming environments and operating systems make
% development of such interfaces more challenging. In addition the
% content and context-aware visualizations required demand state-of-art
% signal processing and machine learning techniques which are not
% familiar to most researchers in human-computer interaction.

{\it AudioScapes} is a framework developed to explore the design space
of non-traditional interfaces for audio and music collection browsing
based on the metaphor of a surface. In this metaphor the individual
audio recordings or music tracks are mapped onto a 2-dimensional
surface which can be navigated using different controller
interfaces. The overall abstract architecture captures the structure
of the majority of existing systems and provides significant design
flexibility in the choice of individual specific components. By
providing well-defined interfaces and conventions, a variety of
different audio collections, controllers and visualization methods can
be combined to create innovative ways of interacting with large audio
collections.

% It is important to note that 
Our design goal is to provide effective interaction without relying on
textual metadata. There are many usage scenarios where having to know
artist names and album titles or having to read text is
impractical. Currently the most common approach in these cases is just
playing random songs (the so-called ``shuffle''). Althout satisfactory
for small and homogeneous collections this approach is not
particularly effective for larger audio collections. These issues
become even more pronounced when the users have vision and/or motion
disabilities. For example, finding a particular artist out of a long
list of text using a scroll-wheel can be very difficult or even
impossible for a user with motor disabilities. Similarly, reading text
on a screen is not directly possible for a blind user. We have used
{\it AudioScapes} to design and prototype interfaces for 
%with feedback from
such users. Although we do not focus on textual metadata, the
proposed interfaces can be 
%augmented or % 
used in conjuction with standard text-based interfaces.



\section{Related Work}\label{sec:relatedwork}

There has been considerable recent interest in the development of
touch-based and gesture based interfaces\cite{ishii97}.  This
represents a movement from traditional Graphic User Interfaces (GUI)
to Touch-Based User Interfaces (TUI) \cite{golshani07}.  These new
forms of interfaces help to bring together the virtual world with the
real world, providing a more inclusive and immersive interaction
environment for users. The iPhone is a device that supports multitouch
interaction, a system where multiple fingers are tracked to provide
different types of functionality. For example, a touch on the surface
with one finger would produce a different effect than when three
fingers are used. In addition gestures such as joining two fingers can
be used for actions such as zooming.


% This is a type of Reality-based interaction \cite{jacob08}, a new
% field which attempts to bridge the world of the virtual with users in
% the real world.  Many such reality-based interaction models use small
% mobile devices.  Another pertinent example is that of ThinSight
% \cite{hodges07}, a technology that allows for multi-touch sensing on
% small, ubiqutious computing devices.

% Another very popular new gesture based interface is the Wii remote
% controller (wiimote) \cite{schou07}, a wireless game controller that
% contains the traditional buttons and gamepads of other game
% controllers, but also contains a three dimensional accelerometer and
% an infrared sensor, which is capable of tracking up to four independent
% infrared light sources in real time. Previous research has included
% work to detect and track fingers \cite{lee08} and also to track two handed
% interaction in open space \cite{vlaming08}.  The wiimote has been used
% to control \cite{wong08} music generation in an interactive music
% performance system, and as a way to track the movement of the hands of
% orchestra conductors \cite{bradshaw08}.  The wiimote has been also
% been used in collaborative computing scenarios as an interactive
% whiteboard \cite{wang08}.

% Another relevant research area is surface based interaction. Surface
% based interaction uses an interface that resembles a tabletop, but
% contains some sort of projection device to render an image on the
% surface, and also a way of tracking one or multiple input sources on
% that surface. The reacTable \cite{jorda05} is one such collaborative 
% interface that has been used by musicians such as Bj\"{o}rk in live 
% musical concert settings.  
%Other such surface based interfaces include
% DiamondSpin \cite{shen04} and SmartSkin \cite{rekimoto02}. These
% systems have paved the way for commercial surface computing platforms
% such as the Microsoft Surface
% \footnote{\url{http://www.microsoft.com/surface}} and the SMART Table
% \footnote{\url{http://www2.smarttech.com/st/en-US/Products/SMART+Table/default.htm}}.
% This type of interaction has been studied in depth in a collaborative
% setting with multiple users with various constraints
% \cite{marshall08}, with elderly users \cite{haiki07}, and as an image
% classification interface \cite{lopezgulliver04}.  Another related
% project was the AudioBrowser \cite{chen06} which developed a touch
% based interface coupled with audio feedback to help blind users access
% information.  Another interesting study is the PHASE installation
% \cite{cahen05}, an interface that used haptic feedback to produce
% music using a game-based metaphor for interaction.  There has also
% been research into the use of collaborative interfaces for the
% generation of music from large crowds of participants
% \cite{feldmeier07} in a dance club like setting.  
% A relevant study to the present work is the MUSICTable \cite{gluck05},
% an interface that used multidimensional scaling to map music onto a
% two dimensional surface.


In the field of Music Information Retrieval, data of high
dimensionality and of considerable complexity is generated. Various
visualization interfaces have been proposed to make this data
accessible and useful to users. Frequently these interfaces rely on
automatically extracted audio features. Islands of Music
\cite{pampalk03} is an example of such a visualization of audio
information which uses the technique of Self-Organizing Maps to
generate a two-dimensional representation of a collection of music.
MusiCream \cite{goto05musicream} is an interface that allows users to
interact with a music collection using a dynamic visualization
interface. 
% MusicRainbow \cite{goto06musicRainbow} is a similar system
% that uses web-based labelling and audio similarity to visualize music
% collections. Another relevant system is MusicSun \cite{PampalkGoto07}
%which combines three different similarity measures to generate music
% recommendations for users.  
The Databionic/MusicMiner system
\cite{morchen05} allows users to organize large collections of music
and employs Emergent Self-Organizing Maps to generate visualizations
of the data involved.
% A very large web
% based system for helping users find new music is part of the LastFM
% website \url{http://playground.last.fm/iom} which provides advanced
% functionality for music recommendation and visualization based on tag
% data. 
A 2006 review of some of the recent trends in visualization in
audio based music information retrieval can be found in Cooper
\cite{cooper06}. 

{\it Audioscapes} is the evolution of several research efforts
by our group \cite{murdoch06, hitchner07} to create novel content and
context-aware music browsing interfaces. We have tried to combine our
previous experience with knowledge from state-of-the-art systems in
this domain to design a flexible framework to explore this new 
and fascinating interface design problem. 


\section{System Architecture}\label{sec:systemarchitecture}

The goal of {\it Audioscapes} is to design a framework for exploring
content and context-aware user interfaces for browsing large audio
collections using controllers beyond the mouse and keyboard. The
abstract system architecture distills the common functional blocks
required to build such interfaces. A large number of existing audio
and music browsing systems fit this architecture which is shown in Figure
~\ref{fig:systemOverview}. The underlying metaphor is that each track in
an audio collection is mapped to a discrete location on a rectangular
grid. More than one track can be mapped to the same
location. Different algorithms for clustering and dimensionality
reduction can be used to map automatically extracted audio features to
the grid coordinates. Controllers take input from the user and either 
interact with the audio collection (for example by initiating playback
or by applying digital audio effects such as pitch-shifting and
time-stretching) or move around on the mapped grid representing the
audio collection. Views are different ways/devices used to display the
grid map. The communication between processing blocks is mainly
accomplished through Open Sound Control (OSC) \cite{osc}
messages or alternatively custom XML or text files. 
% The modular nature
% of the system provides flexibility and extensibility which are crucial
% in this exploratory domain. In the following subsections the currently
% available components of the framework are described. 



\begin{figure}[t]
  \includegraphics[width=\columnwidth]{systemOverview}
  \caption{\it System Architecture}
  \label{fig:systemOverview}
\end{figure}



% \subsection{Data Collections}



\subsection{Audio Processing} 

There are two main aspects of audio processing: digital audio effects
and audio feature extraction. For digital audio effects we currently
support pitch-shifting and time-stretching using a 
% state-of-the-art
phasevocoder
% algorithm \cite{Laroche} 
as well as tunable filters.  The goal of audio feature extraction is
to represent each song in a music collection as a single vector of
features that characterize musical content. Using suitable features,
songs that ``sound'' similar should have vectors that are ``close'' in
the high dimensional feature space.  The features used are the
Spectral Centroid, Rolloff, Flux and the Mel-Frequency Cepstral
Coefficients (MFCC) as well as time-domain zero crossings.

% To capture the feature dynamics we compute a running mean and standard
% deviation over the past M frames:

% \begin{eqnarray}
%   m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
%   s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)]
% \end{eqnarray}

% \noindent 
% where $\Phi(t)$ is the original feature vector. Notice that the
% dynamics features are computed at the same rate as the original
% feature vector but depend on the past M frames (40 in our case
% corresponding to approximately a so called ``texture window'' of 1
% second).  This results in a feature vector of 32 dimensions at the
% same rate as the original 16-dimensional feature vector. The sequence
% of feature vectors is collapsed into a single feature vector
% representing the entire audio clip by taking again the mean and
% standard deviation across the 30 seconds (of the sequence of dynamics
% features) resulting in the final 64-dimensional feature vector per
% audio clip. 

A more detailed description of the features and their
motivation can be found in Tzanetakis and Cook \cite{TC02b}. 
% For the
% calculation of the self-organizing map described in the next section
% all features are normalized so that the minimum of each feature across
% the music collection is 0 and the maximum value is 1. 
This feature set has shown state-of-the-art performance in audio
retrieval and classification tasks in the Music
Information Retrieval Evaluation Exchange (MIREX) 2008
\footnote{\url{http://www.music-ir.org/mirex/}}. 
%The exact details of
%the feature extraction music) for the Freesound collection and the
%samba percussive instruments are slightly different from the process
%described above (which is designed for music).


\begin{figure*}[t]
\centering
\subfigure[Classical]
{
    \label{fig:sub:classical}
    \includegraphics[width=3cm]{classical.pdf}
}
\hspace{1cm}
\subfigure[Metal] 
{
    \label{fig:sub:metal}
    \includegraphics[width=3cm]{metal.pdf}
}
\hspace{1cm}
\subfigure[Hiphop]
{
    \label{fig:sub:hiphop}
    \includegraphics[width=3cm]{hiphop.pdf}
}
\hspace{1cm}
\subfigure[Rock]
{
    \label{fig:sub:rock}
    \includegraphics[width=3cm]{rock.pdf}
}

\caption{Topological mapping of musical content by the Self-Organizing
   Map}
\label{fig:sub1}


\end{figure*}


% \begin{figure*}[t]
% \centering
% \subfigure[Bob Marley]
% {
%     \label{fig:sub1:marley}
%     \includegraphics[width=3cm]{marley.pdf}
% }
% \hspace{1cm}
% \subfigure[Radiohead] 
% {
%     \label{fig:sub1:radiohead}
%     \includegraphics[width=3cm]{radiohead.pdf}
% }
% \hspace{1cm}
% \subfigure[Led Zeppelin]
% {
%     \label{fig:sub1:zeppelin}
%     \includegraphics[width=3cm]{zeppelin.pdf}
% }
% \hspace{1cm}
% \subfigure[Dexter Gordon]
% {
%     \label{fig:sub1:dexter}
%     \includegraphics[width=3cm]{dexter.pdf}
% }
%\caption{Topological mapping of musical content by the Self-Organizing
%   Map}
%\label{fig:sub1}
% \end{figure*}


\subsection{Visualization} 

The primary method used for visualization is the self organizing map
(SOM) which is a type of neural network used to map a high dimensional
input feature space to a lower dimensional representation while
preserving the topology of the high dimensional feature space. This
facilitates both similarity quantization and visualization
simultaneously. The SOM was first documented in 1982 by T. Kohonen,
and since then, it has been applied to a wide variety of diverse
clustering tasks \cite{kohonen95a}. In our system the SOM is used to
map the audio features (64-dimensions) to two discrete coordinates on
a rectangular grid. The traditional SOM consists of a 2D grid of
neural nodes each containing an $n$-dimensional vector, $ {\bf x(t)} $
of data. The goal of learning in the SOM is to cause different
neighbouring parts of the network to respond similarly to certain
input patterns. 
%This is partly motivated by how visual, auditory and
%other sensory information is handled in separate parts of the cerebral
%cortex in the human brain.
The network must be fed a large number of example vectors that
represent, as closely as possible, the kinds of vectors expected
during mapping. 
% The examples are usually applied several times. 
The data associated with each node is initialized to small random
values before training. During training, a series of $n$-dimensional
vectors of sample data are added to the map.  The ``winning'' node of
the map, known as the {\it best matching unit} (BMU), is found by
computing the distance between the added training vector and each of
the nodes in the SOM. This distance is calculated according to some
pre-defined distance metric which in our case is the standard
Euclidean distance on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding
nodes reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. 
% The update formula for a neuron with representative vector N(t) can be
% written as follows:
% \begin{equation}
%     {\bf N}(t + 1) = {\bf N}(t) + \Theta(v,t) \alpha(t) ({\bf x}(t) -
%     {\bf N}(t))
% \end{equation}
% where $\alpha(t)$ is a monotonically decreasing learning coefficient
% and $x(t)$ is the input vector. The neighborhood function
% $\Theta(v,t)$ depends on the lattice distance between the BMU and
% neuron v. We utilize a Gaussian neighborhood function that shrinks
% over time. The time-varying learning rate and neighborhood function
% allow the SOM to gradually converge and form clusters at different
% granularities. In our implementation, $\alpha(t)$ is a linearly-decaying
% function with $t$. Once a SOM has been trained, data may be added to
% the map simply by locating the node whose data is most similar to that
% of the presented sample, ie. the winner.  The reorganization phase is
% omitted when the SOM is not in the training mode. 
% Another interesting
% property of SOMs for our application is that they can be personalized
% by user initialization rather than random initialization.

% We also have explored other possibilities for mapping the
% high-dimensional space to a 2D dimensional grid. Principal Component
% Analysis (PCA) can be used to map the input feature space to the two
% ``highest'' (corresponding to the largest eigen-values) principal
% components followed by quantization to the grid. The main advantage
% of the SOM is that there is no need for quantization and there is
% better coverage of the surface area as it mainly preserves the
% topology of the input space rather than the distance characteristics. 
% We plan to use {\it AudioScapes} as a test-bed for other
% dimensionality reduction and manifold learning methods such as 
% Multidimensional Scaling (MDS), ISOMAP and Local Linear Embedding. 

Figure ~\ref{fig:sub1} illustrates the ability of the extracted
musical content-features and the SOM to represent musical content. The
top subfigures (a), (b), (c) and (d) show how different musical genres
are mapped to different regions of the SOM grid (the black squares are
the ones containing one or more songs from each specific genre). As
can be seen Classical, Heavy Metal and HipHop are well-localized and
distinct whereas Rock is more spread out reflecting its wide
diversity. The SOM is trained on a collection of 1000 songs spanning
10 genres. 
% The bottom subfigures (e), (f), (g), (h) show how different
% artists are mapped to different regions of the SOM grid. The SOM in
%this case is trained on a diverse personal collection of 3000 songs
%spanning many artists and genres. 
It is important to note that in all these cases the only information
used is the automatically analyzed actual audio signal and the
locations of the genres are emergent properties of the SOM.
% genres/artists 

\subsection{View and Control Interfaces}

The common functionality among view interfaces is to display the
automatically calculated grid, respond to navigation events and handle
audio playback and effects. Typically the grid squares are colored
darker or lighter based on the number of tracks that they contain.
The most powerful view is a desktop graphical user interface written
in Qt \footnote{\url{http://www.qtsoftware.com/products}}. In addition
to standard view functionality it provides the ability to write iTunes
music library XML files, advanced coloring modes based on metadata,
and continuous playback mode in which tracks change automatically when
the cursor moves to a different grid square without requiring explicit
clicking by the user. In addition we also provide a web-interface that
although more limited has the advantage that anyone on the internet
can access and interact with the particular {\it AudioScape}
deployed. As the audio is streamed, the audio collection remains on
the server, this can be an important factor in commercial
applications.  
%It also provides an accessible platform to conduct
%demonstrations and user studies that can be updated frequently without
%requiring any user action.


% \begin{figure}[htb]
% \includegraphics[width=60mm]{iphone}
% \label{fig:iPhone} 
% \caption{iPhone control interface}
% \end{figure}


In order to explore non-standard form factors we have developed a
implementation specific to the iPhone
\footnote{\url{http://www.apple.com/iphone}}. Having a
touch-based display surface facilitates spatial awareness especially
for blind or limited vision users. As the user moves her finger across
the various squares, songs from each corresponding node cross-fade
with each other to help her navigate the music collection by hearing
how the songs in each grid location are changing. By laying out a
music collection in this spatial fashion, navigation with only the
knowledge of a few reference points is needed. 
% For example, if it is
% known that Rock music is in the upper left corner, and jazz music is
% in the lower left corner, by dragging from top to bottom along the
% left edge of the grid, rock music will slowly transition into jazz
% music. The use of multi-touch (two or more fingers) may also be used
% to control playback. Swiping the surface with two fingers in the right
% direction skips to another song in the same node while swiping left
% plays the previous song in that node. Figure ~\ref{fig:iPhone} shows
% the iPhone view. Finally we have also explored tabletop views based on
% the desktop and web implementations on a Smart Table system and well
% as a Mitsubishi Diamond Touch. However we did not take advantage of
% their multi-touch capabilities as it would require writing specialized
% versions of the software.
% \subsection{Control interfaces}
In addition to the traditional mouse/keyboard based control, we have
explored various alternative control interfaces. The Radiodrum
\cite{schloss01} is a 
% novel
three dimensional controller that uses
capacitive sensing to detect the positions of two radio frequency
oscillators, usually attached to drum sticks or other similar objects.
% Developed by Max Matthews and Bob Boie, it was originally designed as
% the first three dimensional computer mouse, but found a more pertinent
% application in the area of the production of computer music
% \cite{nevile03}.  It has also been used as a controller to facilitate
% the browsing of music collections \cite{murdoch06}.
In our prototype, we have mapped the x, y and z axes of the sticks of
the Radiodrum to the surface user interface.  Movement of the sticks
in the x and y axes moves the audio track selector cursor on the GUI,
and movement in z controls the volume of that track. Each stick
controls a different music track. 
% We use both of
% the sticks of the Radiodrum, each of which can independently select a
% different musical track.
% With the addition of
% volume control, the interface transforms from a simple music browsing
% interface to a more DJ-like experience. 
%, with the performer able to
% control the sound mix between two different audio tracks by moving the
% sticks up and down on the z axis. 

%  Some preliminary tests of this
% interface show that it is a potent and exciting way to create mashups
% and mixes of a variety of different music styles, with styles like
% reggae, rock and hiphop.

% The Wii remote, or wiimote, is a multimodal interface device developed
% by Nintendo for use with the Wii game system.  The wiimote has
% traditional buttons and rumble functionality, but also contains a
% speaker, an 3-dimensional accelerometer, and an Infrared (IR) sensor.
% This IR sensor has the ability to track up to 4 independent sources of
% infrared light, and reports back the positions and intensities of the
% detected points.  All data from the wiimote is sent back to the
% computer via Bluetooth.

% In order to enhance the configurability and expandability of this
% project, we use the OSC protocol to transmit messages recieved from
% the Wiimote controller. Using OSC, we have added functionality that
% allows for the use of multiple Wiimotes at once, each one returning
% the positions of four different people in a space. As an example we
% have developed SOMba a system for collaborative creation of Samba
% rhythms by dancers in a space. The rhythms and instrument sounds are
% arranged in a grid using a Self-Organizing Map. In the canonical SOMba
% system, we use two wii-motes, allowing up to 8 dancers to be tracked,
% but this number could easily be expanded.  It is important to align
% the sensors of the wiimotes accurately with the performance space and
% with each other, so that an individual dancer is only tracked by one
% wiimote at once. In our current system we use the Wiimote for position
% tracking of the dancers, but the use of OSC allows us to use a variety
% of other position trackers quite easily.


\subsection{Data Collections and Implementation} 

In order to explore different configurations we have created {\it
  AudioScapes} for several large audio data collections which are
known to the MIR and Computer Music community. The Freesound Project
\footnote{\url{http://freesound.org}} is a huge collection of sound
effects, music and environmental songs all licenced under the Creative
Commons licence. 
% The website makes extensive use of folksonomy based
% methods of audio classification primarily through a process of
% collaborative tagging of audio files as well as through geotagging,
% sample packs, and a remix tree view.  
% The audio in this database is an
% wide variety of formats, including .wav, .mp3, .aiff and .au, amongst
% others, and is recorded at many different sample rates and bit depths.
% For our current research, we selected a well-behaved subset of the
% audio in the Freesound database, and converted all the audio to a
% common format, sample rate, and bit depth.
The RWC (Real World Computing) Music Database \cite{goto03} is a
database of music that has been copyright cleared and made available
to the Music Information Retrieval community. 
% It contains large
% amounts of high quality audio samples and musical pieces.  There are
% large numbers of short samples of audio from different musical
% instruments around the world, including both tonal and percussive
% instruments.  
The music in the RWC database is from a wide variety of
genres, with many classical and jazz pieces, as well as a sampling of
the genres of popular, rock, dance, jazz, latin, classical, marches,
world music, vocals, and traditional Japanese music.
A collection of 1000 music tracks from 10 genres was gathered and described
in \cite{TC02b}.  
% The music in this collection includes such diverse
% genres as blues, classical, country, disco, hiphop, jazz, metal, pop,
% reggae and rock.  It has been used extensively by the Music
% Information Retrieval community.
We have also used a large database of
3000 30-second snippets from the personal collection of one of the
authors. 
% Another collection consists of samples of percussion
% instruments of Samba music. 
The Marsyas \footnote{\url{http://marsyas.sourceforge.net}} audio
processing software framework has been used for the audio feature
extraction, digital audio effects, calculation of the SOM, the desktop
graphical user interface and handling of controller data.
% \cite{Marsyas}.  
% The calculation of the SOM in Marsyas is carried out
% after the feature extraction in a data-flow architecture. This
% architecture allows for easy integration and rapid execution of all
% the required processing steps, and is readily extensible to support a
% diverse set of different experimental parameters. The Qt interface to
% Marsyas is used for display of the generated data and user
% interaction.
For the web based interface we employ an \emph{XHTML/CSS} and
\emph{Flash} based interface. 
% The user is presented with a simple and
% \emph{XHTML/CSS} web page that has been designed to be standards
% compliant which will facilitate accessibility by the research
% community on a wide variety of different web browsers and computer
% platforms.  
% The \emph{Flash} based interface is written in the haXe
% \cite{mccoll08} programming language, which compiles the ECMAScript
% language \emph{haXe} down to \emph{Flash} bytecodes. The \emph{Flash}
% interface presents a simple interface to the user with an interface
% similar to the Qt based MarGrid interface.
The Open Sound Control (OSC) protocol \cite{osc} is used in this
project to facilitate communication between the various components of
the system. 
% The main MarGrid Qt interface presents an OSC listener
% which all other controllers send messages to. The Wii-mote Bluetooth
% messages are translated to OSC to update the grid position. In a
% similar manner, the MIDI messages from the RadioDrum are translated
% into OSC messages. This system architecture lets us easily add new
% controllers to our system.  Because OSC is able to send it's messages
% over a network, we are able to run the controller programs on
% different machines than the machine producing the audio, thus
% facilitating experimentation and collaboration between multiple users.


\section{Discussion}\label{sec:conclusions}

{\it AudioScapes} is an extensible framework and architecture for
surface-based interfaces for browsing large audio and music
collections. Given the exploratory nature of the work we have not yet
been able to conduct detailed quantitative user studies which are
planned for the future. It is our hope that the developed interfaces
have the potential to make browsing of audio collections much more
effective, especially for users with special needs. We have been
particularly fortunate to receive initial feedback from two blind
users, one user with limited vision and one user with limited
mobility. In all cases they were very positive about the system and
provided valuable advice. As it is difficult to convey how the system
works in paper we have collected videos and web demonstrations on a
web-page \url{http://audioscapes.sness.net}. 
%Most of the software is
%freely available as part of the {\it Marsyas} audio processing
%framework and we welcome feedback and contributions.



\section{Acknowledgements}\label{sec:acknowledgements}


National Science and Research Council of Canada (NSERC) funded parts
of this work. Manjinder Benning, Darren Minifie, Allan Kumka, Jennifer
Murdoch and Stephen Hitchner worked on different aspects of the
framework. 


\bibliographystyle{IEEEtranS}
\bibliography{icmc2009gtzan-freesound}

\end{document}

% Template: LaTeX file for ICMC 2009 papers, with hyper-references
%
% derived from the DAFx-06 templates
%
% 1) Please compile using latex or pdflatex.
% 2) Please use figures in vectorial format (.pdf); .png or .jpg are working otherwise 
% 3) Please use the "papertitle" and "pdfauthor" commands defined below

%------------------------------------------------------------------------------------------
\documentclass[twoside,10pt]{article}
\usepackage{icmc2009,amssymb,amsmath}
\usepackage{subfigure}
%\setcounter{page}{1}

\usepackage{mathptmx} 

%____________________________________________________________
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
%==== set the title ====
\def\papertitle{Audioscapes: exploring surface interfaces for music exploration}
%\def\papertitle{}	%-- should be empty for the submission anyway!

%==== 1st submission: author name and affiliation are empty for anonymous submission ====
\def\paperauthorA{} 
\affiliation{}{}


%==== final submission: author name and affiliation ====
%---- uncomment 1 to 4 lines, for 1 to 4 authors
%\def\paperauthorA{Steven R. Ness}
%\def\paperauthorB{George Tzanetakis}
%\def\paperauthorB{Darren Minife}


%%---- set correspnding affiliation data for...
%%-- 1 author
%\affiliation{\paperauthorA}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%%-- 2 authors with same affiliation
%\affiliation{\paperauthorA, \paperauthorB}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}
\affiliation{\paperauthorA, \paperauthorB}
  {University of Victoria\\ Department of Computer Science, Victoria, Canada \\ {\tt \href{gtzan@cs.uvic.ca}{mailto:gtzan@cs.uvic.ca}}}

%-- 2 authors with different affiliations
%\twoaffiliations{\paperauthorA}{School\\ Department}
%  {\paperauthorB}{Company\\ Address}

%%-- 3 authors with different affiliations
%\threeaffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}

%%-- 4 authors with different affiliations
%\fouraffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}
%  {\paperauthorD}{School C\\ Department Z}

%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ user defined variables  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%%-- if using .ps or .eps figure files, they will be converted on the fly
%%-- RMK: for faster LaTeX runs, use it only once after adding new \includegraphics[]{} cmds
%\usepackage{epstopdf}	 

%---- the hyperref package must be last to properly work
\usepackage[pdftex,
       pdftitle={\papertitle},
	pdfauthor={\paperauthorA},
	colorlinks=false,bookmarksnumbered,pdfstartview=XYZ]{hyperref}
%\pdfcompresslevel=9
\usepackage[pdftex]{graphicx}	% for compatible graphics with hyperref
\usepackage[figure,table]{hypcap}	% corrects the hyper-anchor of figures/tables

\title{\papertitle}

%------------------------------------------------------------------------------------------
\begin{document}

\DeclareGraphicsExtensions{.png,.jpg,.pdf} % used graphic file format for pdflatex
    
\maketitle

\begin{abstract}
  There is a growing interest in touch-based and gestural
  interfaces as alternatives to the dominant mouse, keyboard and 
  monitor interaction. Content and context-aware visualizations 
  of audio collections have been proposed as a more effective way 
  to interact with the increasing amounts of audio data available
  digitally. {\it Audioscapes} is a framework for prototyping and
  exploring how touch-based and gestural controllers can be used with
  state-of-the-art content and context-aware visualizations. By
  providing well-defined interfaces and conventions a variety of
  different audio collections, controllers and visualization methods 
  can be combined to create innovative ways of interacting with large
  audio collections. We describe the overall system architecture, the
  currently available components and specific case studies. 
\end{abstract}

\section{Introduction}\label{sec:introduction}

The size of digital audio collections has been steadily increasing due
to a combination of factors including digital music distribution,
increases in storage capacity, advances in audio compression and the
wide popularity of portable digital music players. Effective
interaction with these large audio collections poses significant
challenges to traditional user interfaces. Portable players and music
playlist management software typically allow users to select artists,
genres or individual tracks by essentially browsing long lists of
text. This mode of interaction, although adequate for small music
collections, becomes increasingly problematic as the collections become
larger.

The emerging area of Music Information Retrieval (MIR) deals with all
aspects of managing, analyzing and organizing music in digital
formats. In the past ten years many MIR algorithms and user
interfaces have been proposed that can assist with the browsing and
navigation of large music collections. In several cases these are content
and context aware interfaces that rely on automatic analysis of the
audio signal to extract high-level content information. Many of these
interfaces are proof-of-concept prototypes that, although capable of
demonstrating the potential of this approach, are not directly
practical and usable. 

Recently there has been an increasing interest in alternatives to the
traditional mouse/keyboard human-computer interaction. Touch-based and
gestural interfaces have changed status from research curiosities to
being part of many mainstream consumer computing devices. A closely
related trend is the diversification of form factors beyond the
traditional desktop/laptop design. Representative examples range from
netbooks and mobile phones to immersive displays and tabletop
surfaces.

We believe that browsing and navigation of large audio and especially
music collections is a domain that would significantly benefit from
the use of interfaces that go beyond the traditional keyboard,mouse
and monitor paradigm. Unfortunately, the large variety of different
protocols, programming environments and operating systems make
development of such interfaces more challenging. In addition the content and
context-aware visualizations required demand state-of-art signal
processing and machine learning techniques which are not familiar to
most researchers in human-computer interaction.

{\it AudioScapes} is a framework developed to explore the design space
of non-traditional interfaces for audio and music collection browsing
based on the metaphor of a surface. In this metaphor the individual audio
recordings or music tracks are mapped onto a 2-dimensional surface
which can be navigated using different controller interfaces. The
overall abstract architecture captures the structure of the majority
of existing systems and provides significant design flexibility in the
choice of individual specific components. By providing well-defined
interfaces and conventions, a variety of different audio collections,
controllers and visualization methods can be combined to create
innovative ways of interacting with large audio collections.

It is important to note that our design goal is to provide effective
interaction without relying on textual metadata. There are many usage
scenarios where having to know artist names and album titles or having
to read text is impractical. Currently the most common approach in
these cases is just playing random songs (the so-called
``shuffle''). Althout satisfactory for small and homogeneous
collections this approach is not particularly effective for larger
audio collections. These issues become even more pronounced when the
users have vision and/or motion disabilities. For example, finding a
particular artist out of a long list of text using a scroll-wheel can
be very difficult or even impossible for a user with motor
disabilities. Similarly, reading text on a screen is not directly
possible for a blind user. We have used {\it AudioScapes} to design
and prototype interfaces with feedback from such users. Although we
do not focus on textual metadata, the proposed interfaces can be
augmented or used in conjuction with standard text-based interfaces.



\section{Related Work}\label{sec:relatedwork}

There has been considerable recent interest in the development of
touch-based and gesture based interfaces\cite{ishii97}.  This
represents a movement from traditional Graphic User Interfaces (GUI) to
Touch-Based User Interfaces (TUI) \cite{golshani07}.  These new forms
of interfaces help to bring together the virtual world with the real
world, providing a more inclusive and immersive interaction
environment for users.

The iPhone is a device that supports multitouch interaction, a system where
multiple fingers are tracked to provide different types of
functionality.  For example, a touch on the surface with one finger
would produce a different effect than when three fingers are used. In 
addition gestures such as joining two fingers can be used for actions such as
zooming. This is a type of Reality-based interaction \cite{jacob08}, a
new field which attempts to bridge the world of the virtual 
with users in the real world.  Many such reality-based
interaction models use small mobile devices.  Another pertinent example
is that of ThinSight \cite{hodges07}, a technology that allows for
multi-touch sensing on small, ubiqutious computing devices.

Another very popular new gesture based interface is the Wii remote
controller (wiimote) \cite{schou07}, a wireless game controller that
contains the traditional buttons and gamepads of other game
controllers, but also contains a three dimensional accelerometer and
an infrared sensor, which is capable of tracking up to four independent
infrared light sources in real time. Previous research has included
work to detect and track fingers \cite{lee08} and also to track two handed
interaction in open space \cite{vlaming08}.  The wiimote has been used
to control \cite{wong08} music generation in an interactive music
performance system, and as a way to track the movement of the hands of
orchestra conductors \cite{bradshaw08}.  The wiimote has been also
been used in collaborative computing scenarios as an interactive
whiteboard \cite{wang08}.

Another relevant research area is surface based interaction. Surface
based interaction uses an interface that resembles a tabletop, but
contains some sort of projection device to render an image on the
surface, and also a way of tracking one or multiple input sources on
that surface. The reacTable \cite{jorda05} is one such collaborative
interface that has been used by musicians such as Bj\"{o}rk in live
musical concert settings.  Other such surface based interfaces include
DiamondSpin \cite{shen04} and SmartSkin \cite{rekimoto02}. These
systems have paved the way for commercial surface computing platforms
such as the Microsoft Surface
\footnote{\url{http://www.microsoft.com/surface}} and the SMART Table
\footnote{\url{http://www2.smarttech.com/st/en-US/Products/SMART+Table/default.htm}}.

This type of interaction has been studied in depth in a collaborative
setting with multiple users with various constraints
\cite{marshall08}, with elderly users \cite{haiki07}, and as an image
classification interface \cite{lopezgulliver04}.  Another related
project was the AudioBrowser \cite{chen06} which developed a touch
based interface coupled with audio feedback to help blind users access
information.  Another interesting study is the PHASE installation
\cite{cahen05}, an interface that used haptic feedback to produce
music using a game-based metaphor for interaction.  There has also
been research into the use of collaborative interfaces for the
generation of music from large crowds of participants
\cite{feldmeier07} in a dance club like setting.  Particularily
relevant study to the present work is the MUSICTable \cite{gluck05},
an interface that used multidimensional scaling to map music onto a
two dimensional surface.

In the field of Music Information Retrieval, data of high
dimensionality and of considerable complexity is generated. Various
visualization interfaces have been proposed to make this data
accessible and useful to users. Frequently these interfaces rely on
automatically extracted audio features. 

Islands of Music \cite{pampalk03} is an example of such a
visualization of audio information which uses the technique of
Self-Organizing Maps to generate a two-dimensional representation of a
collection of music.  MusiCream \cite{goto05musicream} is an interface
that allows users to interact with a music collection using a dynamic
visualization interface.  MusicRainbow \cite{goto06musicRainbow} is a
similar system that uses web-based labelling and audio similarity to
visualize music collections.  Another relevant system is MusicSun
\cite{PampalkGoto07} which combines three different similarity
measures to generate music recommendations for users.  The
Databionic/MusicMiner system \cite{morchen05} allows users to organize
large collections of music and employs Emergent Self-Organizing Maps
to generate visualizations of the data involved.  A very large web
based system for helping users find new music is part of the LastFM
website \url{http://playground.last.fm/iom} which provides advanced
functionality for music recommendation and visualization based on tag
data. A 2006 review of some of the recent trends in visualization in
audio based music information retrieval can be found in Cooper
\cite{cooper06}. 

{\it Audioscapes} is the evolution of several research efforts
by our group \cite{murdoch06, hitchner07} to create novel content and
context-aware music browsing interfaces. We have tried to combine our
previous experience with knowledge from state-of-the-art systems in
this domain to design a flexible framework to explore this new 
fascinating interface design problem. 


\section{System Architecture}\label{sec:systemarchitecture}

The goal of {\it Audioscapes} is to design a framework for exploring
content and context-aware user interfaces for browsing large audio
collections using controllers beyond the mouse and keyboard. The
abstract system architecture distills the common functional blocks
required to build such interfaces. A large number of existing audio
and music browsing systems fit this architecture shown in Figure
~\ref{fig:systemOverview}. The underlying metaphor is that each track in
an audio collection is mapped to a discrete location on a rectangular
grid. More than one track can be mapped to the same
location. Different algorithms for clustering and dimensionality
reduction can be used to map automatically extracted audio features to
the grid coordinates. Controllers take input from the user and either 
interact with the audio collection (for example by initiating playback
or by applying digital audio effects such as pitch-shifting and
time-stretching) or move around on the mapped grid representing the
audio collection. Views are different ways/devices used to display the
grid map. The communication between processing blocks is mainly
accomplished through Open Sound Control (OSC) \cite{osc}
messages or alternatively custom XML or text files. The modular nature
of the system provides flexibility and extensibility which are crucial
in this exploratory domain. In the following subsections the currently
available components of the framework are described. 



\begin{figure}[t]
  \includegraphics[width=\columnwidth]{systemOverview}
  \caption{\it System Architecture}
  \label{fig:systemOverview}
\end{figure}



\subsection{Data Collections}

In order to explore different configurations we have created {\it
  AudioScapes} for several large audio data collections which are
known to the MIR and Computer Music community. The Freesound Project
\footnote{\url{http://freesound.org}} is a huge collection of sound
effects, music and environmental songs all licenced under the Creative
Commons licence.  The website makes extensive use of folksonomy based
methods of audio classification primarily through a process of
collaborative tagging of audio files as well as through geotagging,
sample packs, and a remix tree view.  The audio in this database is an
wide variety of formats, including .wav, .mp3, .aiff and .au, amongst
others, and is recorded at many different sample rates and bit depths.
For our current research, we selected a well-behaved subset of the
audio in the Freesound database, and converted all the audio to a
common format, sample rate, and bit depth.

The RWC (Real World Computing) Music Database \cite{goto03} is a
database of music that has been copyright cleared and made available
to the Music Information Retrieval community.  It contains large
amounts of high quality audio samples and musical pieces.  There are
large numbers of short samples of audio from different musical
instruments around the world, including both tonal and percussive
instruments.  The music in the RWC database is from a wide variety of
genres, with many classical and jazz pieces, as well as a sampling of
the genres of popular, rock, dance, jazz, latin, classical, marches,
world music, vocals, and traditional Japanese music.

A collection of 1000 music from 10 genres was gathered and described
in \cite{TC02b}.  The music in this collection includes such diverse
genres as blues, classical, country, disco, hiphop, jazz, metal, pop,
reggae and rock.  It has been used extensively by the Music
Information Retrieval community. We have also used a large database of
3000 30-second snippets from the personal collection of one of the
authors. Another collection consists of samples of percussion
instruments of Samba music. 


\subsection{Audio Processing} 

There are two main aspects of audio processing: digital audio effects
and audio feature extraction. For digital audio effects we currently
support pitch-shifting and time-stretching using a state-of-the-art
phasevocoder algorithm \cite{Laroche} as well as tunable filters. 

The goal of audio feature extraction is to represent each song in a
music collection as a single vector of features that characterize
musical content. Using suitable features, songs that ``sound'' similar
should have vectors that are ``close'' in the high dimensional feature
space.  The features used are the Spectral Centroid, Rolloff, Flux and
the Mel-Frequency Cepstral Coefficients (MFCC) as well as time-domain
zerocrossings. 

To capture the feature dynamics we compute a running mean and standard
deviation over the past M frames:

\begin{eqnarray}
  m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
  s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)]
\end{eqnarray}

\noindent 
where $\Phi(t)$ is the original feature vector. Notice that the
dynamics features are computed at the same rate as the original
feature vector but depend on the past M frames (40 in our case
corresponding to approximately a so called ``texture window'' of 1
second).  This results in a feature vector of 32 dimensions at the
same rate as the original 16-dimensional feature vector. The sequence
of feature vectors is collapsed into a single feature vector
representing the entire audio clip by taking again the mean and
standard deviation across the 30 seconds (of the sequence of dynamics
features) resulting in the final 64-dimensional feature vector per
audio clip. A more detailed description of the features and their
motivation can be found in Tzanetakis and Cook \cite{TC02b}. For the
calculation of the self-organizing map described in the next section
all features are normalized so that the minimum of each feature across
the music collection is 0 and the maximum value is 1. This feature set
has shown state-of-the-art performance in audio retrieval and
classification tasks for example in the Music Information Retrieval
Evaluation Exchange (MIREX) 2008. The exact details of the feature
extraction music) for the Freesound collection and the
samba percussive instruments are slightly different from the process
described above (which is desinged for music). 


\begin{figure*}[t]
\centering
\subfigure[Classical]
{
    \label{fig:sub:classical}
    \includegraphics[width=3cm]{classical.pdf}
}
\hspace{1cm}
\subfigure[Metal] 
{
    \label{fig:sub:metal}
    \includegraphics[width=3cm]{metal.pdf}
}
\hspace{1cm}
\subfigure[Hiphop]
{
    \label{fig:sub:hiphop}
    \includegraphics[width=3cm]{hiphop.pdf}
}
\hspace{1cm}
\subfigure[Rock]
{
    \label{fig:sub:rock}
    \includegraphics[width=3cm]{rock.pdf}
}

\label{fig:sub}
\end{figure*}


\begin{figure*}[t]
\centering
\subfigure[Bob Marley]
{
    \label{fig:sub1:marley}
    \includegraphics[width=3cm]{marley.pdf}
}
\hspace{1cm}
\subfigure[Radiohead] 
{
    \label{fig:sub1:radiohead}
    \includegraphics[width=3cm]{radiohead.pdf}
}
\hspace{1cm}
\subfigure[Led Zeppelin]
{
    \label{fig:sub1:zeppelin}
    \includegraphics[width=3cm]{zeppelin.pdf}
}
\hspace{1cm}
\subfigure[Dexter Gordon]
{
    \label{fig:sub1:dexter}
    \includegraphics[width=3cm]{dexter.pdf}
}
\caption{Topological mapping of musical content by the Self-Organizing
  Map}
\label{fig:sub1}
\end{figure*}


\subsection{Visualization} 

The primary method used for visualization is the self-organizing map
(SOM) which is a type of neural network used to map a high dimensional
input feature space to a lower dimensional representation while
preserving the topology of the high dimensional feature space. This
facilitates both similarity quantization and visualization
simultaneously. The SOM was first documented in 1982 by T. Kohonen,
and since then, it has been applied to a wide variety of diverse
clustering tasks \cite{kohonen95a}. In our system the SOM is used to
map the audio features (64-dimensions) to two discrete coordinates on
a rectangular grid.

The traditional SOM consists of a 2D grid of neural nodes each
containing an $n$-dimensional vector, $ {\bf x(t)} $ of data. The goal
of learning in the SOM is to cause different neighbouring parts of the
network to respond similarly to certain input patterns. This is partly
motivated by how visual, auditory and other sensory information is
handled in separate parts of the cerebral cortex in the human brain.

The network must be fed a large number of example vectors that
represent, as closely as possible, the kinds of vectors expected
during mapping. The examples are usually applied several times. The
data associated with each node is initialized to small random values
before training. During training, a series of $n$-dimensional vectors
of sample data are added to the map.  The ``winning'' node of the map
known as the {\it best matching unit} (BMU) is found by computing the
distance between the added training vector and each of the nodes in
the SOM. This distance is calculated according to some pre-defined
distance metric which in our case is the standard Euclidean distance
on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding
nodes reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. 


The update formula for a neuron with representative vector N(t) can be
written as follows:
\begin{equation}
    {\bf N}(t + 1) = {\bf N}(t) + \Theta(v,t) \alpha(t) ({\bf x}(t) -
    {\bf N}(t))
\end{equation}
where $\alpha(t)$ is a monotonically decreasing learning coefficient
and $x(t)$ is the input vector. The neighborhood function
$\Theta(v,t)$ depends on the lattice distance between the BMU and
neuron v. We utilize a Gaussian neighborhood function that shrinks
over time. The time-varying learning rate and neighborhood function
allow the SOM to gradually converge and form clusters at different
granularities. In our implementation, $\alpha(t)$ is a linearly-decaying
function with $t$. Once a SOM has been trained, data may be added to
the map simply by locating the node whose data is most similar to that
of the presented sample, ie. the winner.  The reorganization phase is
omitted when the SOM is not in the training mode. Another interesting
property of SOMs for our application is that they can be personalized
by user initialization rather than random initialization.

We also have explored other possibilities for mapping the
high-dimensional space to a 2D dimensional grid. Principal Component
Analysis (PCA) can be used to map the input feature space to the two
``highest'' (corresponding to the largest eigen-values) principal
components followed by quantization to the grid. The main advantage
of the SOM is that there is no need for quantization and there is
better coverage of the surface area as it mainly preserves the
topology of the input space rather than the distance characteristics. 
We plan to use {\it AudioScapes} as a test-bed for other
dimensionality reduction and manifold learning methods such as 
Multidimensional Scaling (MDS), ISOMAP and Local Linear Embedding. 

Figure ~\ref{fig:sub1} illustrates the ability of the extracted
musical content-features and the SOM to represent musical content. The
top subfigures (a), (b), (c) and (d) show how different musical genres
are mapped to different regions of the SOM grid (the black squares are
the ones containing one or more songs from each specific genre). As
can be seen Classical, Heavy Metal and HipHop are well-localized and
distinct whereas Rock is more spread out reflecting its wide
diversity. The SOM is trained on a collection of 1000 songs spanning
10 genres. The bottom subfigures (e), (f), (g), (h) show how different
artists are mapped to different regions of the SOM grid. The SOM in
this case is trained on a diverse personal collection of 3000 songs
spanning many artists and genres. It is important to note that in all
these cases the only information used is the automatically analyzed
actual audio signal and the locations of the genres/artists are
emergent properties of the SOM.


\subsection{View interfaces}

The common functionality among view interfaces is to display the
automatically calculated grid, respond to navigation events and handle
audio playback and effects. Typically the grid squares are colored
darker or lighter based on the number of tracks that they contain.
The most powerful view is a desktop graphical user interface written
in Qt \footnote{\url{http://www.qtsoftware.com/products}}. In addition
to standard view functionality it provides the ability to write iTunes
music library XML files, advanced coloring modes based on metadata,
and continuous playback mode in which tracks change automatically when
the cursor moves to a different grid square without requiring explicit
clicking by the user. In addition we also provide a web-interface that
although more limited has the advantage that anyone on the internet
can access and interact with the particular {\it AudioScape}
deployed. As the audio is streamed, the audio collection remains on the
server, this can be an important factor in commercial applications.  It also
provides an accessible platform to conduct demonstrations and user
studies that can be updated frequently without requiring any user action.


\begin{figure}[htb]
\includegraphics[width=60mm]{iphone}
\label{fig:iPhone} 
\caption{iPhone control interface}
\end{figure}


In order to explore non-standard form factors we have developed a
implementation specific to the iPhone
\footnote{\url{http://www.apple.com/iphone}}. Having a
touch-based display surface facilitates spatial awareness especially
for blind or limited vision users. As the user moves her finger across
the various squares, songs from each corresponding node cross-fade
with each other to help her navigate the music collection by hearing
how the songs in each grid location are changing. By laying out a
music collection in this spatial fashion, navigation with only the
knowledge of a few reference points is needed. For example, if it is
known that Rock music is in the upper left corner, and jazz music is
in the lower left corner, by dragging from top to bottom along the
left edge of the grid, rock music will slowly transition into jazz
music. The use of multi-touch (two or more fingers) may also be used
to control playback. Swiping the surface with two fingers in the right
direction skips to another song in the same node while swiping left
plays the previous song in that node. Figure ~\ref{fig:iPhone} shows
the iPhone view. Finally we have also explored tabletop views based on
the desktop and web implementations on a Smart Table system and well
as a Mitsubishi Diamond Touch. However we did not take advantage of
their multi-touch capabilities as it would require writing specialized
versions of the software.


\subsection{Control interfaces}

In addition to the traditional mouse/keyboard based control we have
explored various alternative control interfaces. The Radiodrum
\cite{schloss01} is a novel three dimensional controller that uses
capacitive sensing to detect the positions of two radio frequency
oscillators, usually attached to drum sticks or other similar objects.
Developed by Max Matthews and Bob Boie, it was originally designed as
the first three dimensional computer mouse, but found a more pertinent
application in the area of the production of computer music
\cite{nevile03}.  It has also been used as a controller to facilitate
the browsing of music collections \cite{murdoch06}.

In our prototype, we have mapped the x, y and z axes of the sticks of
the Radiodrum to the MarGrid user interface.  Movement of the sticks
in the x and y axes moves the audio track selector cursor on the
MarGrid GUI, and movement in z controls the volume of that track.  We
use both of the sticks of the Radiodrum, each of which can
independently select a different musical track.  With the addition of
volume control, the interface transforms from a simple music browsing
interface to a more DJ-like experience, with the performer able to
control the sound mix between two different audio tracks by moving the
sticks up and down on the z axis.  Some preliminary tests of this
interface show that it is a potent and exciting way to create mashups
and mixes of a variety of different music styles, with styles like
reggae, rock and hiphop.

The Wii remote, or wiimote, is a multimodal interface device developed
by Nintendo for use with the Wii game system.  The wiimote has
traditional buttons and rumble functionality, but also contains a
speaker, an 3-dimensional accelerometer, and an Infrared (IR) sensor.
This IR sensor has the ability to track up to 4 independent sources of
infrared light, and reports back the positions and intensities of the
detected points.  All data from the wiimote is sent back to the
computer via Bluetooth.

In order to enhance the configurability and expandability of this
project, we use the OSC protocol to transmit messages recieved from
the Wiimote controller. Using OSC, we have added functionality that
allows for the use of multiple Wiimotes at once, each one returning
the positions of four different people in a space. As an example we
have developed SOMba a system for collaborative creation of Samba
rhythms by dancers in a space. The rhythms and instrument sounds are
arranged in a grid using a Self-Organizing Map. In the canonical SOMba
system, we use two wii-motes, allowing up to 8 dancers to be tracked,
but this number could easily be expanded.  It is important to align
the sensors of the wiimotes accurately with the performance space and
with each other, so that an individual dancer is only tracked by one
wiimote at once. In our current system we use the Wiimote for position
tracking of the dancers, but the use of OSC allows us to use a variety
of other position trackers quite easily.


\subsection{Implementation} 


The Marsyas \footnote{\url{http://marsyas.sourceforge.net}} audio
processing software framework has been used for the audio feature
extraction, digital audio effects, calculation of the SOM, the desktop
graphical user interface and handling of controller data
\cite{Marsyas}.  The calculation of the SOM in Marsyas is carried out
after the feature extraction in a data-flow architecture. This
architecture allows for easy integration and rapid execution of all
the required processing steps, and is readily extensible to support a
diverse set of different experimental parameters. The Qt interface to
Marsyas is used for display of the generated data and user
interaction.

For the web based interface we employ an \emph{XHTML/CSS} and
\emph{Flash} based interface. The user is presented with a simple and
\emph{XHTML/CSS} web page that has been designed to be standards
compliant which will facilitate accessibility by the research
community on a wide variety of different web browsers and computer
platforms.  The \emph{Flash} based interface is written in the haXe
\cite{mccoll08} programming language, which compiles the ECMAScript
language \emph{haXe} down to \emph{Flash} bytecodes. The \emph{Flash}
interface presents a simple interface to the user with an interface
similar to the Qt based MarGrid interface.

The Open Sound Control (OSC) protocol \cite{osc} is used in this
project to facilitate communication between the various components of
the system. The main MarGrid Qt interface presents an OSC listener
which all other controllers send messages to. The Wii-mote Bluetooth
messages are translated to OSC to update the grid position. In a
similar manner, the MIDI messages from the RadioDrum are translated
into OSC messages. This system architecture lets us easily add new
controllers to our system.  Because OSC is able to send it's messages
over a network, we are able to run the controller programs on
different machines than the machine producing the audio, thus
facilitating experimentation and collaboration between multiple users.


\section{Discussion}\label{sec:conclusions}

{\it AudioScapes} is an extensible framework and architecture for
surface-based interfaces for browsing large audio and music
collections. Given the exploratory nature of the work we have not yet
been able to conduct detailed quantitative user studies which are
planned for the future. It is our hope that the developed interfaces
have the potential to make browsing of audio collections much more
effective especially for users with special needs. We have been
particularly fortunate to receive initial feedback from two blind
users, one user with limited vision and one user with limited
mobility. In all cases they were very positive about the system and
provided valuable advice. As it is difficult to convey how the system
works in paper we have collected videos and web demonstrations on a
web-page \url{http://audioscapes.sness.net}. Most of the software is
freely available as part of the {\it Marsyas} audio processing 
framework and we welcome feedback and contributions. 



\section{Acknowledgements}\label{sec:acknowledgements}


National Science and Research Council of Canada (NSERC) funded parts
of this work. Manjinder Benning, Darren Minifie, Allan Kumka, Jennifer
Murdoch and Stephen Hitchner worked on different aspects of the
framework. 


\bibliographystyle{IEEEtranS}
\bibliography{icmc2009gtzan-freesound}

\end{document}

% Created 2009-02-20 Fri 17:16
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}


\title{table}
\author{Steven Ness}
\date{20 February 2009}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents

\begin{tabular}{rrr}
  2  &   20  &   20  \\
  3  &   38  &   44  \\
  4  &   56  &   56  \\
  5  &   74  &   74  \\
  6  &   92  &   89  \\
  7  &  110  &  105  \\
  8  &  128  &  121  \\
  9  &  146  &  153  \\
 10  &  164  &  161  \\
 11  &  182  &  181  \\
 12  &  200  &  195  \\
 13  &  218  &  212  \\
 14  &  236  &  243  \\
 15  &  254  &  251  \\
\end{tabular}



\begin{tabular}{rrr}
  2  &   20  &   20  \\
  3  &   38  &   44  \\
  4  &   56  &   56  \\
  5  &   74  &   74  \\
  6  &   92  &   89  \\
  7  &  110  &  105  \\
  8  &  128  &  121  \\
  9  &  146  &  137  \\
 10  &  164  &  153  \\
 11  &  182  &  169  \\
 12  &  200  &  185  \\
 13  &  218  &  201  \\
 14  &  236  &  217  \\
 15  &  254  &  233  \\
\end{tabular}


\end{document}
% Template: LaTeX file for ICMC 2009 papers, with hyper-references
%
% derived from the DAFx-06 templates
%
% 1) Please compile using latex or pdflatex.
% 2) Please use figures in vectorial format (.pdf); .png or .jpg are working otherwise 
% 3) Please use the "papertitle" and "pdfauthor" commands defined below

%------------------------------------------------------------------------------------------
\documentclass[twoside,10pt]{article}
\usepackage{icmc2009,amssymb,amsmath} 
%\setcounter{page}{1}

\usepackage{mathptmx} 

%____________________________________________________________
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
%==== set the title ====
\def\papertitle{SOMba : Multiuser music creation using Self-Organizing Maps and motion tracking}
%\def\papertitle{}	%-- should be empty for the submission anyway!

%==== 1st submission: author name and affiliation are empty for anonymous submission ====
%\def\paperauthorA{} 
%\affiliation{}{}


%==== final submission: author name and affiliation ====
%---- uncomment 1 to 4 lines, for 1 to 4 authors
\def\paperauthorA{Steven R. Ness}
\def\paperauthorB{George Tzanetakis}

%%---- set correspnding affiliation data for...
%%-- 1 author
%\affiliation{\paperauthorA}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%%-- 2 authors with same affiliation
\affiliation{\paperauthorA, \paperauthorB}
  {University of Victoria\\ Department of Computer Science, Victoria, Canada \\{\tt {sness@sness.net,gtzan@cs.uvic.ca}}}

%-- 2 authors with different affiliations
%\twoaffiliations{\paperauthorA}{School\\ Department}
%  {\paperauthorB}{Company\\ Address}

%%-- 3 authors with different affiliations
%\threeaffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}

%%-- 4 authors with different affiliations
%\fouraffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}
%  {\paperauthorD}{School C\\ Department Z}

%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ user defined variables  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%%-- if using .ps or .eps figure files, they will be converted on the fly
%%-- RMK: for faster LaTeX runs, use it only once after adding new \includegraphics[]{} cmds
%\usepackage{epstopdf}	 

%---- the hyperref package must be last to properly work
\usepackage[pdftex,
       pdftitle={\papertitle},
	pdfauthor={\paperauthorA},
	colorlinks=false,bookmarksnumbered,pdfstartview=XYZ]{hyperref}
%\pdfcompresslevel=9
\usepackage[pdftex]{graphicx}	% for compatible graphics with hyperref
\usepackage[figure,table]{hypcap}	% corrects the hyper-anchor of figures/tables

\title{\papertitle}

%------------------------------------------------------------------------------------------
\begin{document}

\DeclareGraphicsExtensions{.png,.jpg,.pdf} % used graphic file format for pdflatex
    
\maketitle

\begin{abstract}
SOMba is a system where multiple users create new rhythms and music by
moving around a physical space while being tracked in real time using
the infrared sensor on the Wii remote control.  The physical space
they move around in is mapped to a 2D Self-Organizing map.  This SOM
is created using the Marsyas audio processing framework from a
collection of aligned 1-bar Samba rhythms of a variety of Brazilian
musical instruments.  A user is mapped to a unique point in the SOM,
and this point contains a single rhythm.  As the user moves around the
2D space, different rhythms are played.  Multiple users can move
around a space, and each would generate a different rhythm.


\end{abstract}

\section{Introduction}\label{sec:introduction}

Since the dawn of music in pre-historic tribal societies, music has
often been accompanied by dance.  In some traditions, for example in
native american tribes, the dancers themselves would participate in
the music making experience by wearing noise making artifacts.
However, in most situations, music making and dancing have been
relatively separate activities.  When Samba bands perform in Brazil,
musicians are joined by large numbers of dancers, who only indirectly
participate in the music making experience.

Self-organizing maps \cite{kohonen95a} are a dimensionality reduction
technique where a high dimension dataset is mapped to a lower,
typically 2-dimensional surface.  SOMs and other such dimensionality
reduction tools have been used to allow users to visualize data
relationships within complex datasets.  There have been many
applications of SOMs in Music Information Retrieval including work by
automatically analyzing and organizing music archives \cite{rauber01a}
\cite{rauber98b}, visualizing music genres \cite{pampalk03} and music
recommendation \cite{vembu05a}.

A less explored frontier of SOMs and music is the production of new
music.  Until recent times, the production of music has been solely by
highly trained musicians.  Recently, commericial systems like Rock
Band and Guitar Hero along with many academic research projects have
attempted to put the control of the musical experience in the hands of
less experienced users.  In addition ystems like Dance Dance Revolution and other
academic projects have begun to bridge the gap between dancing and
music creation.

Our current research project proposes to let dancers create their own
music, by generating music collaboratively with other dancers.  We
achieve this by tracking the positions of each of the dancers in two
dimensions on a theatre stage.  We can then translate these stage
positions into positions in a 2D SOM.  We have used the SOM
methodology because it provides a straightforward way to map the
high dimensional audio features to a two dimensional surface.

In the field of embodied music cognition it has been noted
\cite{leman08} that people seek involvement with music in order to
experience a behavioural resonance with physical energy.  Our system
attempts to project both the spatial motion of the dancer, and her
interactions with other dancers, into music.  This allows dancers to
become instrumentalists and intermedial composers, simultaneously
interpreting the generative music composition.  In traditional
full-body tracked music generation systems, the gestures and movements
of dancers are transformed into music, but in our system, we are more
interested in relationships between the dancers within not only a
physical space, but also within their socially constructed
environment.

The SOM that we build contains aligned rhythmic phrases of different
drum patterns from Samba music.  Samba music is built from many
overlayed drum patterns, with each sounding on different beats of a
typically 2/4 metrical pattern.  It is a highly syncopated music style
with each instrument, from the large surdo drum, to the small hand
held clave, playing on different beats of the bar.  Although each
pattern in a typical Samba song is simple, the patterns that result
from all the instruments playing together can be quite complex and
interesting.

\subsection{Related Work}

Self organizing maps have been used extensively in the visualization
of data for audio based music information retrieval \cite{cooper06}.
They have been used to analyze and organize music archives
\cite{rauber01a} \cite{rauber98a} \cite{rauber98b} \cite{rauber02a},
and to visualize the resulting music collections \cite{rauber03a}
\cite{pampalk04} \cite{rauber02}.  A particularily relevant study was
that of Palmalk in his paper Islands of Music \cite{pampalk03}.  SOMs
have also been used for audio retrieval, browsing and constructivist
learning in several papers \cite{cano02} \cite{fruehwirth01}
\cite{honkela00}.  While the previous mentioned studies concentrated
on organizing whole classes of music, SOMs have also been applied to
smaller audio segments, including timbre \cite{toirvainen97},
energy-spectrum \cite{masugi04}, and musical time series analysis
\cite{capinteiro98}.

There has been considerable research on the automatic generation of
music, including the generation of background music 
\cite{rui07} \cite{yoo04}, the creation of rhythmic
patterns \cite{labordus83b} \cite{kasahara07} and more general automated music generation systems
\cite{erb84} \cite{unemi01}.  An excellent study was conducted back in
1970 by Howe \cite{howe70} about compositional considerations when
creating electronic music.

A particularily relevant study was recently conducted
\cite{kasahara07}, in this paper the authors describe a
self-organizing map system that allows users to create rhythms
co-creatively and interactively.  Also closely related was the
\cite{tzimeas07} SENEgal project, which used genetic algorithms to
create rhythms from western Africa.

Often the previous described systems have their user interaction
paradigm centered on the computer system.  In this study we are
interested in bringing the creation of music into the physical
environment.  The creation of new methods of interacting with the
computer has seen much activity, including projects using the
Radiodrum \cite{benning07} \cite{murdoch06} , SmartSkin
\cite{rekimoto02} and Cyber composer \cite{ip04}.  A particularily
relevant paper involved the use of large numbers of giveaway sensors
in a large rave dance setting \cite{feldmeier07}.

Our project extends and simplifies this by using the infrared sensor
capabilities of the Wii remote control, or wiimote.  The wiimote
connects to a computer via Bluetooth, and using the cwiid
\cite{cwiid07} library on Linux, we are able to easily access the data
it provides.  The wiimote has buttons and accelerometers, and also has
the capability to track up to 4 infrared sources.  The positions of
these sources are tracked by the wiimote.

\section{The SOMba System}

The SOMba system consists of three distinct parts, the generated music
tracks for each of the different instruments, a process to map this
audio onto a two dimensional representation, and a way to interact
with the grid that is created.  For each of these parts, we used the
Marsyas \footnote{\url{http://marsyas.sourceforge.net}} programming framework, a toolkit
written in C++ that allows for the creation, analysis and output of
audio.

\subsection{Creation of Musical Tracks}

We obtained transcriptions of authentic Samba music from Brazil and
converted them into a simple string based machine readable
representation.  We also obtained from the RWC \cite{goto03} library
audio samples of some of the musical instruments used by Brazilian
samba bands, including surdo, agogo bells, tambourim, shaker and
quijada.  A program was written to generate audio files of equal
lengths using the transcription along with an audio sample for each
instrument.  The resulting audio was found to be too mechanical in
feel, so we added the ability to add a small amount of random time
jitter to each sample, and created several versions of each
transcription with slightly different timings of each beat.  We also
created versions of each audio track with different audio effects,
including high pass and low pass filters, as well as phaser and
flanger effects.

\subsection{Music Feature Extraction}

Each audio track is represented as a single feature vector. Even
though much more elaborate audio track representations have been
proposed in the literature we have found that a single feature vector
per audio clip is well suited to use in machine learning in general
and the SOM algorithm in particular.  It has been shown that such
song-level features perform quite well \cite{mandel-ismir2005}.

The features used in our approach are Spectral Centroid, Roll-Off,
Flux and Mel-Frequency Cepstral Coefficients (MFCC). To capture the
feature we compute a running mean and standard deviation over the past
$M$ frame.  This results in a feature vector of 32 dimensions at the
same rate as the original 16-dimensional one.  A more detailed
description of the features can be found in Tzanetakis and Cook
\cite{TC02b}.

This process extracts audio features from the tracks, and to more
accurately capture the rhythmic differences between tracks, we use the
original string representation of the track and calculate the Hamming
distance between it and a string that is composed solely of rests.  We
then add this measure to the feature vector.  We also take the average
jitter value for all the notes in the track and add this as another
component of the feature vector.

\subsection{Self-Organizing Map Generation}

The resulting audio tracks were then mapped to a two dimensional space
using Self-Organizing Maps (SOMs).  SOMs are a technique for
transforming data of multiple dimensions into a lower number of
dimensions, typically two.  They were first introduced in 1982 by
Teuvo Kohonen \cite{kohonen95a} and were based on Artificial Neural
Networks.  SOMs are a form of dimensionality reduction, which is a
broad term for any technique that transforms a higher dimensional
space into a lower dimensional space.

We used the existing SOM implementation within Marsyas to transform
the high dimensional feature vectors into a two-dimensional
represention.  For the neighbourhood function, we started with a value
of 0.17, and decreased this by a factor of 0.98 for each round of
training.  We found that a 12x12 grid worked well for this collection
of feature vectors and training parameters and resulted in a final
grid with all cells containing at least one track of audio.

\subsection{Interaction}

The Wii remote, or wiimote, is a multimodal interface device developed
by Nintendo for use with the Wii game system.  The wiimote has
traditional buttons and rumble functionality, but also contains a
speaker, an 3-dimensional accelerometer, and an Infrared (IR) sensor.
This IR sensor has the ability to track up to 4 independent sources of
infrared light, and reports back the positions and intensities of the
detected points.  All data from the wiimote is sent back to the
computer via Bluetooth.  We then use the OSC protocol to transmit
messages recieved from the Wiimote controller to the SOMba program.
In our current system we use the Wiimote for position tracking of the
dancers, but the use of OSC allows us to use a variety of other
position trackers quite easily.

Using OSC, we have added functionality that allows for the use of
multiple Wiimotes at once, each one returning the positions of four
dancers at once.  In the canonical SOMba system, we use two wii-motes,
allowing up to 8 dancers to be tracked, but this number could easily
be expanded.  It is important to align the sensors of the wiimotes
accurately with the performance space and with each other, so that an
individual dancer is only tracked by one wiimote at once.  In real
performance settings static and dynamic reflections and poor camera
resolution deteriorate the tracking results.  State of the art
tracking technologies provide built-in multi-camera calibration and
elaborate 3D point tracking algorithms to solve these problems.  To
help alleviate this problem in our simple system, we have found that
it was important to use very bright infrared emitters, and to limit
the intensity of stage lights on the dancers.


\section{Conclusions}

Dancing and music have long been close companions, and recent advances
in technology can allow us to further blend these together into a new
form of musical and physical articulation.  By allowing dancers to
interact with and create their own music, we hope to create new
exciting opportunities for creative expression.  We are also
interested in the spatial representation of music, and by transferring
Self-Organizing maps from the computer screen into physical space, we
anticipate to discover new ways of interacting with and visualizing
data.  Another part of this research is to use self-organizing maps
not just to organize different songs, but different rhythmic patterns.
Such a project might prove interesting for creation of music using
different interfaces.

\section{Acknowledgements}

We would like to thank Gabrielle Odowichuck and Manjinder Benning for
assistance in this project.


%%\newpage
%\nocite{*}
\bibliographystyle{IEEEbib}
\bibliography{icmc2009gtzan-somba} % requires file template.bib

\end{document}

%
% derived from the DAFx-06 templates
% derived from the ICMC 2009 templates by Steve Beck
% and then derived from the ICMC 2010 template
% 1) Please compile using latex or pdflatex.
% 2) Please use figures in vectorial format (.pdf); .png or .jpg are working otherwise 
% 3) Please use the "papertitle" and "pdfauthor" commands defined below

%------------------------------------------------------------------------------------------
\documentclass[twoside,10pt,a4paper]{article}
\usepackage{icmc2011,amssymb,amsmath} 
%\setcounter{page}{1}

\usepackage{mathptmx} 

%____________________________________________________________
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
%==== set the title ====
\def\papertitle{Gesture Analysis of radiodrum data}
%\def\papertitle{}	%-- should be empty for the submission anyway!

%==== 1st submission: author name and affiliation are empty for anonymous submission ====
\def\paperauthorA{tet} 
\affiliation{}{}


%==== final submission: author name and affiliation ====
%---- uncomment 1 to 4 lines, for 1 to 4 authors
\def\paperauthorA{Steven R. Ness}
\def\paperauthorB{Sonmez Methabi}
\def\paperauthorC{Gabrielle Odowichuk}
\def\paperauthorD{Andrew W. Schloss}
\def\paperauthorE{George Tzanetakis}

%%---- set correspnding affiliation data for...
%%-- 1 author
%\affiliation{\paperauthorA}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%%-- 2 authors with same affiliation
%\affiliation{\paperauthorA, \paperauthorB}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%-- 2 authors with different affiliations
\twoaffiliations{\paperauthorA, \paperauthorB, }{\textit{Gabrielle Odowichuk, George Tzanetakis} \\  \\ University of Victoria\\ Department of Computer Science}
  {\paperauthorD}{University of Victoria\\ School of Music}

%%-- 3 authors with different affiliations
%\threeaffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}

%-- 4 authors with different affiliations
% sness - Had to hack this to get the spacing to work out right
%% \fouraffiliations
%%  	{\paperauthorA \ \ \ }{University of Victoria\\ Department of Computer Science}
%%  	{\paperauthorB}{University of Victoria\\ Department of Computer Science}
%%  	{ \ \ \ \paperauthorC\ \ \ \ \ \ \ }{\ \ \ \ \ \ University of Victoria \ \ \ \ \ \ \ \ \ \ \\ \ \ \ \ \ \  \ \ \ \ \ \ \ School of Music\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  }
%%  	{\paperauthorD}{University of Victoria\\ Department of Computer Science}

%% \threeaffiliations
%%  	{\paperauthorA, \paperauthorB}{University of Victoria\\ Department of Computer Science}
%%  	{\paperauthorC}{University of Victoria\\ School of Music}
%%  	{\paperauthorD}{University of Victoria\\ Department of Computer Science}

%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ user defined variables  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%%-- if using .ps or .eps figure files, they will be converted on the fly
%%-- RMK: for faster LaTeX runs, use it only once after adding new \includegraphics[]{} cmds
%\usepackage{epstopdf}	 

%---- the hyperref package must be last to properly work
\usepackage[pdftex,
       pdftitle={\papertitle},
	pdfauthor={\paperauthorA},
	colorlinks=false,bookmarksnumbered,pdfstartview=XYZ]{hyperref}
%\pdfcompresslevel=9
\usepackage[pdftex]{graphicx}	% for compatible graphics with hyperref
\usepackage[figure,table]{hypcap}	% corrects the hyper-anchor of figures/tables

\title{\papertitle}

%------------------------------------------------------------------------------------------

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}   
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\begin{document}

\DeclareGraphicsExtensions{.png,.jpg,.pdf} % used graphic file format for pdflatex
    
\maketitle

\begin{abstract}
The radiodrum is a virtual controller/interface that has existed in
various forms since its initial design at Bell Laboratories in the
1980's, and it is still being developed.  It is a percussion
instrument, while at the same time an abstract 3D gesture/position
sensor.  There are two main modalities of the instrument that are used
by composers and performers: the first is similar to a percussive
interface, where the performer hits the surface, and the instrument
reports position (x,y,z) and velocity (u) of the hit; thereby it has 6
degrees of freedom.  The other mode, which is unique to this
instrument (at least in the domain of percussive interfaces), is
moving the sticks in the space above the pad, whereby the instrument
also reports (x,y,z) position in space above the surface.

In this paper we describe techniques for identifying different
gestures using the Radio Drum, which could include signals like a
circle or square, or other physically intuitive gestures, like the
pinch-to-zoom metaphor used on mobile devices such as the iPhone. Two
approaches to gesture analysis are explored. The first one is based on
feature classification using support vector machines and the second is
using Dynamic Time Warping. By allowing users to interact with the
system using a complex set of gestures, we have produced a system that
will allow for a richer vocabulary for composers and performers of
electro-acoustic music.  These techniques and vocabulary are useful
not only for this particular instrument, but can be modified for other
3D sensors.

\end{abstract}

\section{Introduction}\label{sec:introduction}

The radiodrum \cite{schloss89} \cite{tindale05} is a virtual
controller/interface that has existed in various forms since its
initial design at Bell Laboratories in the 1980's, and it is still
being developed.  Using capacitive sensing, it can detect the position
of two augmented drum sticks in a space above a receiver antenna.  It
is a percussion instrument, while at the same time an abstract 3D
gesture/position sensor.  Currently, there are two modes of
interaction: the first is similar to a percussive interface, where the
performer hits the surface, and the instrument reports position (x,y,z)
and velocity (u) of the hit; thereby it has 6 degrees of freedom.  We
call this ``whack'' mode.  Note that this mode does not depend on
hitting the surface; it is detected by the change of direction of the
stick and not by any impact as in conventional drum pads.  The other
mode, which we call ``continuous'' mode, is unique to this instrument
(at least in the domain of percussive interfaces).  This mode involves
moving the sticks through space above the pad, whereby the instrument
also reports (x,y,z) position in space above the surface at any
desired sampling rate.

In the current work we propose to extend the radiodrum with a new
modality of interaction with a system that recognizes gestures made by
the performer.  These gestures can be as simple as a sweep across the
surface of the radiodrum, or can be as complex as the composer or
performer desires.  These gestures are then sent to a system the
produces sound or image.  These gestures can either be mapped to
actions that send metadata to the patch, for example, a different
section of a musical piece could be triggered, or can be mapped
directly to sound producing modules.  In the case of a direct mapping
to sound, the speed of the gesture itself could be mapped to musical
properties of the sound.

\section{Background}

The word gesture is a highly overloaded term, and has a variety of
meanings in a number of different fields.  Because of this, the
concept of gesture can be viewed as a boundary object \cite{star89},
that is, a concept that is at once adaptable to different viewpoints,
but also robust enough to maintain its identity across different
fields.  A detailed examination of gestures can be found in Cadoz and
Wanderley \cite{cadoz00}, where they categorize gestures in music as
effective gestures, accompanist gesture and figurative gestures.  The
gestures detected by our system fall into all three of these
categories.  In a review by Overholt \cite{overholt07}, three basic
groups of gestural controllers for music are summarized, which include
those inspired by instruments, augmented instruments and alternative
instruments.  Our system is inspired by drums, but can also be viewed
as an augmented drum.

\section{Related Work}

There have been many approaches to detecting gestures in the current
literature.  These include computer vision based methods, and
approaches using accelerometer data. Early work in the field of
gesture recognition employed the concept of Space-time gestures
\cite{darrell93} in which sets of view models are captured by a
Computer Vision system and are matched to stored gesture patterns
using Dynamic Time Warping (DTW). Although this system was not used to
produce music, it is relevant to the current work because it uses the
same technique of Dynamic Time Warping to recognize patterns in a
multi-dimensional space.

Another more recent paper \cite{corradini01} also uses DTW for the
recognition of a small gesture library, in a non-realtime setting.  It
takes hand-arm gestures from a small, predefined vocabulary and uses a
DTW engine to align these gestures in time and also to perform
normalization on them. 

More relevant to the current work are papers that perform gesture
recognition on the data from accelerometers, such as those now
commonly found on devices such as the iPhone and Wii-mote.  An early
work in this field was described in ``SICIB: An Interactive Music
Composition System Using Body Movements'' \cite{moralesmanzanares01}
where a rule based coupling mechanism linking the position, velocity,
acceleration, curvature, torsion of movements and jumps of dancers is
mapped to intensity and tone in music sequences.

Another recent paper relevant to current work describes
uWave\cite{liu09}, an accelerometer-based personalized gesture based
system.  This system is unusual in that the system uses a single
prototypical example of each gesture, and uses DTW to recognize a
simple gesture vocabulary containing eight signs, derived from the
Nokia gesture alphabet. Akl et al. \cite{akl10} describe an approach
that extends this work using DTW, affinity propagation and compressive
sensing.  In this paper, 18 gestures are recognized, as opposed to 8
for the uWave\cite{liu09} paper.  The addition of compressive sensing
allows gestures to be reduced to a more sparse representation that is
then matched using DTW.

Another interesting new approach \cite{wu09} does not use either DTW
or heuristics, but instead uses the machine learning technique of
Support Vector Machines (SVM) and does gesture recognition with 3D
accelerometer data.  In this paper, a system that uses a frame-based
descriptor and a multi-class SVM is described.  With frame-based
descriptors, instead of using the entire time series, a reduced
representation is used, and in this paper, spectral descriptors
calculated by a Fast Fourier Transform are used.  This paper describes
how this approach outperforms DTW, Naive Bayes, C4.5 and Hidden Markov
Model (HMM) machine learning systems.

The approaches described in the previous paragraphs have positive
attributes as well as challenges.  Computer vision based approaches
have the advantage that electronic cameras are now cheap commodity
hardware and are easily available.  However, computer vision based
approaches are fundamentally limited by their hardware requirements of
cameras and transmitters and high computational load \cite{liu09}.
They also have a low acquisition framerate as compared to other
approaches.

%% Smart glove based solutions can recognize very fine gestures but
%% require the user to wear a glove tagged with multiple sensors to
%% capture finger and hand motions in fine granularity \cite{liu09}.
%% They also require a high overhead of engagement with the user.

Accelerometer based approaches have the advantage that with MEMS
technology small cheap solutions are becoming common \cite{wu09} and
they provide a high speed and continuous source of data, ideal for
machine learning approaches.  However, accelerometers suffers from
abrupt changes due to hand shaking\cite{akl10}.

In the past three years, we collaborated with Bob Boie, the original
designer of the radiodrum at Bell Labs in the 1980's, in the design of
a new instrument that does not use MIDI at all.  Boie worked with Max
Mathews in the 1980's, and is known for being one of the original
designers of capacitive sensing tablets, including very early versions
of trackpads and similar devices.  There is considerable confusion in
the computer music community about the radiodrum vs. Radio Drum
vs. Radio Baton and the history thereof.  We will not go into detail
here, but suffice it to say that this new instrument is more accurate
and satisfying to use in performance that the older versions.  At the
moment, we only have one prototype, but we are in the process of
building more instruments.

MIDI works reasonably well for asynchronous data like drum-hits, but
it is badly suited for continuous position data.  The new instrument
that Boie designed is entirely analog, and we collect data from it by
amplitude-modulating the analog sensor data in order to take it out of
the DC frequency range.  Then we digitize it using an ordinary audio
interface, and this allows us to use almost any sampling rate so we
get very precise temporal data with no jitter or throughput issues,
orders of magnitude faster than data obtained by computer vision
techniques.

Our radiodrum-based approach has the advantage that absolute position
data is transmitted, and these data are in the form of (x,y,z)
three-tuples that have a direct relation to the position of the sticks
in the real world. They also have the advantage that extremely high
rates of data with excellent time resolution can be obtained.  Since
the radiodrum is a 3D sensor, we know exactly where the sticks are at
all times, not just when they strike the surface.


\section{System Description}

Our system works by matching gestures to a library of template
gestures, usually provided by the performer or composer.  It then
performs real-time matching of gesture data from the radiodrum in the
form of a list of tuples of x,y,z values and matches this stream of
data to the template library, returning the matching template gesture.

The mapping of gestures to sounds is in the domain of the composer,
who in our system would be responsible for creating this mapping of
gestures to sounds. To use our system, the composer and performer
would define an alphabet of gestures that will be used.

During the performance of the piece of music, when the musician wants
a gesture to be recognized, they then push a foot switch which
activates the gesture recognition system, and then execute a gesture.
A similarity matrix between this gesture and all examples of all
gestures in the system is then calculated.  A similarity matrix of two
examples of the gesture for ``A'' is shown in Figure \ref{fig:a0-a1}.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm]{a0-a1}
\end{center}
\caption{
Similarity matrix for two examples of the sign ``A'' along with the
best path as determined by Dynamic Time Warping shown in red.}
\label{fig:a0-a1} 
\end{figure} 

\section{Results}

We have implemented two independent systems for doing gesture
recognition of radiodrum data.  The first of these is a system based
on Dynamic Time Warping (DTW) \cite{darrell93} and the second is a
system based on a frame based implementation using a Support Vector
Machine (SVM) machine learning classifier.

\subsection{Dynamic Time Warping}

In our DTW implementation, we take the X and Y pairs of a performed
gesture and compare each of the X and Y time series to all the
examples of the gesture in our gesture alphabet.  Specifically, we
first take the X time series of the performed gesture and calculate a
similarity matrix between all points of this gesture and each gesture
in our database.  We then run the DTW algorithm on this
similarity matrix and find the cost of the path as determined by the
DTW algorithm, where a lower cost path indicates a more likely gesture
match.  We then repeat this for the Y time series and sum the scores
for the X and Y values.  The lowest cost path over all the gestures in
our alphabet is then returned as the most likely match.

We performed a series of experiments to evaluate the performance of
the DTW algorithm.  For the first of these, we compared three very
distinct signs, those of ``A'', ``K'' and ``C'' as used in the Palm
Pilot graffiti alphabet.  Exemplars of these three letters are shown
in Figure \ref{fig:ack}.  We took 8 to 10 examples of each of these
letters and performed DTW against all 30 examples in the gesture
database.  We then calculated the precision and recall of the top 8
returned gestures.  The results of this experiment are shown in Table
\ref{table:precisionrecall}.  As one can see, the precision for each
of these is high, for A and K the precision is 1, which means all of
the 8 returned gestures matched the query gesture correctly.  For all
three examples, the recall was also high, and varied between 0.727 and
0.888.  In addition, for all three gestures the top result (Top-1) was
correct in all cases, this is the most important measure because it
matches more closely the behaviour of the system in a performance
implementation.

In doing these experiments we noticed that we can get a higher recall
and consequently a higher F-Measure if we specify a cut-off threshold
for DTW score, since this way we eliminate more irrelevant gestures in
the retrieved gestures set. In order to investigate this further,
first we found the optimum threshold for a training set and then to
evaluate the system with obtained thresholds, we tested the system on
the testing set and the result is shown in Table 2
\ref{table:fmeasuretraining}.


\subsection{Support Vector Machine}

In order to extract features for each gesture, first, we divided each
gesture into N equal sized segments.  Then frames are formed by
putting two adjacent segments (each with the length $Ls = L/(N)$)
together and in a way that every two adjacent frame have a segment in
common. In other words, the frame $i$ consists of the segments i and
segment $i + 1$. So, we have $N -1$ frames each with the length $2*Ls$
and each frame consists of two time series axis $x_t$ and $y_t$. The
feature vector of each gesture consists of the feature vectors of all of
its frames. So if the feature vector of the frame i is called $f_i$ then
the feature vector of each gesture is: $F=f_0+ ...+ f_{N-1}$.  So we need
to calculate the feature vector of each frame. In order to do that we
input the x and y axis into FFT function separately, and in the
frequency domain, we calculated the mean and the energy feature:

\[ Mean =\mu_{T,K} = t_{T,K0}^0 \]

\[ Energy = \epsilon_{T,k} = \frac{\sum_{n=1}^{Ls,2-1}|t_{T,k}^n|^2}{|Ls,2-1|} \]

Where the vector t is the output of the FFT function, $T\ =\ x,\ y\ and\ k
=0,...,N-1$.  After calculating the features for all the frames, we
end up with the feature vector of the gesture:

\[ \tau = ( \mu_{T,K},\epsilon{T,k}) \]

Now the gesture i can be represented by $(g_i , \tau_i)$ and fed to the
binary SVM to train the classifier to recognize the new features.

In this experiment, we first trained the classifier with the training
set that includes 7 samples of the gesture ``A'' and 7 samples of the
gesture ``B''. Then in order to evaluate the model, we used a test set
including 3 samples of the gesture ``A'' and 3 samples of the gesture
``B''.

We repeated this operation for 10 different number of Ns, and three
pairs of gestures (a,b), (c,o) and (k,x).  The results are shown in
Table \ref{table:svm}.  From this table it can be seen that by
choosing the right frame size of the SVM classifier it is possible for
this SVM method to outperform the DTW method.

\begin{table} 
\begin{tabular}{|l|l|l|l|}
\hline
 Gesture   &  Precision  &  Recall & Top-1  \\
\hline
A & 1.0 & 0.888 & 9/9 \\
B & 1.0 & 1.0 & 10/10 \\
K & 1.0 & 0.727 & 10/10 \\
C & 0.792 & 0.704 & 8/8 \\ 
E & 0.818 & 0.595 & 10/10 \\
O & 1.0   & 0.889 & 10/10 \\
\hline
\end{tabular}
\caption{Precision and Recall for three different gestures}
\label{table:precisionrecall}
\end{table}

\begin{table} 
\begin{tabular}{|l|c|c|c|}
\hline
Threshold & F-measure & F-measure & F-measure \\
          &   ``A'' &  ``K'' &  ``O '' \\
\hline
0.020 & 0.966 & 0.927 & 0.982 \\
0.030 & 0.974 & 0.949 & 0.988 \\
0.031 & 0.974 & 0.951 & 0.989 \\
0.036 & 0.967 & 0.952 & 0.990 \\
0.042 & 0.952 & 0.947 & 0.992 \\
0.050 & 0.921 & 0.929 & 0.986 \\
\hline
\end{tabular}
\caption{F-measure for 3 different signs at a variety of training cutoff levels}
\label{table:fmeasuretraining}
\end{table}

\begin{table} 
\begin{tabular}{|l|l|l|l|}
\hline
Frames & Precision & Precision & Precision \\
       & A,B & C,O & K,E \\
\hline
2    & 1.0    & 0.75    & 1.0     \\
4    & 1.0    & 0.66    & 1.0     \\
6    & 0.75   & 1.0     & 1.0     \\
8    & 1.0    & 1.0     & 1.0     \\
10   & 1.0    & 1.0     & 1.0     \\
DTW  & 1.0    & 0.972   & 0.983   \\
\hline
\end{tabular}
\caption{Average Precision and Recall when using the Support Vector
  Machine testing/training approach to gesture recognition.  Shown in
  the last line of the table are the average results for the DTW
  algorithm.}
\label{table:svm}
\end{table}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm]{ack}
\end{center}
\caption{
Exemplars of the gestures ``A'', ``C'' and ``K'' as X,Y pairs of data,
with the Z axis flattened to the plane of the page.}
\label{fig:ack} 
\end{figure} 

\section{Discussion}

Our system is designed for a professional percussionist for live
performances, and thus time and care can be spent in optimizing the
dictionary of signs.  This would also be dependent on the piece of
music and the importance of distinguishing similar signs.  

Data from an accelerometer suffers from abrupt changes due to hand
shaking \cite{akl10}.  In our case, the user moves a stick through
space, and the added mass of the stick helps to mitigate this.  In
addition, this system is intended to be used by experienced
percussionists, who are trained to have good hand coordination when
using percussion mallets or sticks, this fact is indirectly utilized
by our system because we use actual drum sticks that are augmented by
the addition of a small antenna.

Musicians are specifically trained to produce repeatable actions in
order to create sounds.  By using a natural interface for
percussionists, that of a drum stick, we leverage the large amounts of
training that these musicians undergo.

\bibliographystyle{IEEEtranS}
\bibliography{icmc2011gesturesgtzan} 

\end{document}
% Template: LaTeX file for ICMC 2011 papers, with hyper-references
%
% derived from the DAFx-06 templates
% derived from the ICMC 2009 templates by Steve Beck
% and then derived from the ICMC 2010 template
% 1) Please compile using latex or pdflatex.
% 2) Please use figures in vectorial format (.pdf); .png or .jpg are working otherwise 
% 3) Please use the "papertitle" and "pdfauthor" commands defined below

%------------------------------------------------------------------------------------------
\documentclass[twoside,10pt,a4paper]{article}
\usepackage{icmc2011,amssymb,amsmath} 
%\setcounter{page}{1}

\usepackage{mathptmx} 

%____________________________________________________________
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
%==== set the title ====
\def\papertitle{Controlling Real Time Sound Spatialization using the Radio Drum}
%\def\papertitle{}	%-- should be empty for the submission anyway!

%==== 1st submission: author name and affiliation are empty for anonymous submission ====
\def\paperauthorA{} 
\affiliation{}{}


%==== final submission: author name and affiliation ====
%---- uncomment 1 to 4 lines, for 1 to 4 authors
\def\paperauthorA{Gabrielle Odowichuk}
\def\paperauthorB{Steven Ness}
\def\paperauthorC{George Tzanetakis}
\def\paperauthorD{Peter Driessen}

%%---- set correspnding affiliation data for...
%%-- 1 author
%\affiliation{\paperauthorA}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%%-- 2 authors with same affiliation
%\affiliation{\paperauthorA, \paperauthorB, \paperauthorc, \paperauthord}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%-- 2 authors with different affiliations
%\twoaffiliations{\paperauthorA}{School\\ Department}
%  {\paperauthorB}{Company\\ Address}

%%-- 3 authors with different affiliations
%\threeaffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}

%%-- 4 authors with different affiliations
%\fouraffiliations{\paperauthorA}{University of Victoria \\ Department of Electrical and Computer %Engineering}
%  {\paperauthorB}{University of Victoria \\ Department of Computer Science}
%  {\paperauthorC}{University of Victoria \\ Department of Electrical and Computer Engineering}
%  {\paperauthorD}{University of Victoria \\ Department of Computer Science}



%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ user defined variables  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%%-- if using .ps or .eps figure files, they will be converted on the fly
%%-- RMK: for faster LaTeX runs, use it only once after adding new \includegraphics[]{} cmds
%\usepackage{epstopdf}	 

%---- the hyperref package must be last to properly work
\usepackage[pdftex,
       pdftitle={\papertitle},
	pdfauthor={\paperauthorA},
	colorlinks=false,bookmarksnumbered,pdfstartview=XYZ]{hyperref}
%\pdfcompresslevel=9
\usepackage[pdftex]{graphicx}	% for compatible graphics with hyperref
\usepackage[figure,table]{hypcap}	% corrects the hyper-anchor of figures/tables

%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ PAPER OUTLINE  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%- abstract
%- introduction
%- background
%	- radiodrum
%	- localization (amplitude and time) human perception
%	- 
%	
%	- previous works
%		- how do audio engineers actually do this right now
%			- soft knob
%			- joystick
%			- sliders
%		- panning only
%		
%--------------------------------------------------------------------
%- control
%	- intuitive
%	- contrast with other interfaces
%- visualization
%	- reading/recording the data as positions 
%	- editing that data in a 3D program
%	- visualize positions/reflections with openFrameworks real-time 
%- sound rendering (spatialization)
%
%- implementation
%	- marsyas
%		- data flow architecture
%		- implicit patching
%	- openFrameworks
%		- rapid prototyiping
%		- c++
%		- openGL 3D graphics
%		- marsyas module
%	- online and offline creation and editing of gestures
%   - speaker setup
%	
%- my algorithm
%	- inputs : x,y,z positions and audio file
%	- size of inner and outer rooms 
%	- delay lines
%	- calculating early reflections
%		- snipit of code
%		- figure
%		- text
		



\title{\papertitle}

%------------------------------------------------------------------------------------------
\begin{document}

\DeclareGraphicsExtensions{.png,.jpg,.pdf} % used graphic file format for pdflatex
    
\maketitle

\begin{abstract}


We present a real-time, multi-channel rendering system with the
radiodrum controller as an intuitive, hands-on interface to a 3D
spatialization system.  The radiodrum is a virtual controller and
interface that allows for very rapid tracking of two augmented drum
sticks in 3D space.  We have combined this interface with a real time
3D visualization system that allows a user to control the position of
sound sources using an array of loudspeakers, typically setup in a
cube model inside a rectangular room.  To generate the auditory scene,
we use a model that is a combination of an image source model and a
model developed by Moore in which virtual sounds are positioned and
moved using a combination of delay lines.


\end{abstract}

\section{Introduction}\label{sec:introduction}

Audio-scene rendering is an important aspect in the creation of
realistic auditory environments.  Sound spatialization has many
applications, a prominent example being the film and video game
industry.  As these industries strive towards a fully immersive visual
experience, these experiences will also require immersive audio.
Ideally, we should be able to control aspects such as the strength,
distance, and apparent motion of each sound source in the auditory
scene.  It is also important to properly render the reaction of the
virtual space to a sound with the use of early reflections and
reverberation.  Much work has been done in the area of high-quality
offline sound rendering, as well as lower quality real-time rendering
\cite{Jot95}.  As the computational power of computers increase, it is
possible to create high-quality scenes in real-time.  While the
quality and efficiency of spatialization have been studied at great
length, the control of the system has not been the focus.  We present
here a user control system for virtually rendered sounds, including a
gestural control for the location of sounds, and a graphical user
interface (GUI) to control other parameters and receive visual
feedback.

\section{Background}\label{sec:Background}

\subsection{Previous Work}\label{sec:Previous Work}


There has been a great deal of work done on accurately recreating a
sound space, using many different techniques.  Positioning of a sound
source is done in \cite{Pulkki08} using vector-based amplitude
panning, and in \cite{Braasch05} using virtual microphones. The
perceived direction in \cite{seo07} is controlled by amplitude panning
the direct sound, and perceived distance is controlled by adjusting
the energy decay curve of reverberation and gain of the direct
sound. Scene description models and rendering engine for interactive
virtual acoustics are outlined in \cite{Jot95}. The model includes
both a physical and perceptual approach to the environmental model,
\cite{Hannemann08} introduces a new algorithm to render virtual sound
sources with speakers using the Method of Moments and Singular Value
Decomposition. Li et al. \cite{li06} use the reverberation tails of
measured room impulse responses in addition to the direct path and
early reflections obtained by ray tracing.

Currently, audio engineers use panning controls such as sliders, soft
knobs, and joysticks.  When using sliders, for example, the level of
each speaker must be controlled individually.  When using joysticks,
only two of three dimensions can be controlled simultaneously.  Often,
the process requires many iterations in which automations are adjusted
to produce the desired effect.  Part of the need for these iterations
may be due to the fact that the position of these sounds are not
controlled by a device that is capable of positioning objects in three
dimensions.  These systems also often rely only on panning and not
delay lines.

The development of methods for gesture control of sound spatialization
is presented in \cite{Marshall09}.  Three levels of parameters are
considered: sound source location/orientation, sound source
characteristics (size, directivity, presence/ distance, brilliance/
warmth) and environmental/ room parameters.  3 specific roles for
control of spatialization are identified, the first of which is
directly relevant to our work with the radiodrum: a Spatial Performer
that performs with sound objects in space by moving sound sources in
real-time using gesture. The radiodrum control area is a miniature
model version of the space.  The second role of Spatial Conductor
where the user directly controls large-scale (room and environment)
parameters of the spatialization system using gesture is also
interesting.  The implementations in \cite{Marshall09} are done using
instrumented datagloves, using the radiodrum with its intuitive direct
connection between stick position and sound source position is to the
best of our knowledge a novel idea.

\subsection{Localization}\label{sec:Localization}

When localizing sounds, human perception relies on binaural
differences in amplitude and time.  At high frequencies, the relative
loudness of a sound is the dominant factor in localization.  At lower
frequencies, the difference in time of arrival between our ears
becomes dominant.  This shift in dominant cues is due to the
wavelength and diffraction of a sound.  Lower frequency sounds
diffract around barriers, and are therefore more likely to sound with
nearly equal intensity at both ears.  Sounds above approximately 3kHz
have a wavelength smaller than the distance between our ears, and any
sensed difference in phase is indeterminate.  So, diffraction makes
intensity cues ineffective, and sound with a short wavelength makes
time cues ineffective.  It is therefore important to use both time and
loudness cues to properly model a sound space.

Most of our ability to localize a sound source occurs on a horizontal
plane.  However, the pinnae in the human outer ear allows us to
distinguish sounds coming from above and below.  Depending on the
number of speakers used to recreate a sound scene, it is possible for
auditory illusions to occur, where one or more sounds can be
localized improperly.  Sound spatialization systems strive to simulate
sound above and below the listener, as well as from side to side
accurately.

\subsection{Image Method}\label{sec:Image Method}

The image method for calculating reflections in a room was originally
proposed at Bell Laboratories in 1978 \cite{allen79}.  This
time-domain model involves calculating the position of images in a
space, rather than calculating all modes of a sound within a given
frequency range. The reflected sound paths are created as images
outside the boundaries of the room by reflecting the original
sound across each wall.  This method allows reflections within a room
to be quickly simulated for a rectangular room, and can easily be
expanded for a 3 dimensional rectangular box.  Of course, this is a
simplified model, and involves many assumptions.  For example, the
room must be rectangular, and any wall absorption must be
angle-independent.

\subsection{Moore's Model}\label{Moore's Model}

Moore proposed a model using the image method to calculate the time
and amplitude of each reflection for a set of surround speakers, and
is presented as a room within a room \cite{Moore83}.  This model
treats each speaker as a window into a room that exists beyond the
boundaries of the listening space.  The distance of the original
source and the reflections to each speaker are calculated, treating
each speaker as a listening point.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.5\columnwidth]{RoomWithinARoom}}
\caption{Room within a room model}
\label{fig:RoomWithinARoom}
\end{figure}

In implementing Moore's model, we expanded the model for rooms as
rectangular boxes, with speakers at each of the 8 corners of the inner
room.  Moore's model is improved by treating each wall of the inner
room as opaque. As seen in figure \ref{fig:RoomWithinARoom}, the sound
must be "seen" by a speaker to sound.

\subsection{Radiodrum}\label{sec:Radiodrum}

The radiodrum is a gestural control system created at Bell Laboratories
in 1989 \cite{schloss89}.  It was originally meant to be a replacement
for the mouse as a control for computers.  Instead, the radiodrum is
mostly used as a virtual musical instrument, played in live concert
settings.

This virtual instrument has two sticks, each with a metallic coil at
the tip driven by an electric RF signal.  The x,y and z position of
these sticks is determined by the point of greatest capacitance with
the surface below.  The radiodrum then sends position information over
MIDI.  The end product is a tool that reports the positions of two
points, the tips of the drum sticks, in 3 dimensions.  Recent
improvements to the radiodrum have increased the instruments gestural
control and accuracy.  One improvement is that the radiodrum has 8 audio 
outputs with the x,y,z,dz waveforms for each stick sampled at 44100 Hz 
instead of MIDI.


\subsection{Implementation}\label{sec:Implementation}

MARSYAS (Music Analysis, Retrieval and SYnthesis for Audio Signals)
\cite{Marsyas}, is an open source audio processing framework with
specific emphasis on building MIR systems. It has been under
development since 1998 and has been used for a variety of projects
both in academia and industry.

openFrameworks in an open source toolkit written in c++, with many
similarities to it's precursor, the Processing development
environment.  This toolkit is mainly used by artists and creative
programmers.  It's simple and intuitive framework and cross platform
capabilities allow for rapid development and can easily incorporate
other libraries.  In this project, openFrameworks communicates with
MARSYAS to preform the audio processing, and openGL to create a GUI
capable of visualization and user control.

\section{Implementation}\label{sec:Implementation}

The implementation of this model can be separated into four
components: gesture capturing, audio processing, virtual source and
delay line calculations, and graphics processing.  Figure
\ref{fig:ImplementationFlowChart} shows an implementation flow chart
of the system.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.9\columnwidth]{ImplementationFlowChart}}
\caption{Implementation Flow Chart}
\label{fig:ImplementationFlowChart}
\end{figure}

\subsection{Gesture Capturing}\label{sec:Gesture Capturing}

Our system captures the intended sound source position in real-time
with the radiodrum, and displays the data graphically for additional
feedback.  Gestures created with with the radiodrum sticks can be
monitored visually through the GUI and aurally from the surround
speaker setup.  In contrast with other systems used to control the
position of a sound, the radiodrum is intuitive, because the stick can
be moved freely in three-dimensions.

\subsection{Audio Processing}\label{sec:Audio Processing}

All audio processing for this system is done using Marsyas.  Our
system includes modules that involve reading audio from a sound file
source, computing delay lines, gains, filtering, and outputting
multi-channel audio in real-time.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.4\columnwidth]{CircularDelayLine}}
\caption{Delay Line Implementation}
\label{fig:example}
\end{figure}

A set of delay lines are used to simulate early reflections, and each
delay line has a corresponding gain.  In Marsyas, a module
implementing delay lines was created specifically for this
application.  The delay lines are implemented as a rotating read-heads
on a circular buffer.  The first pointer writes to the buffer with
updated data.  The following pointers read from the buffer at set
delay points. Linear interpolation is used to smooth between frames of
data as the delay lines vary, which occurs every time the sound source
placement changes.  After the delay lines, a simple FIR filter is used to add reverberation.

\subsection{Virtual source and delay line calculations}\label{sec:Virtual source and delay line calculations}

The calculations required to create proper delay lines involve
determining the position of the sound source and each virtual source,
and calculating the corresponding delays and gains for each source.
The virtual position of early reflections were calculated using the
image method, and we calculate the distance to each reflection, and
the corresponding time delay and gain.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.9\columnwidth]{ImageSource2}}
\caption{Image Source Model}
\label{fig:example}
\end{figure}

The early reflections were calculated using the image method.  For
simplicity, the implementation d description begins with only two
dimensions.  A set of imaginary rooms surrounding the center room are
given indices $[i,j]$, $-N<i<N$ and $-N<j<N$, where N is the order of
reflections.  The order in each square is given by
\begin{equation}
N = abs(i) + abs(j)
\label{eq1}
\end{equation}

The images then fall inside each imaginary room at a position of
$[i*p_x,j*p_y]$.  The final step is to account for the displacement
seen in every second block.  For example, along the x-axis, the
positions of each image are either equal to $i*p_x$, or they fall
within that block with a constant displacement.  We define these
displacements as:
\begin{equation}
	disp_x = 2(w/2 - p_x)	
\label{eq2}	
\end{equation}	
\begin{equation}
	disp_y = 2(h/2 - p_y)
\label{eq3}	
\end{equation}	
	

Expanding this algorithm into 3 dimensions, the calculation of each
reflection is done by iterating through the $N^3$ rooms, and
calculating the order and position of each reflection. Once we have
the position of each reflection, we determine if each reflection is
"seen" by the speaker. We then calculate the distance from each
virtual image to each speaker using the distance formula.
\begin{equation}
d = \sqrt{(p_x - s_x)^2 + (p_y - s_y)^2 + (p_z - s_z)^2}
\label{eq4}
\end{equation}
 
The delays and gains are relative to the calculated distances.
\begin{equation}
g = 100000/(4*pi*r^2)
\label{eq5}
\end{equation}
\begin{equation}
t = d/c * fs
\label{eq6}
\end{equation} 
 
The calculated delays and gains are used to update the MARSYAS delay lines.

\subsection{Visualization}\label{sec:Visualization}

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.5\columnwidth]{openFrameworksDirectPath}}
\caption{OpenFrameworks Screenshot}
\label{fig:openFrameworksDirectPath}
\end{figure}

Using openGL to create 3D graphics, a model of the inner and outer
rooms, as well as the position of the sound within the space.  The
visualization shows the size of the outer room, the position of the
speakers in the inner room, and the position of our sound source.
When controlling the position of our sound source, the visualization
allows the user to see where the sound in moving with respect to the
size of the inner and outer room.  Without the visualization as
additional feedback, it is difficult to conceptualize the scale of the
inner and outer rooms.

For testing and debugging, we can add the images and reflected rays to
the visualization, and separately display an impulse responses for
each speaker showing the delay and amplitude of the reflections from
source to speaker.  

\section{Conclusions}\label{Conclusions}

In this paper we presented a novel sound spatialization system using
the radiodrum for gestural control of the movement of sounds within a
space.  The intuitive control and graphical interface distinguish this
system from similar spatialization models.  Future work will include
user studies for gestural control comparisons.  

Videos of this system in action can be viewed at \\
http://spatial.sness.net.


\bibliographystyle{IEEEtranS}
\bibliography{icmc2011spatialgtzan}

\end{document}
% Template: LaTeX file for ICMC 2011 papers, with hyper-references
%
% derived from the DAFx-06 templates
% derived from the ICMC 2009 templates by Steve Beck
% and then derived from the ICMC 2010 template
% 1) Please compile using latex or pdflatex.
% 2) Please use figures in vectorial format (.pdf); .png or .jpg are working otherwise 
% 3) Please use the "papertitle" and "pdfauthor" commands defined below

%------------------------------------------------------------------------------------------
\documentclass[twoside,10pt,a4paper]{article}
\usepackage{icmc2011,amssymb,amsmath} 
%\setcounter{page}{1}

\usepackage{mathptmx} 

%____________________________________________________________
%  !  !  !  !  !  !  !  !  !  !  !  ! user defined variables  !  !  !  !  !  !  !  !  !  !  !  !  !  !
%==== set the title ====
\def\papertitle{Paper Template for ICMC 2011}
%\def\papertitle{}	%-- should be empty for the submission anyway!

%==== 1st submission: author name and affiliation are empty for anonymous submission ====
\def\paperauthorA{} 
\affiliation{}{}


%==== final submission: author name and affiliation ====
%---- uncomment 1 to 4 lines, for 1 to 4 authors
\def\paperauthorA{Gabrielle Odowichuk}
\def\paperauthorB{Steven Ness}
\def\paperauthorC{George Tzanetakis}
\def\paperauthorD{Peter Driessen}

%%---- set correspnding affiliation data for...
%%-- 1 author
%\affiliation{\paperauthorA}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%%-- 2 authors with same affiliation
%\affiliation{\paperauthorA, \paperauthorB, \paperauthorc, \paperauthord}
%  {School\\ Department, City, Country \\ {\tt \href{mailto:email@domain.icmc}{email@domain.icmc}}}

%-- 2 authors with different affiliations
%\twoaffiliations{\paperauthorA}{School\\ Department}
%  {\paperauthorB}{Company\\ Address}

%%-- 3 authors with different affiliations
%\threeaffiliations{\paperauthorA}{School A\\ Department X}
%  {\paperauthorB}{Company\\ Address}
%  {\paperauthorC}{School B\\ Department Y}

%%-- 4 authors with different affiliations
%\fouraffiliations{\paperauthorA}{University of Victoria \\ Department of Electrical and Computer %Engineering}
%  {\paperauthorB}{University of Victoria \\ Department of Computer Science}
%  {\paperauthorC}{University of Victoria \\ Department of Electrical and Computer Engineering}
%  {\paperauthorD}{University of Victoria \\ Department of Computer Science}



%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ user defined variables  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%%-- if using .ps or .eps figure files, they will be converted on the fly
%%-- RMK: for faster LaTeX runs, use it only once after adding new \includegraphics[]{} cmds
%\usepackage{epstopdf}	 

%---- the hyperref package must be last to properly work
\usepackage[pdftex,
       pdftitle={\papertitle},
	pdfauthor={\paperauthorA},
	colorlinks=false,bookmarksnumbered,pdfstartview=XYZ]{hyperref}
%\pdfcompresslevel=9
\usepackage[pdftex]{graphicx}	% for compatible graphics with hyperref
\usepackage[figure,table]{hypcap}	% corrects the hyper-anchor of figures/tables

%  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ PAPER OUTLINE  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ 
%------------------------------------------------------------------------------------------

%- abstract
%- introduction
%- background
%	- radiodrum
%	- localization (amplitude and time) human perception
%	- 
%	
%	- previous works
%		- how do audio engineers actually do this right now
%			- soft knob
%			- joystick
%			- sliders
%		- panning only
%		
%--------------------------------------------------------------------
%- control
%	- intuitive
%	- contrast with other interfaces
%- visualization
%	- reading/recording the data as positions 
%	- editing that data in a 3D program
%	- visualize positions/reflections with openFrameworks real-time 
%- sound rendering (spatialization)
%
%- implementation
%	- marsyas
%		- data flow architecture
%		- implicit patching
%	- openFrameworks
%		- rapid prototyiping
%		- c++
%		- openGL 3D graphics
%		- marsyas module
%	- online and offline creation and editing of gestures
%   - speaker setup
%	
%- my algorithm
%	- inputs : x,y,z positions and audio file
%	- size of inner and outer rooms 
%	- delay lines
%	- calculating early reflections
%		- snipit of code
%		- figure
%		- text
		



\title{\papertitle}

%------------------------------------------------------------------------------------------
\begin{document}

\DeclareGraphicsExtensions{.png,.jpg,.pdf} % used graphic file format for pdflatex
    
\maketitle

\begin{abstract}



\end{abstract}



\section{Introduction}\label{sec:introduction}

Audio-scene rendering is an important aspect in the creation of
realistic auditory environments.  Sound spatialization has many
applications, a prominent example being the film and video game
industry.  As these industries strive towards a fully immersive visual
experience, these experiences will also require immersive audio.
Ideally, we should be able to control aspects such as the strength,
distance, and apparent motion of each sound source in the auditory
scene.  It is also important to properly render the reaction of the
virtual space to a sound with the use of early reflections and
reverberation.  Much work has been done in the area of high-quality
offline sound rendering, as well as lower quality real-time rendering
\cite{Jot95}.  As the computational power of computers increase, it is
possible to create high-quality scenes in real-time.  While the
quality and efficiency of spatialization have been studied at great
length, the control of the system has not been the focus.  We present
here a user control system for virtually rendered sounds, including a
gestural control for the location of sounds, and a graphical user
interface (GUI) to control other parameters and receive visual
feedback.


 
\section{Background}\label{sec:Background}

\subsection{Previous Works}\label{sec:Previous Works}


There has been a great deal of work done on accurately recreating a
sound space, using many different techniques.  Positioning of a sound
source is done in \cite{Pulkki08} using vector-based amplitude
panning, and in \cite{Braasch05} using virtual microphones. The
perceived direction in \cite{seo07} is controlled by amplitude panning
the direct sound, and perceived distance is controlled by adjusting
the energy decay curve of reverberation and gain of the direct
sound. Scene description models and rendering engine for interactive
virtual acoustics are outlined in \cite{Jot95}. The model includes
both a physical and perceptual approach to the environmental model,
with details from the MPEG-4 standard in
\cite{Vaananen04}. \cite{Hannemann08} introduces a new algorithm to
render virtual sound sources with speakers using the Method of Moments
and Singular Value Decomposition. Li et al. \cite{li06} use the
reverberation tails of measured room impulse responses in addition to
the direct path and early reflections obtained by ray tracing.

Currently, audio engineers use panning controls such as sliders, soft
knobs, and joysticks.  When using sliders, for example, the level of
each speaker must be controlled individually.  When using joysticks,
only two of three dimensions can be controlled simultaneously.  Often,
the process requires many iterations in which automations are adjusted
to produce the desired effect.  Part of the need for these iterations
may be due to the fact that the position of these sounds are not
controlled by a device that is capable of positioning objects in three
dimensions.  These systems also often rely only on panning and not
delay lines.


The development of methods for gesture control of sound spatialization
is presented in \cite{Marshall09}.  Three levels of parameters are
considered: sound source location/orientation, sound source
characteristics (size, directivity, presence/ distance, brilliance/
warmth) and environmental/ room parameters.  3 specific roles for
control of spatialization are identified, the first of which is
directly relevant to our work with the radiodrum: a Spatial Performer
- performs with sound objects in space by moving sound sources in
real-time using gesture. The radiodrum control area is a miniature
model version of the space.  The second role of Spatial Conductor -
where the user directly controls large-scale (room and environment)
parameters of the spatialization system using gesture is also
interesting.  The implementations in \cite{Marshall09} are done using
instrumented datagloves, using the radiodrum with its intuitive
direct connection between stick position and sound source position is
appears to be entirely novel.


\subsection{Localization}\label{sec:Localization}

When localizing sounds, human perception relies on binaural
differences in amplitude and time.  At high frequencies, the relative
loudness of a sound is the dominant factor in localization.  At lower
frequencies, the difference in time of arrival between our ears
becomes dominant.  This shift in dominant cues is due to the
wavelength and diffraction of a sound.  Lower frequency sounds
diffract around barriers, and are therefore more likely to sound with
nearly equal intensity at both ears.  Sounds above approximately 3kHz
have a wavelength smaller than the distance between our ears, and any
sensed difference in phase is indeterminate.  So, diffraction makes
intensity cues ineffective, and sound with a short wavelength makes
time cues ineffective.  It is therefore important to use both time and
loudness cues to properly model a sound space.

Most of our ability to localize a sound source occurs on a horizontal
plane.  However, the pinnae in the human outer ear allows us to
distinguish sounds coming from above and below.  Depending on the
number of speakers used to recreate a sound scene, it is possible for
auditory illusions to occur, where one or more sounds can be
localized improperly.  Sound spatialization systems strive to simulate
sound above and below the listener, as well as from side to side
accurately.

\subsection{Image Method}\label{sec:Image Method}

An image method for calculating reflections in a room was originally
proposed at Bell Laboratories in 1978 \cite{allen79}.  This
time-domain model involves calculating the position of images in a
space, rather than calculating all modes of a sound within a given
frequency range. The reflected sound paths are created as images
outside the boundaries of the real room by reflecting the original
sound across each wall.  This method allows reflections within a room
to be quickly simulated for a rectangular room, and can easily be
expanded for a 3 dimensional rectangular box.  Of course, this is a
simplified model, and involved many assumptions.  For example, the
room must be rectangular, and any wall absorption must be
angle-independent.

\subsection{Moore's Model}\label{Moore's Model}

Moore proposed a model using the image method to calculate the time
and amplitude of each reflection for a set of surround speakers, and
is presented as a room within a room model.  It treats each speaker as
a window into a room that exists beyond the boundaries of the
listening space.  The distance of the original source and the
reflections to each speaker are calculated, and each speaker is
treated as a listening point.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.6\columnwidth]{RoomWithinARoom}}
\caption{Room within a room model}
\label{fig:RoomWithinARoom}
\end{figure}

In implementing Moore's model, we expanded the model for rooms as
rectangular boxes, with speakers at each of the 8 corners of the inner
room.  Moore's model is improved by treating each wall of the inner
room as opaque. As seen in figure \ref{fig:RoomWithinARoom}, the sound
must be "seen" by a speaker to sound.

\subsection{radiodrum}\label{sec:radiodrum}

The radiodrum is a gestural control system create at Bell Laboratories
in 1989 \cite{schloss89}.  It was originally meant to be a replacement
for the mouse as a control for computers.  Instead, the radiodrum is
mostly used as a virtual musical instrument, played in live concert
settings.

This virtual instrument has two sticks, each with a metallic coil at
the tip driven by an electric RF signal.  The x,y and z position of
these sticks is determined by the point of greatest capacitance with
the surface below.  The radiodrum then sends position information over
MIDI.  The end product is a tool that reports the positions of two
points, the tips of the drum sticks, in 3 dimensions.  Recent
improvements to the radiodrum have increased the instruments gestural
control and accuracy.

\subsection{Marsyas}\label{sec:Marsyas}

MARSYAS (Music Analysis, Retrieval and SYnthesis for Audio Signals)
\cite{Marsyas}, is an open source audio processing framework with
specific emphasis on building MIR systems. It has been under
development since 1998 and has been used for a variety of projects
both in academia and industry. The guiding principle behind the design
of MARSYAS has always been to provide a flexible, expressive and
extensive framework without sacrificing computational efficiency.

The idea of dataflow programming has been fundamental in the design of
MARSYAS.  Dataflow programming has a long history. The original (and
still valid) motivation for research into dataflow architectures was
to take advantage of parallelism.  Motivated by criticisms of the
classical von Neumann hardware architecture dataflow architectures for
hardware were proposed as an alternative in the 1970s and
1980s. During the 1990s there was a new direction of growth in the
field of dataflow visual programming languages that were domain
specific. In such visual languages programming is done by connecting
processing objects with wires to create patches.  Successful examples
include Labview (\href{http://www.ni.com/labview/}), SimuLink
(\href{http://www.mathworks/com/products/simulink/}) and in the field
of Computer Music Max/MSP
(\href{http://www.cycling74.com/products/max5}) and Pure Data
(\href{http://puredata.info/Puckette, 2002}).

\subsection{openFrameworks}\label{sec:openFrameworks}

openFrameworks in an open source toolkit written in c++, with many
similarities to it's precursor, the Processing development
environment.  This toolkit is mainly used by artists and creative
programmers.  It's simple and intuitive framework and cross platform
capabilities allow for rapid development and can easily incorporate
other libraries.  In this project, openFrameworks communicates with
MARSYAS to preform the audio processing, and openGL to create a GUI
capable of visualization and user control.

\section{Implementation}\label{sec:Implementation}

The implementation of this model can be separated into three
components: audio processing, calculations, and graphics processing.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.9\columnwidth]{ImplementationFlowChart}}
\caption{Implementation Flow Chart}
\label{fig:example}
\end{figure}

\subsection{Audio Processing}\label{sec:Audio Processing}

All audio processing for this system is done using Marsyas.  Our
system includes modules that involve reading audio from a sound file
source, computing delay lines, gains, filtering, and outputting
multi-channel audio in real-time.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.5\columnwidth]{CircularDelayLine}}
\caption{Delay Line Implementation}
\label{fig:example}
\end{figure}

A set of delay lines are used to simulate early reflections, and each
delay line has a corresponding gain.  In Marsyas, a module
implementing delay lines was created specifically for this
application.  The delay lines are implemented as a rotating read-heads
on a circular buffer.  The first pointer writes to the buffer with
updated data.  The following pointers read from the buffer at set
delay points. Linear interpolation is used to smooth between frames of
data as the delay lines vary, which occurs every time the sound source
placement changes.

\subsection{Calculations}\label{sec:Calculations}

The calculations required to create proper delay lines involve
determining the position of the sound source and each virtual source,
and calculating the corresponding delays and gains for each source.
The virtual position of early reflections were calculated using the
image method, and we calculate the distance to each reflection, and
the corresponding time delay and gain.

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.9\columnwidth]{ImageSource2}}
\caption{Image Source Model}
\label{fig:example}
\end{figure}

The early reflections were calculated using the image method.  For
simplicity, the implementation d description begins with only two
dimensions.  A set of imaginary rooms surrounding the center room are
given indices $[i,j]$, $-N<i<N$ and $-N<j<N$, where N is the order of
reflections.  The order in each square is given by
     
\begin{equation}
N = abs(i) + abs(j)
\label{eq1}
\end{equation}

The images then fall inside each imaginary room at a position of
$[i*p_x,j*p_y]$.  The final step is to account for the displacement
seen in every second block.  For example, along the x-axis, the
positions of each image are either equal to $i*p_x$, or they fall
within that block with a constant displacement.  We define these
displacements as:

\begin{equation}
	disp_x = 2(w/2 - p_x)	
\label{eq2}	
\end{equation}	
\begin{equation}
	disp_y = 2(h/2 - p_y)
\label{eq3}	
\end{equation}	
	

Expanding this algorithm into 3 dimensions, the calculation of each
reflection is done by iterating through the $N^3$ rooms, and
calculating the order and position of each reflection. Once we have
the position of each reflection, we determine if each reflection is
"seen" by the speaker. We then calculate the distance from each
virtual image to each speaker using the distance formula.

\begin{equation}
d = \sqrt{(p_x - s_x)^2 + (p_y - s_y)^2 + (p_z - s_z)^2}
\label{eq4}
\end{equation}
 
The delays and gains are relative to the calculated distances.

\begin{equation}
Gain = W/(4*pi*r^2)
\label{eq5}
\end{equation}
\begin{equation}
t = d/c * fs
\label{eq6}
\end{equation} 
 
The calculated delays and gains are used to update the MARSYAS delay lines.

\subsection{Visualization}\label{sec:Visualization}

\begin{figure}[htbp]
\centerline{
	\includegraphics[width=0.9\columnwidth]{openFrameworksDirectPath}}
\caption{OpenFrameworks Screenshot}
\label{fig:openFrameworksDirectPath}
\end{figure}

Using openGL to create 3D graphics, a model of the inner and outer
rooms, as well as the position of the sound within the space.  The
visualization shows the size of the outer room, the position of the
speakers in the inner room, and the position of our sound source.
When controlling the position of our sound source, the visualization
allows the user to see where the sound in moving with respect to the
size of the inner and outer room.  Without the visualization as
additional feedback, it is difficult to conceptualize the scale of the
inner and outer rooms.
  
\subsection{Gesture Creation and Editing}\label{sec:Gesture Creation and Editing}

Our system captures the intended sound source position in real-time
with the radiodrum, and displays the data graphically for additional
feedback.  Gestures created with with the radiodrum sticks can be
monitored visually through the GUI and aurally from the surround
speaker setup.  In contrast with other systems used to control the
position of a sound, the radiodrum is intuitive, because the stick can
be moved freely in three-dimensions.

These gestures are captured and recorded as varying x,y,z positions
over time.


\section{Conclusions}\label{Conclusions}

In this paper we presented a novel sound spatialization system using
the radiodrum for gestural control of the movement of sounds within a
space.  The system

\bibliographystyle{IEEEtranS}
\bibliography{icmc2011spatialgtzan}

\end{document}
% Template for ICME-2011 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{subfigure}
\usepackage{url} 
\def\ICMEPaperID{806} % *** Enter the ICME Paper ID here

\pagestyle{empty}


\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{Strategies for Orca call retrieval \\ to
  support annotation of a large archive}
%
% Single address.
% ---------------
\name{Anonymous ICME submission}
\address{Paper ID \ICMEPaperID}
% Steven Ness, University of Victoria
% Alex Lerch - zplane.development
% George Tzanetakis, University of Victoria

\maketitle


%
\begin{abstract}

  The Orchive is a large audio archive of hydrophone recordings of
  Killer whale (\textit{Orcinus orca}) vocalizations. Researchers and users
  from around the world can interact with the archive using a
  collaborative web-based annotation, visualization and retrieval
  interface. In this paper we describe and compare different
  strategies for the retrieval of discrete Orca calls. In addition,
  the results of the automatic analysis are integrated in the user
  interface facilitating annotation as well as leveraging the existing
  annotations for supervised learning.  The best strategy achieves a
  mean average precision of 0.77 with the first retrieved item being 
  relevant $95\%$ of the time in a dataset of 185 calls belonging to 4
  types. 



\end{abstract}
%
\begin{keywords}
content-based retrieval, dynamic time warping, bioacoustics 
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

In recent years there has been increasing research activity in the
areas of multimedia learning and information retrieval.  Most of it
has been in traditional domains, such as sports video, news video, and
natural images \cite{hauptman97}.  There is broad interest in these
domains and in most cases there are clearly defined objectives such as
highlights in sports videos, explosions in news video or sunsets in
natural images. Some of the important research trends in multimedia
retrieval research have been the use of large collections for
supervised learning, the integration of the user interface and the
annotation/retrieval system, and the shift from single user system to
collaborative web-based interfaces.

Our focus in this paper is applying similar ideas to the Orchive
\cite{tzanetakis07}, a large archive of audio recordings of killer whale (\textit{Orcinus orca})
 vocalizations from the Northern resident community of British
Columbia. It has been shown that different killer whale communities
use distinct vocal signals \cite{ford00}. Pods are stable kin
groups and have unique vocal repertoires consisting of 7-17
distinct calls. Related pods often use structurally distinct versions 
of the same class type. 

Currently the Orchive contains approximately 10000 hours of audio data
digitized from the original analog cassettes with a projected total
size of 20000 hours (it would take approximately 6 years listening 8
hours every day to cover the entire archive). Traditionally
researchers had to digitize the analog cassettes and process the
resulting files individually on their computers. This tedious process
has inhibited researchers from analyzing the amounts of data that are available. Harnessing the
large amounts of data provided in the archive holds enormous potential
in advancing our understanding of how these animals
communicate. However, in order for the data to be effectively analyzed
it needs to be annotated and automatic retrieval tools need to be
developed. We have developed a collaborative web-based interface that
is enhanced with retrieval and classification capabilities which are
used to support the annotation process.

Most of existing work in the automatic analysis for Orca calls has
focused either on the automatic detection of calls either in real-time
\cite{Luke2010771} or offline \cite{ness08} or on their classification
\cite{brown07_orca_dtw}. In contrast our focus is on similarity-based
retrieval. There are several reasons why retrieval is more important
than classification in our situation. Retrieval provides more fine
grained information than classification and supports the study of
variation within a particular call type. In addition it can deal with
calls of an unknown type or with classes that have a very small number
of examples. Users of our interface fall into two categories: experts
and volunteers. In many cases volunteers might not be able to classify
a call type by listening to it but can easily identify to which call
it is more similar from a limited set of examples. That way the call
can be indirectly classified. Similarity retrieval can provide this
limited set of examples. There have been several strategies and
representations proposed in the literature for the classification
and retrieval of Orca calls, but to the best of our knowledge they have
mostly been evaluated on small amounts of data and have not been
compared directly on the same data using retrieval effectiveness
metrics.

The main contributions of this work are: 1) a collaborative web-based
interface that integrates automatic similarity retrieval to enhance
the annotation process 2) a description of different strategies for
the retrieval of Orca calls 3) an experimental evaluation of these
different strategies on a large dataset using established retrieval
effectiveness metrics and 4) a post-processing step for denoising 
and exact boundary identification for presentation of the Orca calls. 




\section{Related Work} 
\label{sec:related} 

The main motivation behind our work has been creating better
interfaces for interacting with the Orchive \cite{tzanetakis07} a
large archive of audio recordings of Orca vocalizations.  There are
stable resident populations of \textit{Orcinus orca} in the
northwest Pacific Ocean, and some of these populations \cite{ford00}]
are found near Hanson Island, off the north tip of Vancouver Island in
Canada. Orcalab is a research station that has been recording audio of
these Orca populations since 1972
\cite{deecke99_quantifying_orca}. They have amassed a huge archive of
more than 20,000 hours of audio recordings collected via a permanent
installation of underwater hydrophones.

Most of existing work in the automatic analysis of Orca calls has
focused on detection and classification rather than retrieval. A
real-time system with low computational requirements for the detection
of Orca vocalizations is described in \cite{Luke2010771}. Annotation
bootstrapping is a technique used to classify/segment hydrophone
recordings into three broad categories: voiceover, background, and
vocalizations \cite{ness08}. 

Our work was influenced by two publications that described different 
representations and methodologies for the classification of Orca
calls. Orca vocalizations consists of well-defined, discrete calls
with tonal signal components. They can be characterized by the pulse
rate contour of the goal which can be viewed as analogous to the pitch
contour of a speech or monophonic music signal. A method for computing 
acoustic similarity between pulse rate contours (normalized so that
they all have the same duration) using the discriminative error of a
Artificial Neural Network is described in
\cite{deecke99_quantifying_orca}. 

Dynamic time warping (DTW) is a technique for measuring the similarity
of two sequence that many vary in time. It mostly known in the context
of speech recognition \cite{sakoe78} but it has found applications in
many areas including video, motion and DNA sequence analysis. The use
of DTW to compute the similarity between two pulse rate contours in
the context of Orca calls has been explored in
\cite{brown07_orca_dtw}. In that work, a similarity matrix is
calculated containing all the DTW alignment costs between pairs of
pulse rate contours. This similarity matrix is then subsequently used to calculate clusters
which are then compared the ground truth call labeling to assess the
feasibility of call classification using this approach. 

In this paper we focus on retrieval rather than classification and
only use the ground truth labels as a way to measure retrieval
effectiveness. We compare different strategies over a large dataset
(185 calls, 4 classes) using well established retrieval effectiveness
measures. To the best of our knowledge this is the first systematic 
evaluation of these different design choices. 


\section{System Description}

\subsection{Contour Extraction and Retrieval Strategies}  

The discrete calls of killer whales are pulsed signals in which a tone
(of a certain tonal frequency) is not emitted continuously but in
pulses given by the pulse-repetition rate. Unlike the
tonal signals of many birds and other delphinids, the highest energy
is not always contained in the first or second harmonic
\cite{deecke99_quantifying_orca}. The high levels of background noise and variety of
recording conditions compound the difficulty of obtaining pulse rate
contours. The pulse rate contour is used as the primary representation
for Orca calls because it is more robust as compared to spectral features
to levels of background noise typical in field recordings.

We have compared three pitch extraction methods for obtaining the
pulse rate contour. The first method ({\bf PRAAT}) is based on time-domain
autocorrelation and is similar to the pitch extraction algorithm
implemented in Praat \cite{boersma93}. It is based on calculating the
time-domain autocorrelation of the signal: 
\begin{equation} 
R(\tau) = \frac{1}{N} \sum_{n=0}^{N-1-m} x[n] x[n+m] \;\;\;\; 0 \leq m
< M 
\end{equation} 

The peaks of the autocorrelation function correspond to the lags 
in which the signal is self-similar. The signal is processed in
windows and the autocorrelation of the windowed signal $R_{xw}$ is divided 
by the autocorrelation of the window $R_{w}$ providing better robustness to 
noise and better accuracy. 

\begin{equation}
R_{x}(\tau) = R_{xw}(\tau) / R_{w}(\tau)
\end{equation} 


The second method is based on the {\bf YIN} pitch extraction method.  The YIN method is 
based on the difference function which is similar to the
autocorrelation: 

\begin{equation}
d{t} = \sum_{n=0}^{N-1} (x[n] - x[n+\tau])^{2}
\end{equation} 

The dips in the difference function correspond to periodicities. 
In order to reduce the occurrence of subharmonic errors, YIN employs a
cumulative mean function which de-emphasizes higher period dips 
in the difference function. 

The third method ({\bf SACF}) is based on the multipitch detection algorithm
described by Tolonen and Karjalainen \cite{tolonen00}.  In this algorithm, the
signal is decomposed into two frequency bands (below and above 1000
Hz) and amplitude envelopes are extracted for each frequency band. The
envelope extraction is performed by applying half-wave rectification
and low-pass filtering.  The envelopes are summed and an enhanced
autocorrelation function is computed so that the effect of integer
multiples of the peak frequencies to multiple pitch detection is
reduced.


We explore three retrieval strategies/representations. Statistical
features characterizing the entire pulse rate contour are computed and
each call is characterized by a single vector of features. The
features are normalized by max-min normalization so that they range
from 0 to 1 over the entire dataset. Similarities are then computed by
taking the Euclidean distance in the normalized space. This strategy
is used as reasonable baseline. The features used in this work are the mean, median,
standard deviation, min and max of the pulse rate contour. The second
strategy consists of resampling the pulse rate contour using linear
interpolation to a fixed number of points. This strategy is similar to
the one used in Deecke \cite{deecke99_quantifying_orca}. Essentially
it assumes that the duration of the call does not play a major role in
its characterization and temporal scaling is applied uniformly across
the contour. The third strategy utilizes dynamic time warping to align
the pulse rate contours. The alignment cost is used to measure the
similarity between calls. The two sequences to be matched are arranged
on the sides of a grid. To find the best match between the sequences
we can find a path through the grid that minimizes the total distance
between them. More details can be found in \cite{sakoe78}.


\begin{figure}[htb]
\begin{center}
\includegraphics[width=85mm]{orchive}
\label{fig:orchive} 
\caption{
Orchive collaborative web-interface showing how annotation can support
retrieval.} 
\end{center} 
\end{figure} 



\subsection{Collaborative web-interface} 



Figure ~\ref{fig:orchive} shows a screen-shot of the collaborative
web-interface. The user has selected a region corresponding to call
call that needs to be annotated. Under the spectrogram a call catalog
is provided to help the user with annotation. The interface uses the
similarity retrieval described in this paper to suggest the most
likely call type to the user by showing it zoomed in the window to the
right. The call is then correctly annotated as of N4 type. The
interface is written in the Django web development framework
\footnote{\url{http://www.djangoproject.com/}} and interface directly
with the Marsyas open source audio analysis software
\footnote{\url{http://marsyas.info}} through Python bindings.



\begin{figure}[htb]
  \begin{center}
\subfigure[Before]{
\includegraphics[height=15mm]{A34_N10}
}
\subfigure[After]{
\includegraphics[height=15mm]{A34_N10_snip}
}
\label{fig:snip} 
\caption{Post-processing of Orca calls for presentation} 
\end{center} 
\end{figure} 





\subsection{Post-processing} 

The annotation process by users is not very accurate and frequently
includes noisy background parts of the signal.  A spectral peak
picking approach is used to identify the tonal components of the input
signal. These components, separated from the noise-like signal parts
are resynthesized to provide a more clear sounding call that still
retains its identifying characteristics.
	
In a first processing step, peaks are pre-selected as candidates for
tonal components by ensuring that a) their amplitude is the largest
local maximum within small blocks of neighboring frequency bins, b)
their frequency is in a pre-defined frequency range and c) their
amplitude is above a threshold that is computed relative to the
overall spectral maximum, the overall spectral energy and a smoothed
version of the magnitude spectrum.  Then, a ``tonal probability'' is
computed for every peak candidate and the candidates with the highest
probability are selected as tonal components. The probability is
computed as the arithmetic mean of a) the gaussian of the distance of
the peak's bin frequency and its instantaneous frequency, b) the
normalized logarithmic distance between the peak's amplitude and a
simultaneous masking threshold computed according to the
psycho-acoustic model in ITU recommendation BS.1387, and c) the
amplitude of a peak spectrogram smoothed in both time and frequency
domain so that sporadic peaks have low amplitude and ``steady'' peaks
have higher amplitude. This tonal probability is also used to remove the
parts of the annotation that contain background noise as shown in
Figure 2.





\section{Experimental Evaluation}

In order to systematically explore the different strategies for Orca
call retrieval we utilized a dataset consisting of 185 recordings of
vocalizations. They have been annotated using the Orchive
collaborative user interface and classified into 4 discrete call types
by volunteers. The ground truth labels have been verified by
experts. Table ~\ref{table:dataset} shows the composition of the
dataset used for evaluation. We use two established evaluation metrics
that measure the retrieval effectiveness. Precision at 1 is simply the
number of queries for which the first retrieved call has the same
class as the query. The mean average precision (MAP) is the most
frequently used summary measure of a ranked retrieval run. Average
precision of a single query is the mean of the precision scores after
each relevant document has been retrieved. The value for the run (a
set of queries) is the mean of the individual average precision
scores. MAP combines aspects of both precision and recall and rewards
returned relevant items higher in the list.


\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Call Type     & N1      & N3      & N4 & N47 \\ 
\hline 
Instances     & 36      & 56      & 60  &  33 \\ 
\hline 
MAP            & 0.63   &  0.94   & 0.78  & 0.58  \\ 
%  Precision@1 &      &       &      &       \\ 
\hline 
\end{tabular} 
\caption{Dataset composition and MAP scores for best configuration
  (Hertz frequency scale, SACF pitch extractor and DTW matching) } 
\label{table:dataset}
\end{center}
\end{table} 


Table ~\ref{table:dataset} shows the best MAP scores achieved for each
type of call. Table ~\ref{table:dataset_map} shows the MAP scores and
average precision score at 1 over the entire dataset for combinations
of different representations and pulse rate extraction strategies. As
can be seen, the SACF pitch extractor is the best performing
independently of the retrieval strategy. The DTW matching is also the
best performing retrieval strategy.

% We have also conducted experiments with different frequency scale 
% representations such as the Bark-scale \cite{} and logarithmic frequency 
% but in all configurations they performed worst than the default linear
% frequency reprsentation in Hertz. 



\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline   
              & Features           & Contour & DTW \\
\hline 
PRAAT     & 0.38 (0.38)      & 0.52 (0.40)        & 0.67 (0.4)
   \\ 
\hline 
YIN          & 0.50 (0.77)       & 0.51 (0.72)        & 0.72 (0.95) \\ 
\hline 
SACF        & 0.63 (0.79)    & 0.66  (0.82)        &  0.77 (0.95)\\ 
%  Precision@1 &      &       &      &       \\ 
\hline 
\end{tabular} 
\caption{Mean Average Precision(Precision at 1) Scores for different pitch extraction 
and retrieval strategies} 
\label{table:dataset_map}
\end{center}
\end{table} 





% \begin{table} 
% \begin{center}
% \begin{tabular}{|l|c|c|c|}
% \hline   
%               & Features & Contour & DTW \\
% \hline 
% PRAAT     &    0.38      &       0.40       &  0.4      \\ 
% \hline 
% YIN          &    0.77         &   0.72          &       0.95 \\ 
% \hline 
% SACF        &    0.79   &    0.82    &  0.95 \\ 
% %  Precision@1 &      &       &      &       \\ 
% \hline 
% \end{tabular} 
% \caption{Average Precision at 1 scores for different pitch extraction 
% and retrieval strategies} 
% \label{table:dataset_prec1}
% \end{center}
% \end{table} 




\section{Conclusions and Future Work} 

We describe a web-based collaborative web interface for retrieval and
annotation of a large archive of Orca vocalizations. A number of
different strategies for pulse rate extraction and retrieval were
experimentally compared. Excellent results were obtained by a 
combination of a computationally efficient pitch extractor 
based on the summary autocorrelation function and a matching strategy
based on dynamic time warping. The best configuration achieves a mean
average precision of 0.77 and the first retrieved call is relevant
$95\%$ of the time. In the future we plan to expand our dataset both in terms of instances and call types.  In addition we plan to
investigate more thoroughly different design choices, possibly 
taking into account information about the hearing system of Orcas
\cite{nummela99_orca_ear_anatomy}. 









\section{Acknowledgments}

We would like to thank Paul Spong and Helena Symonds of Orcalab for
providing the data and inspiration for this project. We would also
like to thank the National Sciences and Engineering Research Council
(NSERC) for their financial support. This work was partly supported by
a fellowship within the Postdoc-Programme of the German Academic
Exchange Service (DAAD).


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{icme2011gtzan}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2013,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2013} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{The Orchive : Data mining a massive bioacoustic archive}

\begin{document} 

\twocolumn[
\icmltitle{The Orchive : Data mining a massive bioacoustic archive}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2013
% package.
\icmlauthor{Steven Ness}{sness@uvic.ca}
\icmladdress{Department of Computer Science,
            University of Victoria, Canada}
\icmlauthor{George Tzanetakis}{gtzan@cs.uvic.ca}
\icmladdress{Department of Computer Science,
            University of Victoria, Canada}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{bioacoustics, machine learning, whale, orca}

\vskip 0.3in
]

\begin{abstract} 
  The Orchive is a large collection of over 20,000 hours of audio
  recordings from the OrcaLab research facility located off the
  northern tip of Vancouver Island.  It contains recorded orca
  vocalizations from the 1980 to the present time and is one of the
  largest resources of bioacoustic data in the world.  We have
  developed a web-based interface that allows researchers to listen to
  these recordings, view waveform and spectral representations of the
  audio, label clips with annotations, and view the results of machine
  learning classifiers based on automatic audio features extraction.
  In this paper we describe such classifiers that discriminate between
  background noise, orca calls, and the voice notes that are present
  in most of the tapes.  Furthermore we show classification results
  for individual calls based on a previously existing orca call
  catalog. We have also experimentally investigated the scalability of
  classifiers over the entire Orchive.

  
  
\end{abstract} 


\section{Introduction}
\label{introduction}
The Orchive is a large archive containing over 20,000 hours of
recordings from the Orcalab research station. These recordings were
made using a network of hydrophones and originally stored on analog
cassette tapes. OrcaLab is a research station on Hanson Island which
is located at the north part of Vancouver Island on the west coast of
Canada. It has been in continuous operation since 1980.  It was
designed as a land based station in order to reduce the impact on the
orcas under study, as the noise and disturbance from boats affects the
orcas in observable but currently unquantified ways.  In collaboration
with OrcaLab, we have digitized the tapes and have made these
recordings available to the scientific community through the Orchive
website (\hyperref[http://orchive.cs.uvic.ca]{http://orchive.cs.uvic.ca}).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{dm_orchive}
\caption{Annotated audio from from the Orchive}
\label{fig:dm_orchive}
\end{figure}



Over the past 5 years, a number of orca researchers using our website
have added over 18,000 clip annotations to our database.  A small
section of annotated audio from the Orchive is shown in Figure
\ref{fig:dm_orchive}.  These clip annotation are of two main types:
The first is clips that differentiate background noise from orca calls
and from the voice notes of the researchers that collected the data.
The second type of clip annotations classify orca vocalizations into
different calls.  Orcas make three types of vocalizations,
echolocation clicks, whistles and pulsed calls.  The pulsed calls are
highly conserved stereotyped vocalizations which have been classified
into a catalog of over 52 different calls by John Ford \cite{ford87}.
Of the 18,000 annotations currently in the Orchive, 3000 are of these
individually classified calls.  In addition, we have a curated call
catalog containing 384 different recordings of different calls
vocalized by a variety of different pods and matrilines. This catalog
is used for training the annotators. 



Many parts of the recordings contain boat noise which makes
identifying orca calls both difficult and tiring. In addition, the
size of the Orchive makes full human annotation practically
impossible. Therefore we have explored machine learning
approaches to the task. One data mining task is to segment and label the
recordings with the labels background, orca, voice. Another is to
subsequently classify the orca calls into the classes specified in the
call catalog.


\section{Related Work}
\label{relatedWork}

Audio feature extraction is the first step in classifying audio using
machine learning algorithms.  Mel-Frequency Cepstral Coefficients
\cite{Logan00melfrequency} (MFCC) have been widely used for this
purpose.  MFCCs have also been used in bioacoustics, and have been
used to classify insect sounds \cite{leqing11}, birds
\cite{changhsing07} and orca calls \cite{ness2008}.  In this work we
also use MFCCs, but supplement them with other audio features
including Centroid frequency, Rolloff frequency, Flux, and
Zero Crossings.

% Another audio feature that is often used is an estimate of the
% fundamental frequency (F0) or pitch, one well known algorithm for this
% is the Yin algorithm \cite{cheveigne02} and another is SWIPEP
% \cite{camachophd}.  We consider these to be very interesting
% algorithms, however, one problem that prevented us using them in this
% work is that harmonics are often detected, these are integer multiples
% of the fundamental frequency and these errors are referred to as
% octave errors. 

%Another type of audio feature extraction that is promising is features
%based on models of the auditory cortex \cite{lyon82}.  These
%algorithms model the properties of the cochlea and peripheral nervous
%system\cite{lyon10}, and have at their core an adaptive filterbank
%coupled to a triggered pulse model \cite{waltersphd}.  They have been
%used successfully to classify music \cite{sness10}.  However, one
%issue with these systems is that instead of a 1-dimensional (waveform)
%or 2-dimensional (spectral), they lead to a 3-dimensional dataset in
%which 2-D audio images change over time.  The added amount of data and
%processing time required made them prohibitive for the size of the
% Orchive. 

%% For this paper, we used the Marsyas \cite{marsyas} Music Information
%% Retrieval (MIR) system.  There are a number of other MIR systems that
%% are in wide use, some of these include jMIR \cite{mckayphd}, SmIrK
%% \cite{wang07} and AIMC \cite{waltersphd}.  However, the Marsyas
%% framework implements most, if not all, of the most popular audio
%% feature extraction algorithms, and does in an computationally
%% efficient manner.

Our system uses two types of web based interfaces.  The first are
tools aimed at expert users, and the second are simpler interfaces
designed for crowdsourcing the annotation. There are a number of tools
that experts use to segment and analyze audio and specifically
bioacoustic data.  One of the most popular is Raven \mbox{(\hyperref[http://www.birds.cornell.edu/raven]{http://www.birds.cornell.edu/raven})},
a toolkit developed at the Cornell Lab of Ornithology.  Our expert
based tools have many similarities to Raven such as the ability to
view waveforms and spectrograms at multiple levels of detail. In
addition, our system tightly integrates the visualization and viewing
of data from machine learning classifiers.  Another tool is the Sonic
Visualizer \cite{cannam10}, which supports a wide variety of waveform
and spectral audio representations. The popular audio program Audacity
has been extended to allow for the annotation of audio data
\cite{li06}.  The biggest difference our system compared to these
systems is that the software is all web-based, so users do not have to
install a separate program and can more easily view long audio files
and analyze data across multiple recordings.

% Crowdsourcing is a new type of collaboration where non-specialists
% help expert scientists \cite{howe08_crowdsourcing} 
% % and has been used to great advantage
% % \cite{surowiecki05_crowdsourcing} in a number
% % \cite{bradham08_crowdsourcing} of research programs
% % \cite{travis08_crowdsourcing}.
% One of the most successful examples is Galaxy Zoo
% \cite{anze08_galaxyzoo}. Games-with-a-purpose (GWAP) \cite{vonahn08},
% which are computer games that harness the ability of people to solve
% tasks in a game setting.  The first GWAP was the ESP game
% \cite{vonahn04} in which two users on computers connected to the
% internet try to guess words that describe an image.  In ``Game-powered
% Machine Learning'' \cite{barrington12}, Barrington, Turnbull and
% Lanckiet describe a system that combines Games With A Purpose (GWAP)
% with Machine Learning in a framework that they call ``Active Machine
% Learning''.  We have developed a framework for allowing scientist to
% easily generate new games for different typos of audio, for example
% whales, birds and human vocalizations. The system has been used to
% acquire annotations for the Orchive.


%Our system
%uses many concepts from these papers, but instead of developing a new
%game for every new type of data, it uses a few casual games, such as a
%tile matching game, and allows scientists easily generate new games
%for different types of audio, for example whales, birds and human
%vocalizations.


\section{System Overview}
\label{systemDescription}

We have developed a collaborative web interface that allows expert
researchers to listen to, view and annotate large collections of audio
data.  The system also supports a variety of audio feature extraction
and machine learning algorithms, and enables users to view the results
of these algorithms.  
% In addition, this
% system enables these researchers to create casual games that allow
% them to quickly label audio data, these games are designed to be used
% both by the researchers, as a quick way to annotate data, and also for
% crowdsourcing this data, both to volunteers and also through
% micropayment systems such as the Amazon Mechanical Turk
% \cite{kittur08_crowdsourcing}.  
A diagram of this system is shown in Figure \ref{fig:systemDiagram}. 
\begin{figure}
\centering
\includegraphics[width=80mm]{systemDiagram}
\caption{System Diagram}

\label{fig:systemDiagram} 
\end{figure} 
For audio features we use the Marsyas\cite{marsyas} Music Information
Retrieval system.  Marsyas allows us to perform both audio feature
extraction and machine learning on audio data directly.  It can also
output audio feature vectors in a form that can be used by other
machine learning tools such as Weka. However, the size of the files
from audio feature extraction can be very large, and a recent audio
feature extraction run on the entire Orchive generated around 700GB of
audio features.  Because of this, we often use Marsyas in its
streamlined mode, where both audio feature extraction and machine
learning are carried out by the same executable.

In order to efficiently analyze large audio archives we utilize
distributed computing. There are many systems for distributing
computation. We currently use the Portable Batch System (PBS)
\cite{henderson95}, a grid-computing system where similar data can be
processed in parallel by a large number of computers. 

%% We also have
%% experimented with Hadoop \cite{shvachko08}, a system that uses a
%% Map-Reduce paradigm \cite{dean08} to process large amounts of data.

\section{Experimental Results}
\label{experimentalResults}

\subsection{Audio Feature Extraction Parameters}

The first set of parameters that needed to be optimized were the
Window Size and Hop Size of the Digital Signal Processing (DSP)
algorithms that take the input audio and calculate spectral
information from them, the fundamental basis for which is the Fast
Fourier Transform (FFT) algorithm.  The length of time over which to
calculate the statistical properties of the features, this is known in
bextract as the ``memory'' and corresponds to the number of frames of
features that are accumulated.  We ran this on a 600 second audio
dataset labeled as orca, background and voice with equal lengths of
each label.  In this dataset, the voice was trimmed by hand, the orca
consisted of the middle 0.023 seconds of approximately 10,000 clips,
and the background consisted of 0.15 seconds of approximately 1300
clips.  The results for this are shown in Table \ref{table:dspParams}.
From this we can see that as we go to longer window sizes, the
classification performance increases, and as we go to longer
accumulation window sizes, the performance also increases. For the
remaining experiments we use these optimal settings.

\begin{table}
\begin{tabular}{|r|r|r|r|r|r|}
\hline
 winsize  &  hopsize  &  memory  &  $\#$ correct  \\
\hline
 20 &  512 &  256 &     70.16 \\
 20 & 1024 &  512 &     71.88 \\
 20 & 2048 & 1024 &     74.17 \\
 20 & 4096 & 2048 &     73.38 \\
\hline
 40 &  512 &  256 &     72.94 \\
 40 & 1024 &  512 &     75.67 \\
 40 & 2048 & 1024 &     78.29 \\
 40 & 4096 & 2048 &     80.58 \\
\hline
 80 &  512 &  256 &     76.53 \\
 80 & 1024 &  512 &     78.39 \\
 80 & 2048 & 1024 &     81.88 \\
 80 & 4096 & 2048 &     85.72 \\
\hline
\end{tabular}
\caption{DSP Parameters}
\label{table:dspParams}
\end{table}


\subsection{Orca/Background/Voice Classification}

The first task we investigate is the classification of audio into
three classes: orca, background, and human voice.  In order to test
the different distributed audio classification systems we first
generated a set of training and testing data, one of these was a set
of calls from the curated call catalog with silence removed, and the
other was an entire 45 minute recording from the Orchive which had
been annotated by an orca researcher. In a previous paper
\cite{ness2008}, we were able to obtain a classification performance
of 82\% when using a SVM classifier on hand labeled data.  We looked
in more detail at the training data, and found that there was a small
amount of silence before and after the vocalization.  The results can
be found in the first line of table \ref{table:handTrimmed} and had
93.5\% of the instances classified correctly.  This large jump in
performance was unexpected but easily understood, because if feature
vectors of silence are labeled as orca, this will cause issues for
the classifier.  We then took a 4 minute region of orca calls and
voice notes and removed all the silences from both of them, for this
we obtained a classification accuracy of 96.1\% when looking at the
call catalog dataset, and 95.0\% when looking at the annotated
recording.

However, this process of hand trimming recordings would be unfeasible
to do on the entire 18,000 current annotations.  For this, we instead
tested a procedure where we extracted a small section of audio from
the middle of each clip where it was most probable that the orca call
would be found.  The results for this procedure for a clip of 0.023
seconds from the middle of each orca call was 96.5\% and for the
recording from the Orchive, the accuracy was 93.4\%.

\begin{table}
\begin{tabular}{|l|c|l|l|r|r|}
\hline
Training       & length  & \% corr.   & \% corr.  & \% corr.  \\
dataset        &  (sec)  &  10-fold     &   (calls)   &   (442A)    \\
\hline
hand-10sec     &    30   &   99.4       &   93.5      &   93.1     \\
hand-4min      &    720  &   99.9       &   96.1      &     95.0   \\
ms 100         &    300  &   99.9       &   96.5      &     93.4   \\
%% ms 200 &    600  &   99.6       &   69.8      &     63.5   \\
%% ms 300 &    900  &   99.6       &   57.1      &     57.5   \\
%% ms 400 &    1200 &   99.6       &   55.7      &     59.4   \\
%% ms 500 &    1500 &   99.7       &   49.8      &     62.6   \\
\hline
\end{tabular}
\caption{Classification results with hand trimmed orca vocalizations
  using bextract.}
\label{table:handTrimmed}
\end{table}


\subsection{Call classification}

Using the Orchive 
%and orcaGame web interfaces, 
we created a collection of 319 calls of 6 classes, these included the
common calls ``N1'', ``N3'', ``N4'', ``N7'', ``N9'' and ``N47''.
Audio features for each 20ms audio frame of these files were
generated, these included the MFCC coefficients, Centroid, Rolloff,
Flux and Zero crossings.  The mean and standard deviation for each of
these features were then calculated and were output as a .arff file.
These were then classified using the J48 tree classifier, which gave
an accuracy of 58\%.  We next tried the Naive Bayes classifier which
gave an accuracy of 65\%.  The SMO SVM classifier produced the best
results, giving an accuracy of 75\%.

The confusion matrix for the SVM classifier is shown in Table
\ref{table:callCatalogConfusionMatrix}.  From this we can see that most
of the classes are classified correctly.  One interesting point is
that there is some confusion between the N7 and N9 calls, and it can
be noted that these calls are often confused with each other by humans
as well.  The confusion between the N4 and N47 calls comes from the
fact that they share many spectral features even though they have a
different temporal structure.  We also ran this analysis on a set of
197 calls directly from the Orchive, the results from this are shown
in \ref{table:orchiveConfusionMatrix}.  These calls have more
variation in volume and boat noise than those in the call catalog, but
the algorithm also performs well on these, with an accuracy of 98.5\%.

\begin{table}
\centering
\begin{tabular}{|c|cccccc|} 
\hline
     & N1  & N3  & N4  & N47 & N7 & N9 \\
\hline
N1   & 30  &  0  &  0  &  1  &  0 &  4 \\
N3   &  1  & 53  &  2  &  0  &  0 &  0 \\
N4   &  0  &  1  & 80  &  9  &  1 &  2 \\
N47  &  0  &  0  & 32  &  0  &  0 &  1 \\
N7   &  0  &  2  &  1  &  0  & 15 & 13 \\
N9   &  3  &  0  &  2  &  0  &  2 & 64 \\
\hline
\end{tabular}
\caption{Confusion matrix for 10-fold crossvalidation with SVM
  classifier on calls from call catalog }
\label{table:callCatalogConfusionMatrix}
\end{table}


\begin{table}
\centering
\begin{tabular}{|c|cccc|} 
\hline
   &   N1   &   N4   &   N7   &   N9   \\
\hline
N1 & 1726  &   0     &   0    &   0    \\
N4 &   12  & 2858    &   0    &   0    \\
N7 &    0  &   2     & 1297   &   59   \\
N9 &    0  &   0     &  70    & 3231   \\
\hline
\end{tabular}
\caption{Confusion matrix for 10-fold crossvalidation with SVM
  classifier on labelled calls from Orchive.}
\label{table:orchiveConfusionMatrix}
\end{table}

\subsection{Performance}

In order to investigate the performance of the classification of
recordings into Orca, Background and Voice, we trained a SVM with a
section of 30 and 240 seconds of hand trimmed data using the bextract
program in Marsyas.  We then used the sfplugin program in Marsyas to
classify all the recordings in the Orchive on the Hermes/Nestor
cluster, part of the Westgrid computational resource.  For this we
divided the data into sets of 1\%, 5\%, 10\% and 100\% of the Orchive.
The timing results of these datasets run on 10 computers are shown in
Table \ref{table:performance}.  From this we can see that the
classifier that had more data took longer to classify, and that the
speedup from taking samples of the data was almost linear.

\begin{table}
\centering
\begin{tabular}{|c|c|c|} 
\hline
Training data & \% of Orchive & Run time \\
 (sec)        &               & (d:h:m:s) \\
\hline
30            &      1      &    00:00:05:18      	 \\
30            &      5      &    00:00:25:20     \\
30            &      10     &    00:00:50:58    \\
30            &      100    &    00:09:01:05       \\
\hline
240           &      1      &    00:06:16      \\
240           &      5      &    00:00:31:21       \\
240           &      10     &    00:04:47:12     \\
240           &      100    &    02:04:18:32    \\
\hline
\end{tabular}
\caption{Performance results of timing on subsets of the entire
  Orchive dataset.}
\label{table:performance}
\end{table}

\section{Conclusion}
\label{conclusion}

In this paper we described a system that allows orca researchers to
listen to, view and annotate the large amount of audio data in the
Orchive.  
%In addition, this system allows researchers to create simple
%games that allow them to use a crowdsourcing approach to label large
%amounts of data.  
The system also allows researchers to run and view the results of
audio feature extraction and machine learning algorithms on this data.

We investigated the performance of different parameters for the audio
feature extraction process and showed that in general, large window
sizes were beneficial, and that increasing the length of time that
statistics were taken over the data was also beneficial.  We showed
that by carefully hand editing clips to remove silence was very
useful, and boosted performance from around 90\% to 96\% on actual
recordings.  We then used these classifiers on a cluster to classify
all the recordings in the Orchive into the classes, Orca,
Background and Voice.

In terms of call classification, we showed that using a Support Vector
Machine outperformed the J48 tree and Naive Bayes classifiers.  A
confusion matrix showed that calls that were difficult for humans to
classify were also difficult for the machine learning system, and also
showed that in some cases, calls with similar spectral content but
different time information could also be confused.  We plan to use
techniques from bioinformatics to model the time evolution of signals.






\bibliography{icml2013}
\bibliographystyle{icml2013}

\end{document} 
\documentclass[%
	%draft,
	%submission,
	%compressed,
	final,
	%
	%technote,
	%internal,
	%submitted,
	%inpress,
	reprint,
	%
	%titlepage,
	notitlepage,
	%anonymous,
	narroweqnarray,
	inline,
	twoside,
        invited,
	]{ieee2008aich}

\newcommand{\latexiie}{\LaTeX2{\Large$_\varepsilon$}}

%\usepackage{ieeetsp}	% if you want the "trans. sig. pro." style
%\usepackage{ieeetc}	% if you want the "trans. comp." style
%\usepackage{ieeeimtc}	% if you want the IMTC conference style

% Use the `endfloat' package to move figures and tables to the end
% of the paper. Useful for `submission' mode.
%\usepackage {endfloat}

% Use the `times' package to use Helvetica and Times-Roman fonts
% instead of the standard Computer Modern fonts. Useful for the 
% IEEE Computer Society transactions.
%\usepackage{times}
% (Note: If you have the commercial package `mathtime,' (from 
% y&y (http://www.yandy.com), it is much better, but the `times' 
% package works too). So, if you have it...
%\usepackage {mathtime}

% for any plug-in code... insert it here. For example, the CDC style...
%\usepackage{ieeecdc}

\begin{document}

%----------------------------------------------------------------------
% Title Information, Abstract and Keywords
%----------------------------------------------------------------------
\title{Collaborative Tools for Computational Ethnomusicology}

% format author this way for journal articles.
% MAKE SURE THERE ARE NO SPACES BEFORE A \member OR \authorinfo
% COMMAND (this also means `don't break the line before these
% commands).
%  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
\author[NESS, WRIGHT, BIR\'{O}, SCHLOSS AND TZANETAKIS]{
	Steven R. Ness,
	\authorinfo{
		S.\,R.\,Ness is with the Department Computer Science, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
\and{}
	Matthew Wright,
	\authorinfo{
		M.\,Wright is with the Department Computer Science, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
\and{}
	D\'{a}niel P\'{e}ter Bir\'{o},
	\authorinfo{
		D.\,P.\,Biro is with the School of Music, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
\and{}
	Andrew Schloss,
	\authorinfo{
		A.\,Schloss is with the School of Music, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
\and{}and
	George Tzanetakis
	\authorinfo{
		G.\,Tzanetakis is with the Department Computer Science, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
}


\journal{IEEE Intelligent Systems}
\titletext{Special issue on AI and Cultural Heritage}
\ieeecopyright{0018--9456/97\$10.00 \copyright\ 2008 IEEE}
\lognumber{xxxxxxx}
\pubitemident{S 0018--9456(97)09426--6}
\loginfo{Manuscript received August 15th, 2008}

\maketitle               

\begin{abstract} 
As we move into the Petabyte age, we are faced with new opportunities
and challenges in regards to our diverse cultural heritages.  Although
the tendancy towards cultural homogeneity pervades much of the world,
there is also a drive towards preserving and disseminating long
established elements of culture.  One of the most important of these
are the traditions of music and vocal performance.  Ethnomusicology is
one of the fields that studies these diverse artistic expressions in
cultures across the world.  Recently founded has been the field of
Computational Ethnomusicology, which attempts to use computational
techniques to explore, annotate, categorize, and reason about the
ethnography of musical traditions.  The current explosion of web-based
collaborative tools provides a framework upon which recently developed
computational tools can be incorporated into and shared between
researchers around the world.  We present here two of our recent
developments of different tools to assist researchers in Computational
Ethnomusicology.  The first is a tool to help researchers of
Afro-Cuban music, the second is an exploration of chant in different
cultures around the world.
\end{abstract}

\begin{keywords}
keywords
\end{keywords}


%----------------------------------------------------------------------
% SECTION I: Introduction
%----------------------------------------------------------------------

\section{Introduction}
The recent explosion of web-based tools has been a boon to researchers
around the world in fields as diverse as high-energy physics, genomics
and astronomy.  The field of Computational Ethnomusicology is one of
the more recent beneficiaries of these tools, which allow researchers
from around the world to listen to, analyse, categorize, explore and
preserve musical artifacts from traditions around the world.  We have
recently developed computational tools and techniques for the fields
of Afro-Cuban music and improvised, partially notated, and
gesture-based \cite{krumhansl90} notational chant traditions including
Hungarian siratok (laments)\footnote{Archived Examples from Hungarian
  Academy of Science (1968-1973)}, Torah cantillation
\cite{zimmerman00} \footnote{Archived Examples from Hungary and
  Morocco from the Feher Music Center at the Bet Hatfatsut, Tel Aviv,
  Israel}, tenth century St. Gallen plainchant
\cite{treitler82}\footnote{Godehard Joppich and Singphoniker:
  Gregorian Chant from St. Gallen (Gorgmarienh√ºtte: CPO 999267-2,
  1994)}, and Koran recitation \footnote{Examples from Indonesia and
  Egypt: in Approaching the Koran (Ashland: White Cloud, 1999)}.

\subsection{Afro-Cuban}

We present a set of techniques and tools designed for studying rhythm
and timing in recordings of Afro-Cuban music with particular emphasis
on ``clave,'' a rhythmic pattern used for temporal organization. In
order to visualize timing information we propose a novel graphical
representation that can be generated by computer from signal analysis
of audio recordings and from listeners' annotations collected in real
time.  The proposed visualization is based on the idea of Bar
Wrapping, which is the breaking and stacking of a linear time axis at
a fixed metric location.

The techniques proposed in this paper have their origins in Music
Information Retrieval (MIR) but have been adapted and extended in
order to analyze the particular music culture studied. Unlike much of
existing work in MIR in which the target user is an ``average'' music
listener, the focus of this work is people who are ``experts'' in a
particular music culture. Examples of the type of questions they would
like to explore include: how do expert players differ from each other,
and also from competent musicians who are not familiar with the
particular style; are there consistent timing deviations for notes at
different metric positions; how does tempo change over the course of a
recording etc. Such questions have been frequently out of reach
because it is tedious or impossible to explore without computer
assistance.

Creating automatic tools for analyzing micro-timing and tempo
variations for Afro-Cuban music has been challenging. Existing
beat-tracking tools either don't provide the required functionality
(for example only perform tempo tracking but don't provide beat
locations) or are simply not able to handle the rhythmic complexity of
Afro-Cuban music because they make assumptions that are not always
applicable, such as expecting more and louder notes on metrically
``strong'' beats.  Finally the required precision for temporal
analysis is much higher than typical MIR applications. These
considerations have motivated the design of a beat tracker that
utilizes domain-specific knowledge about Cuban rhythms.

The proposed techniques fall under the general rubric of what has been
termed \textit{Computational Ethnomusicology} (CE), which refers to
the design and usage of computer tools that can assist
ethnomusicological research \cite{TzanetakisWright2007} .  Futrelle
and Downie argued for MIR research to expand to other domains beyond
Western pop and classical music \cite{FutrelleDownie2002}.  Retrieval
based on rhythmic information has been explored in the context of
Greek and African traditional music \cite{Antonopoulos2007}.

Our focus here is the analysis of music in which percussion plays an
important role, specifically, Afro-Cuban music.  Schloss
\cite{Schloss85} and Bilmes \cite{Bilmes93} each studied timing
nuances in Afro-Cuban music with computers. Beat tracking and tempo
induction are active topics of research, although they have mostly
focused on popular music styles \cite{McKinney2007}.  Our
work follows Collins' suggestion \cite{Collins2006} to build beat
trackers that embody knowledge of specific musical styles.

The \textit{clave} is a small collection of rhythms embedded in
virtually all Cuban music.  Clave is a repeated syncopated rhythmic
pattern that is often explicitly played, but often only implied; it is
the essence of periodicity in Cuban music. An instrument also named
``clave'' (a pair of short sticks hit together) usually plays this
repeating pattern. Clave is found mainly in two forms: \textit{rumba
  clave} and \textit{son clave}. (One way of notating clave is shown
in Figure ~\ref{fig:clave}.)

\begin{figure}[ht]
\centering
\begin{tabular}{c c} 
\includegraphics[width= 0.475 \linewidth]{son-notation} & 
\includegraphics[width= 0.475 \linewidth]{rumba-notation} \\ 
\end{tabular} 
\caption{Son (left) and rumba (right) clave}
\label{fig:clave}
\end{figure}

Our study of timing requires knowing the exact time of every note played by the clave.
We can then decompose this data into an estimate of how tempo changes over time 
(what is called the \textit{tempo curve}) and a measure of each individual note's deviation
from the ``ideal'' time predicted by a metronomic rendition of the
patterns shown in Figure ~\ref{fig:clave}.

Unfortunately, we do not know of any databases of Afro-Cuban music
with an exact ground-truth time marked for every clave note or even
for every downbeat. \footnote{Bilmes recorded about 23 minutes of
  Afro-Cuban percussion at MIT in 1992, and performed sophisticated
  analysis of the timing of the \textit{guagua} and \textit{conga}
  (but not clave) instruments \cite{Bilmes93}; unfortunately these
  analog recordings are not currently available to the research
  community.}  Therefore we constructed a small four-song
database \footnote{Here is the name, artist, and source recording for
  each song, along with the two-character ID used later in the paper:
  \textit{LP}: \textit{La Polemica}, Los Mu\~{n}equitos de Matanzas,
  Rumba Caliente 88.  \textit{CB}: \textit{Cantar Bueno}, Yoruba
  Andabo, El Callejon De Los Rumberos.  \textit{CH}: \textit{Chacho},
  Los Mu\~{n}equitos de Matanzas, Cuba: I Am Time (Vol. 1).
  \textit{PD}: \textit{Popurrit de Sones Orientales}, Conjunto de
  Sones Orientales, Son de Cuba. } and gathered ground truth clave
timing data by having an expert percussionist with Afro-Cuban
experience tap along with the clave part. Custom sample-accurate tap
detection/logging software
%\cite{MattWrightDissertation}
automatically timestamps the taps.

\subsection{Chant}

These various types of chant employ \textit{melodic} \textit{formulae},
figures that define certain melodic identities that help to define
syntax, pronunciation, and expression. \ Each tradition's melodic
framework is governed by the particular religious context for
performance.

The \textit{{sirat\'o}} is a lament ritual from Hungary that goes back
at least to the Middle Ages. \footnotemark{}\footnotetext{\ {Lamenting
    by women was common already in Biblical times:} ``Mourning songs
  for the dead also go back to primitive times. \ Although every
  religion and secular form of legislation{\dots} has endeavored to
  control mourning practices, they are still customary even today''
  \cite{kodaly60}, (p. 76).  This improvised song type is integral for
  our study, as it exemplifies inherent relationships between speech
  and singing while demonstrating stable melodic formulae within an
  oral/aural ritual context.} Jewish Torah trope is ``read'' using the
twenty-two cantillation signs of the \textit{te'amei
  hamikra,}{\footnotemark{}}\footnotetext{\ The term ``ta'amei
  hamikra'' means literally ``the meaning of the reading.''}  \textit{
}developed by the Masorete rabbis \footnote{\ ``Originally, the
  biblical books were written as continuous strings of letters,
  without breaks between words. \ This led to great confusion in the
  understanding of the text. To ensure the accuracy of the text, there
  arose a number of scholars known as the Masoretes in the sixth
  century CE, and continuing into the tenth century'' \cite{wigoder89}
  (p. 468). } between the sixth to the ninth centuries. The melodic
formulae of Torah trope govern syntax, pronuciation and meaning. While
the written \textit{te'amim} have not changed since the tenth century
C.E., their corresponding melodic formulae are determined not only by
Jewish tradition of cantillation but also by the melodic framework of
their surrounding musical environment.

The performance framework for Koran recitation is not determined by
text or by notation but by rules of recitation that are primarily
handed down orally \cite{zimmerman00}, (p.128). \footnote{``Like the
  Hebrew \textit{miqra' }the primary name `Koran' derives from the
  root q-r, i.e., `reading': the visual implication of text is not
  implied with this root. \ Rather the concepts `pronounce, calling,
  reciting' are expressed with the word, so that an adequate
  translation of Koran (Qur' \~an) could be `the recited'''
  \cite{zimmerman00}, (p. 27, translation by Bir\'o).} Here the
hierarchy of spoken syntax, expression and pronunciation play a major
role in determining the vocal styles of
\textit{{Tajw}}\textit{\=\i}\textit{{d}}{
  \footnotemark{}}\footnotetext{\
  ``\textit{{Tajw}}\textit{\=\i}\textit{{d }}{[is] the system of rules
    regulating the correct oral rendition of the Qur'an. The
    importance of }\textit{{Tajw}}\textit{\=\i}\textit{{d}}{ to any
    study of the Qur'an cannot be overestimated:
  }\textit{{Tajw}}\textit{\=\i}\textit{{d,}}{ preserves the nature of
    a revelation whose meaning is expressed as much as by its sound as
    by it content and expression, and guards it from distortion by a
    comprehensive set of regulations which govern many of the
    parameters of the sound production, such as duration of syllable,
    vocal timbre and pronunciation'' }\cite{nelson85} (p.  14).{
    {\textless}http://www.grovemusic.com.ezproxy.library.uvic.ca{\textgreater}}}\textit{
}and
\textit{{Tart}}\textit{\=\i}\textit{{l}}{\footnotemark{}}\footnotetext{\
  ``\textit{{Tart}}\textit{\=\i}\textit{{l,}}{ another term for
    recitation, especially implies slow deliberate attention to
    meaning, for contemplation.'' (Neubaurer and Doublday). }}
\textit{{.}} The resulting melodic phrases, performed not as ``song''
but ``recitation'' are, like those of Torah trope, determined by both
the religious and larger musical cultural contexts.

The early plainchant neumes came from a logogenic culture that was
based on textual memorization; the singing of memorized chants was
central to the preservation of a tradition that developed over
centuries \cite{treitler82}. Already in the ninth century the
technology of writing was advanced enough to allow for new degrees of
textual nuance. Here the ability for formulae to transcend textual
syntax is at hand, pointing to the possibility for melodic autonomy
from text. \footnote{``The Gregorian Chant tradition was, in its early
  centuries, an oral performance practice{\dots} The oral tradition
  was translated after the ninth century into writing. But the
  evolution from a performance practice represented in writing, to a
  tradition of composing, transmission, and reading, took place over a
  span of centuries'' \cite{treitler82} (p. 237).} Chant scholars have
investigated historical and phenomenological aspects of chant formulae
to discover how improvised melodies might have developed to become
stable melodic entities, paving the way for the development of
notation. \footnote{``The church musicians who opted for the inexact
  aides-m\'emoire of staffless neumes - for skeletal notations that
  ignored exact pitch-heights and bypassed many nuances - were content
  with incomplete representations of musical substance because the
  full substance seemed safely logged in memory'' \cite{levy98} (p.
  137).} A main aspect of such investigations has been to explore the
ways in which melodic contour defines melodic identities \cite{karp98}
We hope that our computational tools will allow for new possibilities
for paradigmatic and syntagmatic chant analysis in both culturally
defined and cross-cultural contexts. This might give us a better sense
of the role of melodic gesture in melodic formulae and possibly a new
understanding of the evolution from improvised to notation-based
singing in and amongst these divergent chant traditions.


%**********************************************************************
%
% AGORA
% 
%**********************************************************************

\section{AGORA Paper}

\section{Formula, Gesture and Syntax }

\section{Melodic Contour Analysis Tool}

Our tool takes in a (digitized) monophonic or heterophonic recording and
produces a series of successively more refined and abstract
representations of the melodic contours.

It first estimates the fundamental frequency (``F0,'' in this case
equivalent to pitch) and signal energy (related to loudness) as
functions of time. We use the SWIPEP fundamental frequency
estimator\cite{camachophd} with all default parameters except for
upper and lower frequency bounds hand-tuned for each example. For
signal energy we simply take the sum of squares of signal values in
each non-overlapping 10-ms rectangular window.

The next step is to identify pauses between phrases, so as to
eliminate the meaningless and wildly varying F0 estimates during these
noisy regions. We define an energy threshold, generally 40 decibels
below each recording's maximum. If the signal energy stays below this
threshold for at least 100 ms then the quiet region is treated as
silence and its F0 estimates are ignored. Figure
~\ref{fig:agora_inna_example} shows an excerpt of the F0 and energy
curves for an excerpt from the Koran \textit{sura} (``section'')
\textit{Al{}-Qadr} (``destiny'') recited by the renowned Sheikh
Mahm\^ud Khal\^il al-Husar\^i from Egypt.

\begin{figure}[htb]
\includegraphics[width=80mm]{agora_inna_example}
\label{fig:agora_inna_example}
\caption{Pitch (top, MIDI units) and Energy (bottom, decibels) Contours} 
\end{figure} 

\begin{figure}[htb]
\includegraphics[width=80mm]{agora_histogram_scale}
\label{fig:agora_histogram_scale}
\caption{Recording-specific scale derivation} 
\end{figure} 

The next step is pitch quantization. Rather than externally imposing a
particular set of pitches such as an equal-tempered chromatic or
diatonic scale, we have developed a novel method for extracting a
scale from an F0 envelope that is continuous (or at least very densely
sampled) in both time and pitch. Our method is inspired by Krumhansl's
time-on-pitch histograms adding up the total amount of time spent on
each pitch \cite{krumhansl90}. We demand a pitch resolution of one
cent \footnote{One cent is 1/100 of a semitone, corresponding to a
  frequency difference of about 0.06\%.} , so we cannot use a simple
histogram. \footnote{F0 envelopes of singing generally vary by much
  more than one cent even within a steadily held note, even if there
  is ``no vibrato.'' Another way of thinking about the problem is that
  there isn't enough data for so many histogram bins: if a 10-second
  phrase spans an octave (1200 cents) and our F0 envelope is sampled
  at 100 Hz then we have an average of less than one item per
  histogram bin.}
Instead we use a statistical technique known as \textit{nonparametric
  kernel density estimation}, with a Gaussian kernel.
\footnote{\ Thinking statistically, our scale is related to a
  distribution giving the relative probability of each possible
  pitch. We can think of each F0 estimate (i.e., each sampled value of
  the F0 envelope) as a sample drawn from this unknown distribution,
  so our problem becomes one of estimating the unknown distribution
  given the observations.}  The resulting curve is our density
estimate; like a histogram, it can be interpreted as the relative
probability of each pitch appearing at any given point in time. Figure
~\ref{fig:agora_histogram_scale} shows this method's density estimate
given the F0 curve from Figure ~\ref{fig:agora_inna_example}.

We interpret each peak in the density estimate as a note of the scale.
We restrict the minimum interval between scale pitches (currently 80
cents by default) by choosing only the higher peak when there are two
or more very close peaks. This method's free parameter is the standard
deviation of the Gaussian kernel, which provides an adjustable level
of smoothness to our density estimate; we have obtained good results
with a standard deviation of 30 cents. Note that this method has no
knowledge of octaves.

Once we have determined the scale, pitch quantization is the trivial
task of converting each F0 estimate to the nearest note of the scale.

In our opinion these derived scales are more true to the actual nature
of pitch-contour relationships within oral/aural and semi-notated
musical traditions. Instead of viewing these pitches to be deviations
of pre-existing ``normalized'' scales our method defines a more
differentiated scale from the outset. With our approach the scale
tones do not require ``normalization'' and thereby exist in an
autonomous microtonal environment defined solely on statistical
occurrence of pitch within a temporal unfolding of the given melodic
context.

\section{Interactive Web-Based Visualization and Exploration of
Melodic Contours}

We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways. The user
manually segments each recording into the appropriate units for each
chant type (such as trope sign, neumes, semantic units, or words). The
pitch contours of these segments can be viewed at different levels of
detail and smoothness using a histogram-based method. The segments
can also be rearranged in a variety of ways both manually and
automatically. That way one can compare the beginning and ending
pitches of any trope sign, neume or word or compare the relationships
of one neume or trope sign to its neighbors.

\begin{figure}[htb]
\includegraphics[width=80mm]{cantillion_interface}
\label{fig:cantillion_interface}
\caption{Screen-shot of interface} 
\end{figure} 

Our eventual goal is to explore the stability of melodic gesture and
pitch content in a variety of contexts both within a given chant style
and across chant styles. In addition we want to explore how the
stability related to the chant texts and textual syntax. There are
hard questions without clear answers. The user interface has been
design to assist and support the analysis conducted by expert
musicologists without trying to impose a specific approach. Being able
to categorize melodic formulae in a variety of ways allows for a
larger database of their gestural identities, their functionality to
parse syntax, and their regional traits and relations. A better
understanding of how pitch and contour helps to create gesture in
chant might allow for a more comprehensive view of the role of gesture
in improvised, semi-improvised and notated chant examples.

We have chosen to implement the interface as a web-based Flash
program, which can be accessed at http://cantillation.sness.net Web
interfaces can increase the accessibility and usability of a program,
make it easier to provide updates, and can enhance collaboration
between colleagues by providing functionality that lets researchers
more easily communicate their results to each other. The interface
(shown in Figure ~\ref{fig:cantillion_interface}) has four main
sections: a \textit{sound player}, a \textit{main window} to display
the pitch contours, a \textit{control window}, and a \textit{histogram
  window}.

The \textit{sound player} window displays a spectrogram representation
of the sound file with shuttle controls to let the user choose the
current playback position in the sound file. It also provides controls
to start and pause playback of the sound, to change the volume.

The \textit{main window} shows all the pitch contours for the song as
icons that can be repositioned automatically based on a variety of
sorting criteria, or alternatively can be manually positioned by the
user. The name of each segment (from the initial segmentation step)
appears above its F0 contour. The shuttle control of the main sound
player is linked to the shuttle controls in each of these icons,
allowing the user to set the current playback state either by clicking
on the sound player window, or directly in the icon of interest. When
the user mouses over these icons, some salient data about the sign is
displayed at the bottom of the screen.

The \textit{control window} has a variety of buttons that control the
sorting order of the icons in the main F0 display window. A user can
sort the icons in playback order, alphabetical order, length order,
and also by the beginning, ending, highest and lowest F0. The user can
also display the sounds in an X-Y graph, with the x-axis representing
highest F0 minus lowest F0, and the y-axis showing the ending F0 pitch
minus the beginning F0 pitch. Also in this section are controls to
toggle a mode to hear individual sounds when they are clicking on, and
controls to hide the pitch contour window leaving just the label.
There are also buttons allowing the user to choose to hear the
original sound file, the F0 curve applied to a sine wave, or the
quantized F0 curve applied to a sine wave.\footnote{\ The sine waves
  also follow the computed energy curves.}  When an icon in the main
F0 display window is clicked, the \textit{histogram window} shows a
histogram of the distribution of quantized pitches in the selected
sign. Below this histogram is a slider to choose how many of the
largest histogram bins will be used to generate a simplified contour
representation of the F0 curve. In the limiting case of selecting
\textit{all} histogram bins, the reduced curve is exactly the
quantized F0 curve. At lower values, only the histogram bins with the
most items are used to draw the reduced curve, which has the effect of
reducing the impact of outlier values and providing a smoother
``abstract'' contour.

Shift-clicking selects multiple signs; in this case the histogram
window includes the data from all the selected signs. We often select
all segments with the same word, trope sign, or neume; this causes the
simplified contour representation to be calculated using the sum of
all the pitches found in that particular sign, enhancing the quality
of the simplified contour representation.

Below the histogram window is a window that shows a zoomed-in graph of
the selected F0 contours. When more than one F0 contour is selected,
the lines in the graph are color coded to make it possible to easily
distinguish the different selected signs.

\section{Discussion and Future Work }

The identity of chant formulae in oral/aural chant traditions is to a
large extent determined by gesture/contour rather than by discrete
pitches. Computational approaches assist with the analysis of these
gestures/contours and enables the juxtaposition of multiple views at
different levels of detail in a variety of analytical (paradigmatic and
syntagmatic) contexts.

 The possibilities for such complex analysis methods would be difficult
if not impossible without such computer-assisted analysis. Employing
these tools we hope to better understand the role of and interchange
between melodic formulae in oral/aural and written chant cultures.
While our present analysis investigates melodic formulae primarily in
terms of their gestural content and semantic functionality, we hope
that these methods might allow scholars to reach a better understanding
of the historical development of melodic formulae within various chant
traditions. 

%**********************************************************************
%
% ISMIR2008
% 
%**********************************************************************


\section{Ismir2008}\label{sec:introduction}

Recordings of Afro-Cuban music challenge existing state-of-the-art
beat-tracking algorithms because of the complex and dense rhythm and
the lack of regular approximately isochronous pulses. Figure
~\ref{fig:tempocurves} shows how two recent state-of-the-art
beat-tracking systems (BeatRoot \cite{Dixon2007} and a beat tracker
using dynamic programming proposed by Ellis \cite{Ellis2007}) do not
generate an accurate tempo curve for the recording \textit{CB}.  The
plots in the figure are shown only in order to motivate the proposed
approach. The comparison is not fair, as the other algorithms are more
generally applicable and designed with different assumptions, but in
any case it demonstrates the advantage of a domain-specific method to
deal with these recordings: our method is specifically designed to
take into account clave as the rhythmic backbone.

\begin{figure}
% Crappy old way with a table of four pdf files:
% \begin{center}
% \begin{tabular}{c c}
% \includegraphics[width = 0.53 \linewidth] {groundtruth_tempocurve}
% & 
% \includegraphics[width = 0.53 \linewidth] {templatedp_tempocurve}
% \\
% \includegraphics[width=0.53 \linewidth] {beatroot_tempocurve}
% &
% \includegraphics[width= 0.53 \linewidth] {ellisdp_tempocurve}
% \\
%\end{tabular}
%\end{center}
%
% Matt's new single tiff file:
\includegraphics[width=3.33in]{unfair-comparison} 

\caption{Four estimates of the tempo curve for our recording \textit{CB}:  Ground truth calculated from a human expert's tap times (upper left),  curve from our method (top right), 
curve from BeatRoot (lower left), and curve from Ellis' dynamic programming approach (lower right).}
\label{fig:tempocurves}
\end{figure}

\section{Data preparation} 

It is common for Afro-Cuban songs to begin with just the sound of the
clave for one or two repetitions to establish the initial tempo.
However as other instruments (both percussive and pitched) and voices
enter the mix the sound of the clave tends to become masked.  The
first step of data preparation is to enhance the sound of the clave
throughout the song using a matched filter approach. In addition onset
detection is performed.

\subsection{Clave enhancement using Matched-Filtering} 

A matched filter detects or enhances the presence of an {\it a priori}
known signal within an unknown signal.  Its impulse response is a
time-reversed copy of the known signal, which in our case is the
beginning portion of one isolated clave note. The clave instrument
affords little timbral variety and therefore every note of clave in a
given recording sounds substantially like all the others, so a matched
filter made from any single note (frequently easily obtained from the
beginning of the song) will enhance the presence of the clave
throughout the song and suppress the remaining signal.  One free
parameter is the filter order, i.e., the duration of the segment of
the clave note; in each case we selected a ``good'' matched filter
experimentally by listening to the output of different
configurations. All the curves in Figure ~\ref{fig:tempocurves} and
results in this paper have been calculated on audio signals output by
matched filtering.

\subsection{Onset detection} 

Onset detection aims at finding the starting time of musical events
(e.g. notes, chords, drum events) in an audio signal; see 
 \cite{Bello2005},\cite{Dixon2006} for recent tutorials.
%However,
%polyphonic music poses an increased challenge since nominally
%simultaneous notes might be spread over tens of milliseconds, turning
%the definition of onsets ambiguous. Similarly, it is hard to define a
%precise onset time for sounds with slow attacks.
%
%In a recent tutorial article, Dixon revisited the problem of onset
%detection \cite{Dixon2006}, where a number of onset detection
%algorithms were reviewed and compared on two datasets. This study was
%itself based on a previous article in which a theoretical and
%empirical comparison of several state-of-the-art onset detection
%approaches is presented \cite{Bello2005}. Following the findings and
%results in~\cite{Dixon2006}, the approach used in this work is based
%on the use of the 
We used \textit{spectral flux} as the onset detection function,
defined as:

\begin{equation}
SF(n) = \sum_{k=0}^{N/2}{HWR(|X(n,k)|-|X(n-1,k)|)}
\end{equation}

\noindent
where $HWR(x)=\frac{x+|x|}{2}$ is the half-wave rectifier function,
$X(n,k)$ represents the $k$-th frequency bin of the $n$-th frame of
the power magnitude (in dB) of the short time Fourier transform, and
$N$ is the corresponding Hamming window size. For the experiments
performed in this work all data had a sampling rate $f_s=44100$ Hz and
we used a window size of 46 ms ($N=2048$) and a hop size of about 11ms
($R=512$). The onsets are subsequently detected from the spectral flux
values by a causal peak-picking algorithm that finds local maxima as
follows.  A peak at time $t=\frac{nR}{f_s}$ (the time of the beginning
of the $n$th frame) is selected as an onset if it fulfills the
following conditions:

\begin{enumerate}
\item $SF(n)\geq SF(k) \;\; \forall k \; : n-w \leq k \leq n+w$
\item $SF(n)> \frac{\sum_{k=n-mw}^{n+w}{SF(k)}}{mw+w+1}\times thres + \delta$
\end{enumerate}

\noindent
where $w=6$ is the size of the window used to find a local maximum,
$m=4$ is a multiplier so that the mean is calculated over a larger
range before the peak, $thres = 2.0$ is a threshold relative to the
local mean that a peak must reach in order to be sufficiently
prominent to be selected as an onset, and $\delta=10^{-20}$ is a
residual value to avoid false detections on silent regions of the
signal. All these parameter values were derived from preliminary
experiments using a collection of music signals with varying onset
characteristics.

In order to reduce the false detection rate, we smooth the detection
function $SF(n)$ with a Butterworth filter to reduce the effect of
spurious peaks:

\begin{equation}
H(z)=\frac{0.1173 + 0.2347z^{-1} + 0.1174z^{-2}}{1-0.8252z^{-1}+0.2946z^{-2}} 
\label{eq:ButtFilt}
\end{equation}

\noindent
(These coefficients were found by experimentation based on the
findings in \cite{Bello2005},\cite{Dixon2006}.)  In order to avoid
phase distortion (which would shift the detected onset time away from
the $SF(n)$ peak) the signal is filtered in both the forward and
reverse directions.

\section{Template-based tempo tracking}\label{sec:template_tempo}

We propose a new method to deal with the challenges of beat tracking
in Afro-Cuban music. The main idea is to use domain specific
knowledge, in this case the clave pattern, directly to guide the
tracking. The method consists of the following four basic steps: 1)
Consider each detected onset time as a potential note of the clave
pattern. 2) Exhaustively consider every possible tempo (and clave
rotation) at each onset by cross-correlating each of a set of
clave-pattern templates against an onset strength envelope signal
beginning at each detected onset. 3) Interpret each cross-correlation
result as a score for the corresponding tempo (and clave rotation)
hypothesis. 4) Connect the local tempo and phase estimates to provide
a smooth tempo curve and deal with errors in onset detection, using
dynamic programming.

The idea of using dynamic programming for beat tracking was proposed
by Laroche \cite{Laroche2003}, where an onset function was compared to
a predefined envelope spanning multiple beats that incorporated
expectations concerning how a particular tempo is realized in terms of
strong and weak beats; dynamic programming efficiently enforced
continuity in both beat spacing and tempo.  Peeters \cite{Peeters2007}
developed this idea, again allowing for tempo variation and matching
of envelope patterns against templates. An approach assuming
constant tempo that allows a simpler formulation at the cost of more
limited scope has been described by Ellis \cite{Ellis2007}. 

\subsection{Clave pattern templates}

At the core of our method is the idea of using entire rhythmic
patterns (templates) for beat tracking rather than individual
beats. First we construct a template for each possible tempo.  We take
the ideal note onset times in units of beats (e.g., for rumba clave,
the list 0, 0.75, 1.75, 2.5, 3) and multiply them by the duration of a
beat at each tempo, giving ideal note onset times in seconds.  We
center a Gaussian envelope on each ideal note onset time to form the
template.  The standard deviation (i.e., width) of these Gaussians is
a free parameter of this method.  Initial results with a constant
width revealed a bias towards higher tempi,
%%% XXX Say why!
so widths are specified in units of beats, i.e., we scale the width
linearly with tempo. Better results were obtained by making each
template contain multiple repetitions of the clave, e.g., three
complete patterns. Figure ~\ref{fig:templates} shows a visual
representation of the template rotations for all considered tempi. 

\begin{figure}[ht]
\includegraphics[width= \linewidth]{templates_inbeats} 
\caption{Clave Templates for all rotations and tempi}
\label{fig:templates}
\end{figure}

With a 5-note clave pattern, any given note played by the clave could
be the 1st, 2nd, 3rd 4th or 5th note of the pattern.  Therefore we
make templates for all ``rotations'' of the clave, i.e., for the
repeating pattern as started from any of the five notes.  For example,
rotation $0$ of rumba clave is [0, 0.75, 1.75, 2.5, 3], and rotation
$1$ (starting from the second note) is [0.75 1.75 2.5 3 4] - 0.75 = [0
  1 1.75 2.25 3.25].  Time $0$ always refers to the onset time of the
current note.

\noindent
\begin{figure}[ht]
\includegraphics[width=\linewidth]{template-match-example} 
\caption{Matching different templates (filled grey) to the \textit{CB} recording's energy envelope (black line).  The X-axis is time (seconds), with zero the time of the onset under consideration. The score for each template match represents how well that template lines up with the energy envelope.} 
\label{fig:templates_matching}
\end{figure}

We cross-correlate (in other words, take the dot product of) these
templates against segments of an onset strength envelope (in our case,
simply the total energy in each 1024-sample window of the matched
filter output) beginning at the time of each detected onset.  We
interpret the dot product between the onset strength signal ${O(t)}$
and a template $T_{j,k} (t)$ with tempo $j$ and rotation $k$ as the
strength of the hypothesis that the given onset is the given note of
clave at the given tempo.  Figure ~\ref{fig:templates_matching}
depicts this process for some tempi and rotations and the
corresponding scores.  We exhaustively compute these dot products for
every candidate tempo $j$ (e.g,. from 95 to 170 BPM in 1 BPM
increments), for all five rotations of the clave pattern $k$ , for
every detected onset $i$ at time $t_{i}$ to produce a {\it score
  grid}:

\begin{equation} 
score(i, j, k) = \sum_{t=0} ^{LT_{j,k}-1} T_{j,k}(t) 
%\times 
O(t_{i} + t)
\end{equation} 
\noindent
where $LT_{j,k}$ is the length of template $T_{j,k}$ . 

\subsection{Rotation-blind dynamic programming}

It is trivial to look at a given onset, pick the tempo and rotation
with the highest score, and call that the short-term tempo estimate.
However, due to the presence of noise, inevitable onset detection
errors, and the matched filter's far-from-perfect powers of auditory
source separation, simply connecting these short-term tempo estimates
does not produce a usable estimate of the tempo curve.  Better results
can be achieved by explicitly discouraging large tempo changes. We use
dynamic programming \cite{Bellman1957} as an efficient means to
estimate the best tempo path (i.e., time-varying tempo).  In the next
section we will consider the rotations of the template; for now let
the ``rotation-blind'' score be:

\begin{equation} 
scoreRB(i, j) = max(score(i, j,k))\; k:1..5
\end{equation} 

We convert each score $scoreRB$ to a cost $C_{i,j}$ with a linear
remapping so that the highest score maps to cost $0$ and the lowest
score maps to cost $1$. We define a {\it path} $P$ as a sequence of
tempo estimates (one per onset), so that $P(i)$ is P's estimate of the
tempo at time $t_{i}$.  Our algorithm minimizes the {\it path cost}
$PC$ of the length $n$ path $P$:

\begin{equation} 
PC(P) = \sum_{i=0:n-1}C_{i,P(i)} + \sum_{i=0:n-2}F(P(i), P(i+1))
\end{equation} 
where $F(tempo_1,tempo_2)$ is a ``tempo discontinuity cost function''
expressing the undesirability of sudden changes in tempo. $F$ is
simply a scalar times the absolute difference of the two
tempi. Dynamic programming can efficiently find the lowest-cost path
from the first onset to the last because the optimal path up to any
tempo at time $t_{i}$ depends only on the optimal paths up to time
$t_{i-1}$.  We record both the cost $PC(i,j)$ and the previous tempo
$Previous(i,j)$ for the best path up to any given onset $i$ and tempo
$j$.

% We initialize the path cost for each tempo $j$ at time $t_{0}$ to
% be simply ${C_{0,j}}$.  Then we advance sequentially through the
% onsets, at each point considering all possible tempi for both the
% current and previous onsets:


% \begin{verbatim} 
% for i = 2 to num_onsets
%   for j_now = 1 to num_tempi
%     lowest_cost = \inf
%     best_previous_tempo = NULL
%     for j_before = 1 to num_tempi
%       this_cost = PC(i-1,j_before) + C(i,j_now) + F(j_before,j_now)
%       if this_cost < lowest_cost
%         lowest_cost = this_cost
%         best_previous_tempo = j_before
%     Previous(i,j_now) = best_previous_tempo;
%     PC(i,j_now) = lowest_cost;
% \end{verbatim} 

% Although this algorithm is quadratic in the number of tempi, it is
% only linear in the number of onsets (which should be approximately
% proportional to the song's duration).



\subsection{Rotation-aware dynamic programming} 

Now we will extend the above algorithm to consider rotation, i.e., our
belief about which note of clave corresponds to each onset.  Now our
cost function $C_{i,j,k}$ is also a function of the rotation $k$.  Our
path tells us both the tempo $P_{tempo}(i)$ at time $t_{i}$ and also
the rotation $P_{rot}(i)$, so we must keep track of both previous
$Previous_{tempo}(i,j)$ and $Previous_{rot}(i,j)$ (corresponding to
the best path up to $i$ and $j$).  Furthermore, considering rotation
will also give us a principled way for the path to skip over ``bad''
onsets, so instead of assuming that every path reaches onset $i$ by
way of onset $i-1$ we must also keep track of $Previous_{onset}(i,j)$.

The key improvement in this algorithm is the handling of rotation.
Rotation (which indexes the notes in the clave pattern) is converted
to {\it phase}, the proportion (from $0$ to $1$) of the distance from
one downbeat to the next.  (So the phases for the notes of rumba clave
are [0, 0.1875, 0.4375, 0.625, 0.75]).  The key idea is predicting
what the phase of the next note ``should be'': Given phase $\phi_1$
and tempo $j_1$ for onset $i_1$, a candidate tempo $j_2$ for onset
$i_2$, and the time between onsets $\Delta T = t_2-t_1$, and assuming
linear interpolation of tempo during the (short) time between these
nearby onsets, we can use the fact that tempo (beat frequency) is the
derivative of phase to estimate the phase $\hat\phi_2$:

\begin{equation} 
\hat\phi_2 = \phi_1 + \Delta T ((j_1+j_2)/2)/(4 \times 60)
\end{equation}
Dividing by $4 \times 60$ converts from BPM to bars per second.

Now we can add an extra term to our cost function to express the
difference between the predicted phase $\hat\phi_2$ and the actual
phase $\phi_2$ corresponding to the rotation of whatever template
we're considering for the onset at time $t_2$ (being careful to take
this difference modulo 1, so that, e.g., the difference between 0.01
and .98 is only 0.03, not 0.97).  We'll call this phase distance the
``phase residual'' $R$, and add the term $\alpha * R$ to our cost
function.

Now let's consider how to handle ``false'' detected onsets, i.e.,
onsets that are not actually notes of clave.  For onset $n$, we
consider not just onset $n-1$ as the previous onset, but every onset
$i$ with $t_i > t_n - K$, i.e., every onset within $K$ seconds before
onset $n$, where $K$ is set heuristically to 1.5 times the largest
time between notes of clave (one beat) at the slowest tempo.  We
introduce a ``skipped onset cost'' $\beta$ and include $\beta \times
(n-i-2)$ in the path cost when the path goes from onset $i$ to onset
$n$.

\begin{table} 
\begin{center} 
\begin{tabular}{|c|c|c|c|c|c|c|} 
\hline 
       & LP & CB & CH    & PD        & LPWT & PDWT \\ 
RB  & 40 & 26 & 22.9  &  63.1    &  39.96 &  62.7           \\
\hline     
RA  & 1.75 & 11 & 1.54 &  3.10    &  2.043 &    57.9 \\
\hline 
\end{tabular} 
\end{center}
\caption{RMS (in BPM) results for tempo curve estimation}
\label{table:rms}
\end{table} 

Table ~\ref{table:rms} shows the Root-mean-square (RMS) error between
the ground truth tempocurve and the tempocurves estimated by the
rotation-blind (RB) and rotation-aware (RA) configurations of our
method.  In all cases the rotation-aware significantly outperforms the
rotation-blind method (which usually tracks correctly only parts of
the tempo curve).  The first three recordings (LP, CB, CH) have
rumba-clave and the fourth piece (PD) has son-clave.  The last two
columns show the results when using the ``wrong'' template.
Essentially when the template is not correct the matching cost of the
beat path is much higher and the tempo curve estimation is wrong.
Figure ~\ref{fig:rotcomp} shows the score grid for the rotation-blind
(top) and rotation-aware (bottom) configurations overlaid with the
estimated and ground truth tempocurves.

\begin{figure}[ht]
\centering
\begin{tabular}{c} 
\includegraphics[width= 0.8 \linewidth]{rotation_blind} \\
\includegraphics[width= 0.8 \linewidth]{rotation_aware} \\
\end{tabular} 
\caption{Rotation-blind (top) and rotation-aware (bottom) beat tracking}
\label{fig:rotcomp}
\end{figure}

\section{Bar-Wrapping Visualization}\label{sec:visualization}

\begin{figure}[ht]
\centering
\includegraphics[width= \linewidth]{barwrap-stretched} 
\caption{Bar-wrapping visualization}
\label{fig:barwrap}
\end{figure}

A performance typically consists of about 625-1000 clave
``notes''. Simply plotting each point along a linear time axis would
require either excessive width, or would make the figure too small to
see anything; this motivates bar wrapping. Conceptually, we start by
marking each event time (in this case, each detected onset) on a
linear time axis. If we imagine this time axis as a strip of magnetic
tape holding our recording, then metaphorically we cut the tape just
before each downbeat, so that we have 200 short pieces of tape, which
we then stack vertically, so that time reads from left to right along
each row, and then down to the next row, like text in languages such
as English.  Each of these ``strips'' is then stretched horizontally
to fill the figure width, adding a tempo curve along the right side to
show the original duration of each bar.  Figure ~\ref{fig:barwrap}
depicts the times of our detected onsets for {\it LP} with this
technique. The straight lines show the theoretical clave locations. By
looking at the figure one can notice that the 5th clave note is
consistently slightly later than the theoretical location. This would
be hard to notice without precise estimation of the tempocurve.

Rotation-aware dynamic programming is used to find the downbeat times.
An explicit downbeat estimate occurs whenever the best path includes a
template at rotation $0$.  But there might not be a detected onset at
the time of a downbeat, so we must also consider implicit downbeats,
where the current onset's rotation is not $0$ but it is lower than the
rotation of the previous onset in the best path.  The phase is
interpolated to estimate the downbeat time that ``must have occurred''
between the two onsets.

\section{Discussion and Conclusions} 

Our beat-tracking method works particularly well for Afro-Cuban clave
for many reasons: 1) The clave part almost never stops in traditional
Afro-Cuban music (although it can be hard to hear when many other
percussion instruments are playing).\footnote{Our method's phase- and
  tempo-continuity constraints allow it to stay on track in the face
  of extra or missing onsets and occasional unduly low template match
  scores, so we expect that it would still perform correctly across
  short gaps in the clave part.}  2) The clave pattern almost never
changes in Afro-Cuban music.\footnote{One subtlety of Afro-Cuban music
  is the notion of ``3-2'' versus ``2-3'' clave, which refers to a
  180-degree phase shift of the clave part with respect to the
  ensemble's downbeat.  Our method has no notion of the ensemble's
  downbeat and ``doesn't care'' about this distinction.  Some songs
  change between 3-2 and 2-3 in the middle, but never by introducing a
  discontinuity in the clave part (which would be a problem for our
  algorithm); instead the other instruments generally play a phrase
  with two ``extra'' beats that shifts their relationship to the
  clave.}  3) The clave instrument produces an extremely consistent
timbre with every note, so matched filtering does a good job
emphasizing it.\footnote{In rare cases a different instrument carries
  the clave part; this should not be a problem for our method as long
  as a relatively isolated sample can be located.}  4) Songs often
begin with the clave alone, making it easy to construct our matched
filter.\footnote{As future work we would like to explore the
  possibility of creating a ``generic'' clave enhancement filter that
  doesn't rely on having an isolated clave note in every recording, a
  weakness of the current method.}  5) The clave plays one of a few
predetermined syncopated parts, favoring the use of predefined
templates rather than assumptions of isochrony.

There are many future work directions. Rhythmic analysis can be used
to categorize recordings into different styles and possibly identify
particular artists or even percussionists. We also plan to apply the
method to more recordings and continue working with ethnomusicologists
and performers interested in exploring timing. It is our belief that
our template-based rotation-aware formulation can also be applied to
popular music by utilizing different standard drum patterns as
templates. All the code implementing the method can be obtained by
emailing the authors.

\bibliographystyle{plain}
\bibliography{ieee2008aich}

%----------------------------------------------------------------------

\begin{biography}{Steven R. Ness} 
Steven Ness Bio
\end{biography}

\begin{biography}{Matthew Wright}
Matthew Wright Bio
\end{biography}

\begin{biography}{Daniel P. Biro} 
Daniel P. Biro Bio
\end{biography}

\begin{biography}{Andrew Schloss} 
Andrew Schloss Bio
\end{biography}

\begin{biography}{George Tzanetakis} 
George Tzanetakis Bio
\end{biography}

\end{document}
\documentclass[%
	%draft,
	%submission,
	%compressed,
	final,
	%
	%technote,
	%internal,
	%submitted,
	%inpress,
	reprint,
	%
	%titlepage,
	notitlepage,
	%anonymous,
	narroweqnarray,
	inline,
	twoside,
        invited,
	]{ieee2009gtzan_orca}

\newcommand{\latexiie}{\LaTeX2{\Large$_\varepsilon$}}

%\usepackage{ieeetsp}	% if you want the "trans. sig. pro." style
%\usepackage{ieeetc}	% if you want the "trans. comp." style
%\usepackage{ieeeimtc}	% if you want the IMTC conference style

% Use the `endfloat' package to move figures and tables to the end
% of the paper. Useful for `submission' mode.
%\usepackage {endfloat}

% Use the `times' package to use Helvetica and Times-Roman fonts
% instead of the standard Computer Modern fonts. Useful for the 
% IEEE Computer Society transactions.
%\usepackage{times}
% (Note: If you have the commercial package `mathtime,' (from 
% y&y (http://www.yandy.com), it is much better, but the `times' 
% package works too). So, if you have it...
%\usepackage {mathtime}

% for any plug-in code... insert it here. For example, the CDC style...
%\usepackage{ieeecdc}

\begin{document}

%----------------------------------------------------------------------
% Title Information, Abstract and Keywords
%----------------------------------------------------------------------
\title{Collaborative Tools for Computational Ethnomusicology}

% format author this way for journal articles.
% MAKE SURE THERE ARE NO SPACES BEFORE A \member OR \authorinfo
% COMMAND (this also means `don't break the line before these
% commands).
%  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
\author[NESS, WRIGHT, BIR\'{O}, SCHLOSS AND TZANETAKIS]{
	Steven R. Ness,
	\authorinfo{
		S.\,R.\,Ness is with the Department Computer Science, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
\and{}
	D\'{a}niel P\'{e}ter Bir\'{o},
	\authorinfo{
		D.\,P.\,Biro is with the School of Music, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
\and{}and
	George Tzanetakis
	\authorinfo{
		G.\,Tzanetakis is with the Department Computer Science, University
       of Victoria, BC, Canada V8W 3P6.
       Phone: $+$1\,250\, 472--5700, e-mail: sness@sness.net}
}


\journal{IEEE Intelligent Systems}
\titletext{Special issue on AI and Cultural Heritage}
\ieeecopyright{0018--9456/97\$10.00 \copyright\ 2008 IEEE}
\lognumber{xxxxxxx}
\pubitemident{S 0018--9456(97)09426--6}
\loginfo{Manuscript received August 15th, 2008}

\maketitle               

\begin{abstract} 
\end{abstract}

\begin{keywords}
keywords
\end{keywords}


%----------------------------------------------------------------------
% SECTION I: Introduction
%----------------------------------------------------------------------

\section{Introduction}

\bibliographystyle{plain}
\bibliography{ieee2009gtzan_orca}

%----------------------------------------------------------------------

\begin{biography}{Steven R. Ness} 
Steven Ness Bio
\end{biography}

\begin{biography}{Daniel P. Biro} 
Daniel P. Biro Bio
\end{biography}

\begin{biography}{George Tzanetakis} 
George Tzanetakis Bio
\end{biography}

\end{document}
% -----------------------------------------------
% Template for ISMIR 2008
%     ismir2008.sty -> style file
% Modified by Juan P. Bello (ismir2008-papers@ismir.net)
% By Rainer Typke (ismir07.rainer@safersignup.com)
% Based on the 2004 template by Eloi Batlle.
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2008,amsmath}
% To use when using pdflatex
\usepackage{graphicx}
% To use when using latex, dvips and ps2pdf
% \usepackage[dvips]{graphicx}

% Title.
% ------
\title{Paper Template for ISMIR 2008}

% IMPORTANT NOTICE:
% Reviews are double-blind
% Author information should be kept hidden from reviewers
% Please avoid evident self references in the text

\begin{document}
%
\maketitle
%
\begin{abstract}
The abstract should be placed at the top left column and should contain
about 150-200 words.
\end{abstract}

\section{Introduction}\label{sec:introduction}

This template includes all the information about formatting manuscripts
for the ISMIR 2008. Please follow this guidelines to give the final
proceedings a uniform look. If you have any questions, please
contact the Conference Management. 

This template can be downloaded
from the ISMIR 2008 web site (\texttt{http://ismir2008.ismir.net/}).

\section{Pdflatex versus Latex/dvips}
You can prepare a PDF file in two ways: either directly with pdflatex (using
Template.tex) or with latex, dvips, and ps2pdf (using Template\_ps.tex). The
latter method allows you to include Postscript images, while the former
works nicely with PNG or PDF images, just to name two examples.

\section{Page Size}\label{sec:page_size}

The proceedings will be printed on letter-size paper (21.6 x 27.9 cm or
8.5" x 11").
All material on each page should fit within a rectangle of 7" x 9" (17.78~cm x 22.86~cm), 
centered on the page, beginning 1'' (2.54~cm)
from the top of the page and ending with 1'' (2.54~cm) from the
bottom. The left and right margins should be 0.75'' (1.9~cm).
The text should be in two 3.3" (8.4~cm) columns with a 0.4'' (1~cm) gutter.
All {\it text} must be in a two-column format. Text must be
fully justified.


\section{Typeset Text}\label{sec:typeset_text}

\subsection{Normal or Body Text}\label{subsec:body}

Please use a 10pt (point) Times font, or other Roman font with serifs,
as close as possible in appearance to Times. Please use sans-serif or
non-proportional fonts only for special purposes, such as distinguishing
source code text.

The first paragraph in each section should not be indented, but all
other paragraphs should be.

\subsection{Title and Authors}

The title is 14pt Times, bold, caps, upper case, centered.
Authors' names are centered. The lead author's name is to be listed first
(left-most), and the co-authors' names after. If the addresses for all authors
is the same, include the address only once, centered. If the authors have
different addresses, put the addresses, evenly spaced, under each authors' name.

\subsection{Page Numbering, Headers and Footers}

Do not include headers, footers or page numbers in your submission.
These will be added when the publications are assembled.

\section{First Level Headings}

First level headings are in Times 10pt bold, centered with 1 line
of space above the section head, and 1/2 space below it.  For a section
header immediately followed by a subsection header, the space should be merged.

\subsection{Second Level Headings}

Second level headings are in Times 10pt bold, flush left, with 1 line of
space above the section head, and 1/2 space below it. The first letter of
each significant word is capitalised.

\subsubsection{Third and Further Level Headings}

Third level headings are in Times 10pt italic, flush left, with 1/2 line
of space above the section head, and 1/2 space below it. The first letter
of each significant word is capitalised.

Using more than three levels of headings is highly discouraged.

\section{Footnotes and Figures}

\subsection{Footnotes}

Indicate footnotes with a number in the text.\footnote{This a footnote}
Use 8pt type for footnotes. Place the footnotes at the bottom of the page
on which they appear. Precede the footnote with a 0.5pt horizontal rule.

\subsection{Figures, Tables and Captions}

All artwork must be centered, neat, clean, and legible. All lines should
be very dark for purposes of reproduction and art work should not be hand-drawn.
The proceedings is not in color, and therefore all figures must make
sense in black-and-white form.
Figure and table numbers and captions always appear below the figure. Leave 1
line space between the figure or table and the caption. Each figure or table
is numbered consecutively. Captions should be Times 10pt.
Place tables/figures in text as close to the reference as possible.
References to figures and tables should be capitalised, for example:
see Figure \ref{fig:example} and Table \ref{tab:example}.
Figures and tables may extend across both columns to a maximum
width of 7'' (17.78~cm).

\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
String value & Numeric value \\
\hline
hello ismir  & 1073 \\
\hline
\end{tabular}
\end{center}
\caption{Table captions should be placed below the table}
\label{tab:example}
\end{table}

\begin{figure}
\centerline{\framebox{
% To use when using pdflatex
	\includegraphics[width=\columnwidth]{figure.png}}}
	% To use when using latex, dvips and ps2pdf
% 	\includegraphics[width=\columnwidth]{figure.eps}}}
\caption{Figure captions should be placed below the figure}
\label{fig:example}
\end{figure}

\section{Equations}

Equations should be placed on separated lines and numbered.
The number should be on the right side, in parentheses.

\begin{equation}
E=mc^{2}
\end{equation}

\section{Citations}

All bibliographical references should be listed at the
end, inside a section named ``REFERENCES'', numbered
and in alphabetic order. Also, all
references listed should be cited in the text.
When refering to a document, type the number in
square brackets~\cite{Author:00}.

\begin{thebibliography}{citations}

\bibitem {Author:00} Author, E.
``The title of the conference paper'',
{\it Proceedings of the International Symposium on Music Information
Retrieval}, Plymouth, USA, 2000.

\bibitem{Someone:02} Someone, A.
{\it  Title of the book}.
Editorial Acme, Barcelona, 2004.

\end{thebibliography}

\end{document}
How can we visualize the timing of the clave notes in pieces like these?
%In this genre, a typical performance is about 5 minutes
%long with tempos around 100-160 BPM, equivalent to
%125-200 four-beat measures. With five notes per bar, a
A
clave performance 
%therefore 
typically consists of about
625-1000 time points. Simply plotting each point along
a linear time axis would require either excessive width,
or would make the figure too small to see anything; this
motivates bar wrapping. Conceptually, we start by
marking each event time (in this case, each detected onset) on a linear time axis. If we
imagine this time axis as a strip of magnetic tape holding
our recording, then metaphorically we cut the tape just
before each downbeat, so that we have 200 short pieces
of tape, which we then stack vertically, so that time
reads from left to right along each row, and then down to
the next row, like text in languages such as English.  We can
then stretch each of these ``tape strips'' horizontally to fill
the entire width of the figure, adding a tempo curve along the right
side to tell us the original duration of each bar.  Figure XXXbarwrapping shows 
the times of our detected onsets for \it{La Polemica} with this technique.

We use our phase-aware dynamic programming algorithm to find the time
of each downbeat. An explicit downbeat estimate occurs whenever the
best path includes a template at rotation zero.  But there might not
be a detected onset at the time of a downbeat, so we must also
consider implicit downbeats, where the current onset's rotation is not
zero but it is lower than the rotation of the previous onset in the
best path.  In this case we interpolate the phase to estimate the time
of the downbeat that ``must have occurred'' between the two onsets.

% -----------------------------------------------
% Template for ISMIR 2007
%     ismir.sty -> style file
% By Rainer Typke (ismir07.rainer@safersignup.com)
% Based on the 2004 template by Eloi Batlle.
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath}
\usepackage{graphicx}
\usepackage{url}

% Title.
% ------
\title{Stereo Panning Features for Classifying Recording Production Style}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
%  {Author} {School \\ Department}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\oneauthor
  {George Tzanetakis, Randy Jones, and Kirk McNally} {University of Victoria \\ Computer Science \\ gtzan@cs.uvic.ca, rj@csc.uvic.ca, kmcnally@uvic.ca}

\begin{document}
%
\maketitle
%
\begin{abstract}


  Recording engineers, mixers and producers play important yet often
  overlooked roles in defining the sound of a particular record,
  artist or group. The placement of different sound sources in space
  using stereo panning information is an important component of the
  production process. Audio classification systems typically convert
  stereo signals to mono and to the best of our knowledge have not
  utilized information related to stereo panning. In this paper we
  propose a set of audio features that can be used to capture stereo
  information. These features are shown to provide statistically
  important information for non-trivial audio classification tasks
  and are compared with the traditional Mel-Frequency Cepstral
  Coefficients. The proposed features can be viewed as a first attempt
  to capture extra-musical information related to the production
  process through music information retrieval techniques.

\end{abstract}
%

\section{Introduction}

Starting in the 1960s the recording process for rock and popular music
moved beyond the convention of recreating as faithfully as possible
the illusion of a live performance. Facilitated by technological
advances including multi-track recording, tape editing, equalization
and compression, the creative contributions of record producers became
increasingly important in defining the sound of artists, groups, and
styles \cite{Moorefield2005}. Although not as well known as the
artists they worked with, legendary producers including Phil Spector,
George Martin, Quincy Jones, and Brian Eno changed the way music was
created.

So far, research in music information retrieval has largely ignored
information about the recording process, focusing instead on
capturing information about pitch, rhythm and timbre. A common
methodology is to extract features, quantifiable attributes of music
signals, from recordings, then to classify these features into
distinct groups using machine learning techniques. This two-part
process has enabled tasks such as automatic identification of genres, 
albums and artists. 

The influence of the recording process on automatic classification has
been acknowledged and termed the \emph{album effect.} The performance
of artist identification systems degrades when music from different
albums is used for training and evaluation
\cite{whitmanNNSP2001}. Therefore, the classification results of such
systems are not based entirely on the musical content.  Various stages
of production of the recorded artifact, including recording, mixing,
and mastering, all have the potential to influence classification.
This has led to research which attempts to quantify the effects of
production on acoustic features. By detecting equalization curves used
in album mastering, it is possible to compensate for the effects of
mastering so that multiple instances of the same song on different
albums can be better compared \cite{kim_tqa}. We believe that other
information related to the recording process, specifically mixing, is
an important component of understanding modern pop and rock music and
should be incorporated rather than being removed from music
information retrieval systems.

Our goal is to explore stereo panning information as an
aspect of the recording and production process. Stereo information has
been utilized for source separation purposes
\cite{avendanoWASPAA2003, woodruffISMIR2006}.  However, to the best of
our knowledge, it has not been used in classification systems
for audio signals. In this paper we show that stereo panning
information is indeed useful for automatic music classification.


\section{Stereo Panning Information Extraction} 

In this section we describe the process of calculating stereo panning
information for different frequencies based on the short-time Fourier
transform (STFT) of the left and right channels. Using the extracted
Stereo Panning Spectrum we propose features for classification.

\subsection{Stereo Panning Spectrum} 

Avendano \cite{avendanoWASPAA2003} describes a frequency-domain source
identification system based on a cross-channel metric called the {\it
  panning index}. We use the same metric as the basis for calculating
stereo audio features for classification. For the remainder of the
paper the term {\it Stereo Panning Spectrum (SPS)} is used instead of
the {\it panning index} as we feel it is a more accurate term. The SPS
holds the panning values (between -1 and +1 with 0 being center) for
each frequency bin.

The derivation of the SPS assumes a simplified model of the stereo
signal. In this model each sound source is recorded individually and
then mixed into a single stereo signal by amplitude panning.  Stereo
reverberation is then added artificially to the mix. The basic idea
behind the SPS is to compare the left and right signals in the
time-frequency plane to derive a two-dimensional map that identifies
the different panning gains associated with each time-frequency bin.
By selecting time-frequency bins with similar panning it is possible
to separate particular sources \cite{avendanoWASPAA2003}. In this
paper we utilize the SPS directly as the basis for extracting
statistical features without attempting any form of source
separation. Our SPS definition directly follows Avendano
\cite{avendanoWASPAA2003}.

If we denote the STFT of the left,right signals $x_{l}(t), x_{r}(t)$
for a particular analysis window as $X_{l}(k), X_{r}(k)$, where k is
the frequency index we can define the following similarity measure:

\begin{equation} 
\psi(k) = 2 * \frac{|X_{l}(k) X_{r}^{*}(k)|}{|X_{l}(k)|^{2} + |X_{r}(k)|^{2}}
\end{equation}

\noindent
where $*$ denotes complex conjugation. For a single source with
amplitude panning the similarity function will have a value
proportional to the panning coefficient $\alpha$ in those time
frequency regions where the source has energy. More specifically if we
assume the sinusoidal energy-preserving panning law: $a_{r} =
\sqrt{1-a_{l}^{2}}$ then:

\begin{equation} 
  \psi(k) = 2 \alpha \sqrt{1-\alpha^{2}}
\end{equation}

\noindent
If the source is panned to the center (i.e $\alpha = 0.7071$) then the function 
will attain its maximum value of 1, and if the source is completely panned to either side 
the function will attain its minimum value of zero. The ambiguity with regards 
to the later direction of the source can be resolved using the partial similarity measures: 

\begin{equation} 
\psi_{l}= \frac{|X_{l}(k) X_{r}^{*}(k)|}{|X_{l}(k)|^{2}},  \psi_{r}= \frac{|X_{r}(k) X_{l}^{*}(k)|}{|X_{r}(k)|^{2}}
\end{equation}

\noindent
and their difference: 
\begin{equation} 
\Delta(k) = \psi_{l} - \psi_{r}
\end{equation} 

\noindent
where positive values of $\Delta(k)$ correspond to signals panned towards the left and 
negative values correspond to signals panned to the right. Thus we can define the following 
ambiguity-resolving function: 

\begin{equation} 
\hat\Delta(k) = \begin{cases} 
  +1, &\text{if $\Delta(k) > 0$} \\
  \;\; 0, &\text{if $\Delta(k) = 0$} \\
  -1, &\text{if $\Delta(k) < 0$} \\
  \end{cases}
\end{equation}

\noindent
Shifting and multiplying the similarity function by $\hat\Delta(k)$ we obtain 
the Stereo Panning Spectrum (or panning index) as: 

\begin{equation} 
SPS(k) = [1 - \psi(k)] * \hat\Delta(k)
\end{equation} 

Figure ~\ref{fig:hellsbells} shows a visualization of the Stereo
Panning Spectrum for the song ``Hell's Bells'' by ACDC. The
visualization is similar to a Spectrogram with the X-axis
corresponding to time, measured in number of analysis frames, and the
Y-axis corresponding to frequency bin. No panning is represented by
gray, full left panning by black and full right panning by white. The
songs starts with four bell sounds that alternate between slight
panning to the left and to the right, visible as changes in grey
intensity. Near the end of the first 28 seconds a strong electric
guitar enters on the right channel, visible as white. 

Figure ~\ref{fig:supervixen} shows a visualization of the SPS for the
song ``Supervixen'' by Garbage. Several interesting stereo
manipulations can be observed in the figure and heard when listening
to the song. The song starts with all instruments centered for a brief
period and then moves them to the left and right creating an explosion
like effect.  Most of the sound of a fast repetitive hi-hat is panned
to the right (the wide dark bar over the narrow horizontal white bar)
with a small part of it panned to the left (the narrow horizontal
white bar). Near the end of the first 28 seconds the voice enters with
the a crash cymbal panned to the left, visible as the large black
area.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{hellsbells} 
  \caption{\it Stereo Panning Spectrum of ``Hell's Bells'' by ACDC (approximately 28 seconds).}
  \label{fig:hellsbells}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{supervixen} 
  \caption{\it Stereo Panning Spectrum of ``Supervixen'' by Garbage (approximately 28 seconds)}
  \label{fig:supervixen}
\end{figure}


\subsection{Stereo Panning Spectrum Features} 

In this section we describe a set of features that summarize the
information contained in the Stereo Panning Spectrum that can be used
for automatic music classification. The main idea is to capture the
amount of panning in different frequency bands as well as how it
changes over time.

We define the Panning Root Mean Square for a particular frequency band as: 

\begin{equation} 
  P_{l,h} = \\ \sqrt{\frac{1}{h-l}{\sum_{k=l}^{h} [SPS(k)]^{2}}}
\end{equation} 

\noindent
where $l$ is the lower frequency of the band, $h$ is the high
frequency of the band, and $N$ is the number of frequency bins. 
By using RMS we only consider the amount of panning without 
taking into account whether it is to the left or right. We
consider the following 4-dimensional feature vector corresponding to an analysis window t: 
\begin{equation} 
  \Phi(t) = [P_{total}(t), P_{low}(t), P_{medium}(t), P_{high}(t)]
\end{equation} 

\noindent 
The PRMS values correspond to overall panning (0--22050 Hz), and
panning for low (0--250 Hz), medium (250--2500 Hz) and high
frequencies (2500--22050 Hz) respectively.

To capture the dynamics of panning information we compute a running
mean and standard deviation over the past M frames:

\begin{eqnarray} 
  m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
  s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)] 
\end{eqnarray} 

\noindent
This results in a 8-dimensional feature vector at the same rate as the
original 4-dimensional one. For the experiments M is set to 40
corresponding to approximately 0.5 seconds.  To avoid any duration
effects on classification we only consider approximately the first 30
seconds of each track, resulting in a sequence of 1000 8-dimensional
feature vectors for each track. The tracks are stereo, 16-bit, 44100
Hz sampling rate audio files and the STFT window size is set to 1024
samples. The sequence of feature vectors is collapsed to a single
feature vector representing the entire track by taking again the mean
and standard deviation across the first 30 seconds resulting in the
final 16-dimensional feature vector per track.



\section{Experiments}

In order to evaluate the effectiveness of the proposed features we
considered two non-trivial tasks. As a sidenote, using the proposed
features it is trivial (although quite useful) to detect mono
recordings directly converted to stereo without remastering.

The first classification task we consider is distinguishing two
collections of rock music, one from the 1960s and another from the
1990s.  In genre terms, these can be loosely categorized as `garage'
and `grunge.'  Both of these styles would be classified to the
top-level genre of rock. To isolate the effects of recording
production, we only included albums which had as their main
instrumentation the standard rock ensemble of electric guitar,
electric bass, drums and vocals.  Albums with an excess of keyboards
or experimental studio techniques, late 1960s Beatles for example,
were excluded.  We used 227 tracks from the 1960s and 176 tracks from
the 1990s.  Example `garage' groups include The Byrds, The Kinks and
Buddy Holly. Example `grunge' groups include Nirvana, Pearl Jam and
Radiohead.

The second classification task we consider is distinguishing electric
jazz from acoustic jazz.  Both of these styles would be classified to
the top-level genre of jazz. Acoustic jazz tends to have relatively
pronounced panning of the solo instruments (saxophone and trumpet)
that doesn't vary over time. We used 175 electric jazz tracks and 184
acoustic jazz tracks. Example electric jazz groups include: Weather
Report, Return to Forever, Medeski, Martin and Wood, and Mahavishnu
Orchestra. Example acoustic jazz groups led by artists include: Miles
Davis, John Coltrane, Lee Morgan and Branford Marsalis.

\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
  Garage/Grunge & ZeroR & NBC  & SMO & J48 \\
\hline
\hline
  SPSF                    &  56.4   &  77.2  & 81      & 84.2 \\
\hline
  SMFCC              &   56.4   &  74. 6 & 76.7  & 71.6  \\
\hline
  SPSF+SMFCC     &    56.4   &  82.7 &  83.7  & 83.2  \\
\hline
\end{tabular} 
\end{center}
\caption{Classification accuracies for Garage/Grunge} 
\label{table:gagr}
\end{table}



\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
  Acoustic/Electric & ZeroR & NBC  & SMO & J48 \\
\hline
\hline
  SPSF                    &  51.3   &  99.4  & 99.7    & 99.1 \\
\hline
  SMFCC              &   51.3  &  71.8   & 79.4    & 68.4  \\
\hline
  SPSF+SMFCC     &    51.3  &  98.5 &  99.1  & 99.1  \\
\hline
\end{tabular} 
\end{center}
\caption{Classification accuracies for Acoustic/Electric Jazz} 
\label{table:ajej}
\end{table}


Tables ~\ref{table:gagr},~\ref{table:ajej} show the classification
accuracy results for the Stereo Panning Spectrum Features and compares
them with the results obtained from stereo Mel-Frequency Cepstrum
Coefficients (MFCC) (basically the MFCC of the left and right channels
concatenated) as well as their combination for the two tasks. MFCCs
are the most common feature front-end for evaluating timbral
similarity \cite{aucouturier2004}. The accuracies are in percentages
and are computed using stratified 10-fold cross-validation. The ZeroR
classifier is a simple baseline, NBS corresponds to a simple Naive
Bayes classifier, SMO corresponds to a linear Support Vector Machine
trained with Sequential Minimal Optimization and J48 is a decision
tree. More information about these representative classifiers can be
found in \cite{wekaBook} or any pattern recognition textbook. As can
be seen the Stereo Panning Spectrum Features (SPSF) perform well and
for the acoustic vs electric jazz task achieve almost perfect
classification.  As a sidenote the classification accuracy of mono
MFCC were almost identical to the stereo MFCC therefore were not
included in the Tables.

It is important to note that the proposed features only capture stereo
information and are not influenced by any spectral content or
amplitude dynamics. For example applying any amplitude changes to both
channels doesn't change their values and the spectrum could be
completely altered without changing the features as long as the
changes are proportional to the panning coefficients.

Figure~\ref{fig:ajazzHist} shows the histograms of a single feature:
the mean total RMS panning for acoustic jazz (left) and electric jazz
(right).  As can be seen acoustic jazz has lower but more consistent
panning values whereas electric jazz has more pronounced and spread
panning values.

Table~\ref{table:gagrajej} shows the classification results for all
four styles combined. Although somewhat artificial as a task, this
provides information about the robustness of the proposed features as
well as the value of combining the standard MFCC features with the
proposed Stereo Panning Spectrum Features.


Researchers interested in replicating these experiments can obtain the
complete lists of tracks and albums for both of these tasks by
contacting the authors via email. The code for the calculation of the
SPS features has been integrated into {\it Marsyas}\footnote{\url{http://marsyas.sourceforge.net}} \cite{tzanetakis2000},
an open source framework for audio processing with specific emphasis
on Music Information Retrieval. The machine learning part of the
experiments were conducted using Weka\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}} \cite{wekaBook}.


 
\begin{figure}[t]
  \includegraphics[width=4cm]{ajazzHist} 
  \includegraphics[width=4cm]{ejazzHist} 
  \caption{\it Histogram of mean overall panning for Acoustic Jazz (Left) and Electric Jazz (Right) }
  \label{fig:ajazzHist}
\end{figure}




\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
  Gr/Ga/Aj/Ej   & ZeroR & NBC  & SMO & J48 \\
\hline
\hline
  SPSF                    &  29.8   &  73.6  & 81    & 76.5 \\
\hline
  SMFCC              &   29.8  &  56.4  & 65.9   & 52.3  \\
\hline
  SPSF+SMFCC     &    29.8  &  75.2 &  87.4  & 79.9  \\
\hline
\end{tabular} 
\end{center}
\caption{Classification accuracies for four styles} 
\label{table:gagrajej}
\end{table}



\section{Conclusions and Future Work}

A new feature set based on the Stereo Panning Spectrum was proposed and shown to be
effective for two non-trivial audio classification tasks. It has been
argued that the approach of modeling timbral similarity
using MFCC has reached a ``glass ceiling'' \cite{aucouturier2004}.  We
believe that information related to the recording process such as the
stereo panning information used in this paper can help future audio MIR systems escape this ceiling. More detailed features
related to stereo information than the ones proposed in this paper can
be envisioned.  For example, by clustering the panning values it
might be possible to determine how many tracks were used in the
mix. 

We are also interested in exploring other aspects of the studio
production process for MIR purposes.  Examples include equalization,
compression, and effects including reverberation and delay. One of the
authors is a professional studio recording engineer who teaches
recording techniques. We are planning to develop visualization and
editing tools that can help reverse-engineer the stereo mixing of
audio recordings for pedagogical purposes.

Engineers communicate about mixing with a particular lexicon of
qualitative terms.  A good example comes from an interview with Mix
Magazine where Dave Pensado describes one of his mixes as having
``massive club bottom, hip hop sensibility in the middle, and this
real smoothed-out, classy, Quincy Jones-type top.''  \cite{mix2001}.
Our hope is to eventually be able to translate this type of discussion
into a more quantitative domain.

\subsubsection*{Acknowledgments}

The authors would like to thank the National Sciences and Engineering
Research Council (NSERC) and Social Sciences and Humanities Research
Council (SSHRC) of Canada for funding this work, Perry Cook for
suggesting the idea of using stereo a long time ago, and Carlos
Avendano for describing his method with sufficient clarity and detail
to be re-implemented.  

\bibliographystyle{plain}
\bibliography{ismir2007stereo}

\end{document}
% -----------------------------------------------
% Template for ISMIR 2008
%     ismir2008.sty -> style file
% Modified by Juan P. Bello (ismir2008-papers@ismir.net)
% By Rainer Typke (ismir07.rainer@safersignup.com)
% Based on the 2004 template by Eloi Batlle.
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2008,amsmath}
% To use when using pdflatex
\usepackage{graphicx}
% To use when using latex, dvips and ps2pdf
% \usepackage[dvips]{graphicx}

% Title.
% ------
\title{Analyzing Afro-Cuban Rhythm using Rotation-aware 
Clave Template Matching with
Dynamic Programming} 


\oneauthor
  {Matthew Wright, W. Andrew Schloss, George Tzanetakis} 
  {University of Victoria, Computer Science and Music Departments\\ mattwrig@uvic.ca, aschloss@finearts.uvic.ca, gtzan@cs.uvic.ca}

% IMPORTANT NOTICE:
% Reviews are double-blind
% Author information should be kept hidden from reviewers
% Please avoid evident self references in the text

\begin{document}
%
\maketitle
%
\begin{abstract}
  The majority of existing research in Music Information Retrieval
  (MIR) has focused on either popular or classical music and
  frequently makes assumptions that do not generalize to other music
  cultures. We use the term Computational Ethnomusicology (CE) to
  describe the use of computer tools to assist the analysis and
  understanding of musics from around the world. Although
  existing MIR techniques can serve as a good starting point for CE,
  the design of effective tools can benefit from incorporating domain-specific 
  knowledge about the musical style and culture of interest. In
  this paper we describe our realization of this approach in the
  context of studying Afro-Cuban rhythm. More specifically we show how
  computer analysis can help us characterize and appreciate the
  complexities of tracking tempo and analyzing micro-timing in these
  particular music styles. A novel template-based method for tempo
  tracking in rhythmically complex Afro-Cuban music is
  proposed. Although our approach is domain-specific, we believe that
  the concepts and ideas used could also be used for studying other
  music cultures after some adaptation.




\end{abstract}

\section{Introduction}\label{sec:introduction}

%the introduction to the introduction.  Andy, please rearrange and trim down thi%s section however you like.

%\subsection{Afro-Cuban Clave}

%\subsection{Computational Ethnomusicology}

% \subsection{Rhythmic Analysis and Beat Tracking}

%\subsection{Existing Text}

% The use of computers for automatic and semi-automatic rhythmic
% analysis of audio recordings can greatly enhance the study of music
% from cultures in which rhythm plays a dominant role. Full-scale metric
% analysis of any music requires determining a function that maps each
% time instant to a continuous \textit{metric phase}.  From this we can
% derive BPM, tempo curve (BPM over time), and per-note displacement.


We present a set of techniques and tools designed for studying rhythm
and timing in recordings of Afro-Cuban music with particular emphasis
on ``clave,'' a rhythmic pattern used for temporal organization. In
order to visualize timing information we propose a novel graphical
representation that can be generated by computer from signal analysis
of audio recordings and from listeners' annotations collected in real
time.  The proposed visualization is based on the idea of Bar
Wrapping, which is the breaking and stacking of a linear time axis at
a fixed metric location.

The techniques proposed in this paper have their origins in Music
Information Retrieval (MIR) but have been adapted and extended in
order to analyze the particular music culture studied. Unlike much of
existing work in MIR in which the target user is an ``average'' music
listener, the focus of this work is people who are ``experts''
in a particular music culture. Examples of the type of questions they 
would like to explore include: how do expert players differ from each
other, and also from competent musicians who are not familiar with the
particular style; are there consistent timing deviations for notes at
different metric positions; how does tempo change over the course of a
recording etc. Such questions have been frequently out of reach because 
it is tedious or impossible to explore without computer assistance. 

Creating automatic tools for analyzing micro-timing and tempo
variations for Afro-Cuban music has been challenging. Existing beat-tracking
tools either don't provide the required functionality (for example
only perform tempo tracking but don't provide beat locations) or are
simply not able to handle the rhythmic complexity of Afro-Cuban music
because they make assumptions that are not always applicable, such as
expecting more and louder notes on metrically ``strong'' beats.
Finally the required precision for temporal analysis is much
higher than typical MIR applications. These considerations have
motivated the design of a beat tracker that utilizes
domain-specific knowledge about Cuban rhythms.

The proposed techniques fall under the general rubric of what has been
termed \textit{Computational Ethnomusicology} (CE), which refers to the design
and usage of computer tools that can assist ethnomusicological
research \cite{TzanetakisWright2007} .  
Futrelle and Downie argued for MIR
research to expand to other domains beyond Western pop and classical
music \cite{FutrelleDownie2002}.
Retrieval based on rhythmic information has been
explored in the context of Greek and African traditional music
\cite{Antonopoulos2007}. 

Our focus here is the analysis of music in which percussion plays an
important role, specifically, Afro-Cuban music.  Schloss
\cite{Schloss85} and Bilmes \cite{Bilmes93} each studied timing
nuances in Afro-Cuban music with computers. Beat tracking and tempo
induction are active topics of research, although they have mostly
focused on popular music styles \cite{McKinney2007}.  Our
work follows Collins' suggestion \cite{Collins2006} to build beat
trackers that embody knowledge of specific musical styles.


The \textit{clave} is a small collection of rhythms embedded in
virtually all Cuban music.  Clave is a repeated syncopated rhythmic
pattern that is often explicitly played, but often only implied; it is
the essence of periodicity in Cuban music. An instrument also named
``clave'' (a pair of short sticks hit together) usually plays this
repeating pattern. Clave is found mainly in two forms: \textit{rumba clave} and
\textit{son clave}. (One way of notating clave is shown in Figure ~\ref{fig:clave}.)  

\begin{figure}[ht]
\centering
\begin{tabular}{c c} 
\includegraphics[width= 0.475 \linewidth]{son-notation} & 
\includegraphics[width= 0.475 \linewidth]{rumba-notation} \\ 
\end{tabular} 
\caption{Son (left) and rumba (right) clave}
\label{fig:clave}
\end{figure}

Our study of timing requires knowing the exact time of every note played by the clave.
We can then decompose this data into an estimate of how tempo changes over time 
(what is called the \textit{tempo curve}) and a measure of each individual note's deviation
from the ``ideal'' time predicted by a metronomic rendition of the
patterns shown in Figure ~\ref{fig:clave}.

Unfortunately, we do not know of any databases of Afro-Cuban music
with an exact ground-truth time marked for every clave note or even
for every downbeat.\footnote{Bilmes recorded about 23 minutes of
  Afro-Cuban percussion at MIT in 1992, and performed sophisticated
  analysis of the timing of the \textit{guagua} and \textit{conga}
  (but not clave) instruments \cite{Bilmes93}; unfortunately these
  analog recordings are not currently available to the research
  community.}  Therefore we constructed a small four-song
database\footnote{Here is the name, artist, and source recording for
  each song, along with the two-character ID used later in the paper:
  \textit{LP}: \textit{La Polemica}, Los Mu\~{n}equitos de Matanzas,
  Rumba Caliente 88.  \textit{CB}: \textit{Cantar Bueno}, Yoruba
  Andabo, El Callejon De Los Rumberos.  \textit{CH}: \textit{Chacho},
  Los Mu\~{n}equitos de Matanzas, Cuba: I Am Time (Vol. 1).
  \textit{PD}: \textit{Popurrit de Sones Orientales}, Conjunto de
  Sones Orientales, Son de Cuba. } and gathered ground truth clave
timing data by having an expert percussionist with Afro-Cuban
experience tap along with the clave part. Custom sample-accurate tap
detection/logging software
%\cite{MattWrightDissertation}
automatically timestamps the taps.

Recordings of Afro-Cuban music challenge existing state-of-the-art
beat-tracking algorithms because of the complex and
dense rhythm and the lack of regular approximately isochronous
pulses. Figure ~\ref{fig:tempocurves} shows how two recent
state-of-the-art beat-tracking systems (BeatRoot \cite{Dixon2007} and a beat tracker using dynamic
programming proposed by Ellis \cite{Ellis2007}) 
do not
generate an accurate tempo curve for the recording \textit{CB}.
The plots in the figure are shown only in order to
motivate the proposed approach. The comparison is not fair, as the
other algorithms are more generally applicable and designed with
different assumptions, but in any case it demonstrates the advantage of a
domain-specific method to deal with these recordings: 
our method is specifically designed to take into account clave as the
rhythmic backbone.

\begin{figure}
% Crappy old way with a table of four pdf files:
% \begin{center}
% \begin{tabular}{c c}
% \includegraphics[width = 0.53 \linewidth] {groundtruth_tempocurve}
% & 
% \includegraphics[width = 0.53 \linewidth] {templatedp_tempocurve}
% \\
% \includegraphics[width=0.53 \linewidth] {beatroot_tempocurve}
% &
% \includegraphics[width= 0.53 \linewidth] {ellisdp_tempocurve}
% \\
%\end{tabular}
%\end{center}
%
% Matt's new single tiff file:
\includegraphics[width=3.33in]{unfair-comparison} 

\caption{Four estimates of the tempo curve for our recording \textit{CB}:  Ground truth calculated from a human expert's tap times (upper left),  curve from our method (top right), 
curve from BeatRoot (lower left), and curve from Ellis' dynamic programming approach (lower right).}
\label{fig:tempocurves}
\end{figure}










\section{Data preparation} 

It is common for Afro-Cuban songs to begin with just the sound of the
clave for one or two repetitions to establish the initial tempo.
However as other instruments (both percussive and pitched) and voices
enter the mix the sound of the clave tends to become masked.  The
first step of data preparation is to enhance the sound of the clave
throughout the song using a matched filter approach. In addition onset 
detection is performed. 

\subsection{Clave enhancement using Matched-Filtering} 

A matched filter detects or enhances the presence of an {\it a priori}
known signal within an unknown signal.  Its impulse response is 
a time-reversed copy of the known signal, which in our case is the
beginning portion of one isolated clave note. The clave instrument
affords little timbral variety and therefore every note of
clave in a given recording sounds substantially like all the others, so a
matched filter made from any single note (frequently easily obtained
from the beginning of the song) will enhance the presence of the clave
throughout the song and suppress the remaining signal.  One free
parameter is the filter order, i.e., the duration of the segment of
the clave note; in each case we selected a ``good'' matched filter
experimentally by listening to the output of different
configurations. All the curves in Figure ~\ref{fig:tempocurves} and
results in this paper have been calculated on audio signals output
by matched filtering.


\subsection{Onset detection} 

Onset detection aims at finding the starting time of musical events
(e.g. notes, chords, drum events) in an audio signal; see 
 \cite{Bello2005},\cite{Dixon2006} for recent tutorials.
%However,
%polyphonic music poses an increased challenge since nominally
%simultaneous notes might be spread over tens of milliseconds, turning
%the definition of onsets ambiguous. Similarly, it is hard to define a
%precise onset time for sounds with slow attacks.
%
%In a recent tutorial article, Dixon revisited the problem of onset
%detection \cite{Dixon2006}, where a number of onset detection
%algorithms were reviewed and compared on two datasets. This study was
%itself based on a previous article in which a theoretical and
%empirical comparison of several state-of-the-art onset detection
%approaches is presented \cite{Bello2005}. Following the findings and
%results in~\cite{Dixon2006}, the approach used in this work is based
%on the use of the 
We used \textit{spectral flux} as the onset detection function,
defined as:

\begin{equation}
SF(n) = \sum_{k=0}^{N/2}{HWR(|X(n,k)|-|X(n-1,k)|)}
\end{equation}

\noindent
where $HWR(x)=\frac{x+|x|}{2}$ is the half-wave rectifier function,
$X(n,k)$ represents the $k$-th frequency bin of the $n$-th frame of
the power magnitude (in dB) of the short time Fourier transform, and
$N$ is the corresponding Hamming window size. For the experiments
performed in this work all data had a sampling rate $f_s=44100$ Hz and
we used a window size of 46 ms ($N=2048$) and a hop size of about 11ms
($R=512$). The onsets are subsequently detected from the spectral flux
values by a causal peak-picking algorithm that finds local
maxima as follows.  A peak at time $t=\frac{nR}{f_s}$ (the time of the
beginning of the $n$th frame) is selected as an onset if it fulfills
the following conditions:

\begin{enumerate}
\item $SF(n)\geq SF(k) \;\; \forall k \; : n-w \leq k \leq n+w$
\item $SF(n)> \frac{\sum_{k=n-mw}^{n+w}{SF(k)}}{mw+w+1}\times thres + \delta$
\end{enumerate}

\noindent
where $w=6$ is the size of the window used to find a local maximum,
$m=4$ is a multiplier so that the mean is calculated over a larger
range before the peak, $thres = 2.0$ is a threshold relative to the
local mean that a peak must reach in order to be sufficiently
prominent to be selected as an onset, and $\delta=10^{-20}$ is a
residual value to avoid false detections on silent regions of the
signal. All these parameter values were derived from preliminary
experiments using a collection of music signals with varying onset
characteristics.

In order to reduce the false detection rate, we smooth the detection
function $SF(n)$ with a Butterworth filter to reduce the effect of spurious peaks:

\begin{equation}
H(z)=\frac{0.1173 + 0.2347z^{-1} + 0.1174z^{-2}}{1-0.8252z^{-1}+0.2946z^{-2}} 
\label{eq:ButtFilt}
\end{equation}

\noindent
(These coefficients were found by experimentation based on the findings in 
 \cite{Bello2005},\cite{Dixon2006}.)
In order to avoid phase distortion (which would shift the detected
onset time away from the $SF(n)$ peak) the signal is filtered in
both the forward and reverse directions. 


\section{Template-based tempo tracking}\label{sec:template_tempo}

We propose a new method to deal with the challenges of beat tracking
in Afro-Cuban music. The main idea is to use domain specific
knowledge, in this case the clave pattern, directly to guide the
tracking. The method consists of the following four basic steps: 1)
Consider each detected onset time as a potential note of the clave
pattern. 2) Exhaustively consider every possible tempo (and clave
rotation) at each onset by cross-correlating each of a set of clave-pattern
templates against an onset strength envelope signal beginning at each
detected onset. 3) Interpret each cross-correlation result as a score for
the corresponding tempo (and clave rotation) hypothesis. 4) Connect the
local tempo and phase estimates to provide a smooth tempo curve and
deal with errors in onset detection, using dynamic programming.

The idea of using dynamic programming for beat tracking was proposed
by Laroche \cite{Laroche2003}, where an onset function was compared to
a predefined envelope spanning multiple beats that incorporated
expectations concerning how a particular tempo is realized in terms of
strong and weak beats; dynamic programming efficiently enforced
continuity in both beat spacing and tempo.  Peeters \cite{Peeters2007}
developed this idea, again allowing for tempo variation and matching
of envelope patterns against templates. An approach assuming
constant tempo that allows a simpler formulation at the cost of more
limited scope has been described by Ellis \cite{Ellis2007}. 



\subsection{Clave pattern templates}


At the core of our method is the idea of using entire rhythmic
patterns (templates) for beat tracking rather than individual
beats. First we construct a template for each possible tempo.  We take
the ideal note onset times in units of beats (e.g., for rumba clave, the list
0, 0.75, 1.75, 2.5, 3) and multiply them by the duration of a beat at
each tempo, giving ideal note onset times in seconds.  We center a
Gaussian envelope on each ideal note onset time to form the template.
The standard deviation (i.e., width) of these Gaussians is a free
parameter of this method.  Initial results with a constant width
revealed a bias towards higher tempi,
%%% XXX Say why!
so widths are specified in units of beats, i.e., we scale the width
linearly with tempo. Better results were obtained by making each
template contain multiple repetitions of the clave, e.g., three
complete patterns. Figure ~\ref{fig:templates} shows a visual
representation of the template rotations for all considered tempi. 

\begin{figure}[ht]
\includegraphics[width= \linewidth]{templates_inbeats} 
\caption{Clave Templates for all rotations and tempi}
\label{fig:templates}
\end{figure}




With a 5-note clave pattern, any given note played by the clave could
be the 1st, 2nd, 3rd 4th or 5th note of the pattern.  Therefore we
make templates for all ``rotations'' of the clave, i.e., for the
repeating pattern as started from any of the five notes.  For example,
rotation $0$ of rumba clave is [0, 0.75, 1.75, 2.5, 3], and
rotation $1$ (starting from the second note) is [0.75 1.75 2.5 3 4] -
0.75 = [0 1 1.75 2.25 3.25].  Time $0$ always refers to the onset time
of the current note.

\noindent
\begin{figure}[ht]
\includegraphics[width=\linewidth]{template-match-example} 
\caption{Matching different templates (filled grey) to the \textit{CB} recording's energy envelope (black line).  The X-axis is time (seconds), with zero the time of the onset under consideration. The score for each template match represents how well that template lines up with the energy envelope.} 
\label{fig:templates_matching}
\end{figure}

We cross-correlate (in other words, take the dot product of) these templates 
against segments of an onset strength
envelope (in our case, simply the total energy in each 1024-sample
window of the matched filter output) beginning at the time of each
detected onset.  We interpret the dot product between the onset
strength signal ${O(t)}$ and a template $T_{j,k} (t)$ with tempo $j$
and rotation $k$ as the strength of the hypothesis that the given
onset is the given note of clave at the given tempo.  Figure
~\ref{fig:templates_matching} depicts this process for some tempi and
rotations and the corresponding scores.  We exhaustively compute these
dot products for every candidate tempo $j$ (e.g,. from 95 to 170 BPM in 1 BPM
increments), for all five rotations of the clave pattern $k$ , for
every detected onset $i$ at time $t_{i}$ to produce a {\it score grid}:

\begin{equation} 
score(i, j, k) = \sum_{t=0} ^{LT_{j,k}-1} T_{j,k}(t) 
%\times 
O(t_{i} + t)
\end{equation} 
\noindent
where $LT_{j,k}$ is the length of template $T_{j,k}$ . 


\subsection{Rotation-blind dynamic programming}

It is trivial to look at a given onset, pick the tempo and rotation
with the highest score, and call that the short-term tempo estimate.
However, due to the presence of noise, inevitable onset detection
errors, and the matched filter's far-from-perfect powers of auditory
source separation, simply connecting these short-term tempo estimates
does not produce a usable estimate of the tempo curve.  Better results
can be achieved by explicitly discouraging large tempo changes. We use
dynamic programming \cite{Bellman1957} as an efficient means to
estimate the best tempo path (i.e., time-varying tempo).  In the next
section we will consider the rotations of the template; for now let
the ``rotation-blind'' score be:

\begin{equation} 
scoreRB(i, j) = max(score(i, j,k))\; k:1..5
\end{equation} 

We convert each score $scoreRB$ to a cost $C_{i,j}$ with a linear
remapping so that the highest score maps to cost $0$ and the lowest score maps
to cost $1$. We define a {\it path} $P$ as a sequence of 
tempo estimates (one per onset), so that $P(i)$ is P's estimate of the
tempo at time $t_{i}$.  Our algorithm minimizes the {\it path cost}
$PC$ of the length $n$ path $P$:

\begin{equation} 
PC(P) = \sum_{i=0:n-1}C_{i,P(i)} + \sum_{i=0:n-2}F(P(i), P(i+1))
\end{equation} 
where $F(tempo_1,tempo_2)$ is a ``tempo discontinuity cost function''
expressing the undesirability of sudden changes in tempo. $F$ is
simply a scalar times the absolute difference of the two
tempi. Dynamic programming can efficiently find the lowest-cost path
from the first onset to the last because the optimal path up to any
tempo at time $t_{i}$ depends only on the optimal paths up to time
$t_{i-1}$.  We record both the cost $PC(i,j)$ and the previous tempo
$Previous(i,j)$ for the best path up to any given onset $i$ and tempo
$j$.

% We initialize the path cost for each tempo $j$ at time $t_{0}$ to
% be simply ${C_{0,j}}$.  Then we advance sequentially through the
% onsets, at each point considering all possible tempi for both the
% current and previous onsets:


% \begin{verbatim} 
% for i = 2 to num_onsets
%   for j_now = 1 to num_tempi
%     lowest_cost = \inf
%     best_previous_tempo = NULL
%     for j_before = 1 to num_tempi
%       this_cost = PC(i-1,j_before) + C(i,j_now) + F(j_before,j_now)
%       if this_cost < lowest_cost
%         lowest_cost = this_cost
%         best_previous_tempo = j_before
%     Previous(i,j_now) = best_previous_tempo;
%     PC(i,j_now) = lowest_cost;
% \end{verbatim} 

% Although this algorithm is quadratic in the number of tempi, it is
% only linear in the number of onsets (which should be approximately
% proportional to the song's duration).



\subsection{Rotation-aware dynamic programming} 

Now we will extend the above algorithm to consider rotation, i.e., our
belief about which note of clave corresponds to each onset.  Now our
cost function $C_{i,j,k}$ is also a function of the rotation $k$.  Our
path tells us both the tempo $P_{tempo}(i)$ at time $t_{i}$ and also
the rotation $P_{rot}(i)$, so we must keep track of both previous
$Previous_{tempo}(i,j)$ and $Previous_{rot}(i,j)$ (corresponding to the
best path up to $i$ and $j$).  Furthermore, considering rotation will
also give us a principled way for the path to skip over ``bad''
onsets, so instead of assuming that every path reaches onset $i$ by
way of onset $i-1$ we must also keep track of $Previous_{onset}(i,j)$.

The key improvement in this algorithm is the handling of rotation.
Rotation (which indexes the notes in the clave pattern) is converted
to {\it phase}, the proportion (from $0$ to $1$) of the distance from one
downbeat to the next.  (So the phases for the notes of rumba clave are
[0, 0.1875, 0.4375, 0.625, 0.75]).  The key idea is predicting what
the phase of the next note ``should be'': Given phase $\phi_1$ and
tempo $j_1$ for onset $i_1$, a candidate tempo $j_2$ for onset $i_2$,
and the time between onsets $\Delta T = t_2-t_1$, and assuming linear
interpolation of tempo during the (short) time between these nearby
onsets, we can use the fact that tempo (beat frequency) is the
derivative of phase to estimate the phase $\hat\phi_2$:

\begin{equation} 
\hat\phi_2 = \phi_1 + \Delta T ((j_1+j_2)/2)/(4 \times 60)
\end{equation}
Dividing by $4 \times 60$ converts from BPM to bars per second.

Now we can add an extra term to our cost function to express the
difference between the predicted phase $\hat\phi_2$ and the
actual phase $\phi_2$ corresponding to the rotation of whatever template
we're considering for the onset at time $t_2$ (being careful
to take this difference modulo 1, so that, e.g., the difference between 0.01
and .98 is only 0.03, not 0.97).  We'll call this phase distance the
``phase residual'' $R$, and add the term $\alpha * R$ to our cost function.

Now let's consider how to handle ``false'' detected onsets, i.e.,
onsets that are not actually notes of clave.  For onset $n$, we consider
not just onset $n-1$ as the previous onset, but every onset $i$ with $t_i >
t_n - K$, i.e., every onset within $K$ seconds before onset $n$, where $K$ is
set heuristically to 1.5 times the largest time between notes of clave
(one beat) at the slowest tempo.  We introduce a ``skipped onset
cost'' $\beta$ and include $\beta \times (n-i-2)$ in the path cost when the
path goes from onset $i$ to onset $n$.



\begin{table} 
\begin{center} 
\begin{tabular}{|c|c|c|c|c|c|c|} 
\hline 
       & LP & CB & CH    & PD        & LPWT & PDWT \\ 
RB  & 40 & 26 & 22.9  &  63.1    &  39.96 &  62.7           \\
\hline     
RA  & 1.75 & 11 & 1.54 &  3.10    &  2.043 &    57.9 \\
\hline 
\end{tabular} 
\end{center}
\caption{RMS (in BPM) results for tempo curve estimation}
\label{table:rms}
\end{table} 

Table ~\ref{table:rms} shows the Root-mean-square (RMS) error between
the ground truth tempocurve and the tempocurves estimated by the
rotation-blind (RB) and rotation-aware (RA) configurations of our method.  In
all cases the rotation-aware significantly outperforms the
rotation-blind method (which usually tracks correctly only parts of the tempo
curve).  The first three recordings (LP, CB, CH) have
rumba-clave and the fourth piece (PD) has son-clave.
The last two columns show the
results when using the ``wrong'' template.  Essentially when the
template is not correct the matching cost of the beat path is much
higher and the tempo curve estimation is wrong.  Figure
~\ref{fig:rotcomp} shows the score grid for the rotation-blind (top) and
rotation-aware (bottom) configurations overlaid 
with the estimated and ground truth tempocurves.


\begin{figure}[ht]
\centering
\begin{tabular}{c} 
\includegraphics[width= 0.8 \linewidth]{rotation_blind} \\
\includegraphics[width= 0.8 \linewidth]{rotation_aware} \\
\end{tabular} 
\caption{Rotation-blind (top) and rotation-aware (bottom) beat tracking}
\label{fig:rotcomp}
\end{figure}




\section{Bar-Wrapping Visualization}\label{sec:visualization}


\begin{figure}[ht]
\centering
\includegraphics[width= \linewidth]{barwrap-stretched} 
\caption{Bar-wrapping visualization}
\label{fig:barwrap}
\end{figure}

A performance typically consists of about 625-1000 clave ``notes''. Simply
plotting each point along a linear time axis would require either
excessive width, or would make the figure too small to see anything;
this motivates bar wrapping. Conceptually, we start by marking each
event time (in this case, each detected onset) on a linear time
axis. If we imagine this time axis as a strip of magnetic tape holding
our recording, then metaphorically we cut the tape just before each
downbeat, so that we have 200 short pieces of tape, which we then
stack vertically, so that time reads from left to right along each
row, and then down to the next row, like text in languages such as
English.  Each of these ``strips'' is then stretched horizontally to
fill the figure width, adding a tempo curve along the right side to
show the original duration of each bar.  Figure ~\ref{fig:barwrap}
depicts the times of our detected onsets for {\it LP} with this
technique. The straight lines show the theoretical clave locations. By
looking at the figure one can notice that the 5th clave note is
consistently slightly later than the theoretical location. This would
be hard to notice without precise estimation of the tempocurve.


Rotation-aware dynamic programming is used to find the downbeat times.
An explicit downbeat estimate occurs whenever the best path includes a
template at rotation $0$.  But there might not be a detected onset at
the time of a downbeat, so we must also consider implicit downbeats,
where the current onset's rotation is not $0$ but it is lower than the
rotation of the previous onset in the best path.  The phase is
interpolated to estimate the downbeat time that ``must have occurred''
between the two onsets.

\section{Discussion and Conclusions} 

Our beat-tracking method works particularly well for Afro-Cuban clave
for many reasons: 1) The clave part almost never stops in traditional
Afro-Cuban music (although it can be hard to hear when many other
percussion instruments are playing).\footnote{Our method's phase- and
  tempo-continuity constraints allow it to stay on track in the face
  of extra or missing onsets and occasional unduly low template match
  scores, so we expect that it would still perform correctly across
  short gaps in the clave part.}  2) The clave pattern almost never
changes in Afro-Cuban music.\footnote{One subtlety of Afro-Cuban music
  is the notion of ``3-2'' versus ``2-3'' clave, which refers to a
  180-degree phase shift of the clave part with respect to the
  ensemble's downbeat.  Our method has no notion of the ensemble's
  downbeat and ``doesn't care'' about this distinction.  Some songs
  change between 3-2 and 2-3 in the middle, but never by introducing a
  discontinuity in the clave part (which would be a problem for our
  algorithm); instead the other instruments generally play a phrase
  with two ``extra'' beats that shifts their relationship to the
  clave.}  3) The clave instrument produces an extremely consistent
timbre with every note, so matched filtering does a good job
emphasizing it.\footnote{In rare cases a different instrument carries
  the clave part; this should not be a problem for our method as long
  as a relatively isolated sample can be located.}  4) Songs often
begin with the clave alone, making it easy to construct our matched
filter.\footnote{As future work we would like to explore the
  possibility of creating a ``generic'' clave enhancement filter that
  doesn't rely on having an isolated clave note in every recording, a
  weakness of the current method.}  5) The clave plays one of a few
predetermined syncopated parts, favoring the use of predefined
templates rather than assumptions of isochrony.

There are many future work directions. Rhythmic analysis can be used
to categorize recordings into different styles and possibly identify
particular artists or even percussionists. We also plan to apply the
method to more recordings and continue working with ethnomusicologists
and performers interested in exploring timing. It is our belief that
our template-based rotation-aware formulation can also be applied to
popular music by utilizing different standard drum patterns as
templates. All the code implementing the method can be obtained by
emailing the authors.





\bibliographystyle{plain}
\bibliography{ismir2008gtzan}



% XXX MAYBE TABLE WITH DETAILS OF TRACKS and 
% RECORDINGS XXX 
% Cantar Bueno from the 1993 recording
% El Callejon De Los Rumberos by Yoruba Andabo
% (DiscMedi DM203 CD), or the piece La Cachamba from
% the 1993 recording Tambores Cubanos by Los Papines
% (EGREM CD 0037)


\end{document}
\subsection{Clave enhancement using Matched-Filtering} 

A matched filter detects or enhances the presence of an \it{a priori}
known signal within an unknown signal.  Its impulse response is simply
a time-reversed copy of the known signal, which in our case is the
beginning portion of one isolated note of clave.  The clave instrument
affords very little timbral variety, in other words, every note of
clave in a given sounds substantially like all the others, so a
matched filter made from any single note (frequently easily obtained
from the beginning of the song) will enhance the presence of the clave
throughout the song and suppress the remaining signal.  One free
parameter is the filter order, i.e., the duration of the segment of
the clave note; in each case we selected a ``good'' matched filter
experimentally by listening to the output of different
configurations. All the curves in Figure ~\ref{fig:tempocurves} and
results in this paper have been calculated on audio signals output
by matched filtering.

\subsection{Clave pattern templates}

First we construct a template for each possible tempo.  We take the
ideal note onset times in units of beats (e.g., for Rumba clave, the
list 0, 0.75, 1.75, 2.5, 3) and multiply them by the duration of a
beat at each tempo, giving ideal note onset times in seconds.  We
center a Gaussian curve on each ideal note onset time, and the sum of
these Gaussians is the template.  The standard deviation (i.e., width)
of these Gaussians is a free parameter of this method.  Initial
results with a constant width revealed a bias towards higher tempi, so
we now specifiy width in units of beats, i.e., we scale the width
linearly with tempo.

We observed better results by making each template contain multiple
repetitions of the clave pattern, e.g., three complete patterns.

Both clave patterns contain 5 notes, so any given note played by the
clave could be the first, second, third, fourth, or fifth note of the
pattern.  Therefore we make templates for all ``rotations'' of the
clave pattern, in other words, for the repeating pattern as started
from any of the five notes.  For example, the zeroth rotation of Rumba
clave is [0, 0.75, 1.75, 2.5, 3], and the first rotation (starting
from the second note) is [0.75 1.75 2.5 3 4] - 0.75 = [0 1 1.7 2.25
  3.25].  Time zero always refers to the onset time of the current
note.

We correlate these templates with segments of an onset strength signal
(in our case, simply the total energy in each 1024-sample window of
the output of the matched filter) beginning at the time of each
detected onset.  We interpret the correlation amount between the onset
strength signal \math{O(t)} (with sampling rate \math{fs_O})
and a template with a given tempo and rotation as the
strength of the hypothesis that the given onset is the given note of
clave at the given tempo.  Figure XXX depicts this process.  We
exhaustively compute these correlations for every tempo (e.g,. from 95
to 170 BPM in 1 BPM increments), for all five rotatons of the clave
pattern, for every detected onset to produce a \it{score grid}.

score_(onset_i, tempo_j, rotation_k) = 
   \sum_t=0:TL_j,k{template_j,k(t) * O(onset_i+t*fs_O)

where TL_j,k is the length of template_j,k.


\subsection{Rotation-blind dynamic programming} 

It's trivial to look a a given onset, pick the tempo and rotation with
the highest score, and call that the short-term tempo estimate.
However, due to the presence of noise, inevitable errors from the
onset detector, and the matched filter's far-from-perfect powers of
auditory source separation, simply connecting these short-term tempo
estimates does not produce a usable estimate of the tempo curve.  We
can achieve much better results by explicitly discouraging large
changes in tempo.

We use dynamic programming [Refs:  Bellman 1957 (from Ellis' paper),
Ellis 2007...] as an efficient means to estimate the best
tempo path (i.e., time-varying tempo).  In the next section we will
consider the rotations of the template; for now let

score_rotation-blind(onset_i, tempo_j) = 
   \max_k=1:5(score(onset_i, tempo_j, k)

We convert each score \math{score_rotation-blind(i,j)} to a cost \math{C_{i,j}}
with a simple linear remapping so that the highest score goes to cost
0 and the lowest score goes to cost 1.

We define a \it{path} \math{P} as a sequence consisting of a tempo
estimate for each onset, so that \math{P(i)} is P's estimate of the
tempo at time t_{i}.  Our algorithm will minimize the \it{path cost}
\math{PC} of the length \math{n} path \math{P}:

PC(P) = \sum_i=0:n-1{C_{i,P(i)}} + \sum_i=0:n-2{F(P(i), P(i+1))}

where F(tempo_1,tempo_2) is a ``tempo discontinuity cost function''
expressing the undesirability of sudden changes in tempo.  Currently
we use just a simple linear function of the linear difference in
tempi:

F(tempo_1,tempo_2) = k \times abs(tempo_1-tempo_2)

Dynamic programming can efficiently find the lowest-cost path from the
first onset to the last because the optimal path up to any tempo at
time t_{i} depends only on the optimal paths up to time t_{i-1}.  We
will record both the cost PC(i,j) and the previous tempo Previous(i,j)
for the best path up to any given onset i and tempo j.

We initialize the path cost for each tempo \math{j} at time t_{0} to
be simply \math{C_{0,j}}.  Then we advance sequentially through the
onsets, at each point considering all possible tempi for both the
current and previous onsets:

for i = 2 to num_onsets
  for j_now = 1 to num_tempi
    lowest_cost = \inf
    best_previous_tempo = NULL
    for j_before = 1 to num_tempi
      this_cost = PC(i-1,j_before) + C(i,j_now) + F(j_before,j_now)
      if this_cost < lowest_cost
        lowest_cost = this_cost
        best_previous_tempo = j_before
    Previous(i,j_now) = best_previous_tempo;
    PC(i,j_now) = lowest_cost;

Although this algorithm is quadratic in the number of tempi, it is
only linear in the number of onsets (which should be approximately
proportional to the song's duration).

\subsection{Dynamic programming considering rotation}

Now we will extend the above algorithm to consider rotation, i.e., our
belief about which note of clave corresponds to each onset.  Now our
cost function C_{i,j,k} is also a function of the rotation
\{k}, our path tells us both the tempo P_tempo(i) at time
t_{i} but also the rotation P_rotation(i), and we must keep track of
both Previous_tempo(i,j) and Previous_rotation(i,j).  Furthermore,
considering rotation will also give us a principled way for the path
to skip over ``bad'' onsets, so instead of assuming that every path reaches
onset i by way of onset i-1 we must also keep track of
Previous_onset(i,j).

The key improvement in this algorithm is the handling of rotation.
First we convert rotation, which indexes the number of notes in the
clave pattern, to \it{phase}, the proportion (from 0 to 1) of the way
from one downbeat to the next.  (So the phases for the notes of Rumba
clave are [0, 0.1875, 0.4375, 0.625, 0.75].)  The central idea is the
ability to predict what the phase of the next note ``should be'':
Given phase \phi_1 and tempo j_1 for onset i_1, a candidate tempo j_2
for onset i_2, and the time between onsets \delta T = t_2-t_1, and
assuming linear interpolation of tempo during the (short) time between
these nearby onsets, we can use the fact that tempo (beat frequency)
is the derivative of phase to estimate the phase
\put_a_hat_over{\phi_2}:

\put_a_hat_over{\phi_2} = \phi_1 + \delta T * ((j_1+j_2)/2)/4*60

The 4*60 converts from beats per minute to bars per second.

Now we can add an extra term to our cost function to express the
difference between the predicted phase \put_a_hat_over{\phi_2} and the
actual phase \phi_2 corresponding to the rotation of whatever template
we're considering for the onset at time t_2 (being careful
to take this difference modulo 1, so that, e.g., the difference between 0.01
and .98 is only 0.03, not 0.97).  We'll call this phase distance the
``phase residual'' \math{R}, and add the term \alpha * R to our cost function.

Now let's consider how to handle ``false'' detected onsets, i.e.,
onsets that are not actually notes of clave.  For onset n, we consider
not just onset n-1 as the previous onset, but every onset i with t_i >
t_n - K, i.e., every onset within K seconds before onset n, where K is
set heuristically to 1.5 times the largest time between notes of clave
(one beat) at the slowest tempo.  We introduce a ``skipped onset
cost'' \beta and include \beta times n-i-2 in the path cost when the
path goes from onset i to onset n.
\section{Template-based tempo tracking}\label{sec:template_tempo}

We propose a new method to deal with the challenges of beat tracking
in Afro-Cuban music. The main idea is to use domain specific
knowledge, in this case the clave pattern, directly to guide the
tracking using a template-based approach. The method consists of the
following four basic steps: 1) Consider each detected onset time as a
potential note of the clave pattern. 2) Exhaustively consider every
possible tempo (and clave pattern phase) at each onset by correlating
each of a set of clave-pattern templates against an onset strength
envelope signal beginning at each detected onset. 3) Interpret each
correlation amount as a score for the corresponding tempo (and pattern
phase) hypothesis. 4) A dynamic programming solution connects the
local tempo and phase estimates to provide a smooth tempo curve and
deal with errors in onset detection. The next subsections provide details.


\subsection{Tracking using dynamic programming} 

The idea of using dynamic programming for beat tracking was proposed by Laroche [2003], 
where an onset function was compared to a predefined envelope spanning multiple beats that 
incorporated expectations concerning how a particular tempo is realized in terms of strong and 
weak beats; dynamic programming efficiently enforced continuity in both beat spacing and tempo. 
Peeters [2007] developed this idea further, again allowing for tempo variation and matching of envelope 
patterns against templates. By contrast, the current system assumes a constant tempo which allows 
a much simpler formulation and realization, at the cost of a more limited scope of application. 

XXX I think this is backwards:  Peeters assumes constant tempo (?) and we certainly don't.

\subsection{Phase-aware dynamic programming} 
% -----------------------------------------------
% Template for ISMIR 2010
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2010,amsmath,cite}
\usepackage{graphicx}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage{mathptmx} 
\usepackage{algorithmic}
\usepackage{algorithm}

% Title.
% ------
\title{Geoshuffle: Location-aware, content-based music browsing using 
self-organizing tag clouds}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
  

\begin{document}
%
\maketitle
%
\begin{abstract}

  In the past few years the computational capabilities of mobile
  phones have been constantly increasing. Frequently these smartphones
  are also used as portable music players. In this paper we
  describe GeoShuffle -- a prototype system for content-based music
  browsing and exploration that targets such devices. One of the most
  interesting aspects of these portable devices is the
  inclusion of positioning capabilities based on GPS. GeoShuffle adds
  location-based and time-based context to a user's listening
  preferences. Playlists are dynamically generated based on the
  location of the user, path and historical preferences. 
  
  Browsing large music collections having thousands of tracks is
  challenging. The most common method of interaction is using long lists
  of textual metadata such as artist name or genre. Current
  smartphones are characterized by small screen real-estate which
  limits the amount of textual information that can be displayed.  We
  propose self-organizing tag clouds, a 2D tag cloud representation
  that is based on an underlying self-organizing map calculated using
  automatically extracted audio features. To evalute the system the
  Magnatagatune database is utilized. The evaluation indicates that
  location and time context can improve the quality of music
  recommendation and that self-organizing tag clouds provide faster
  browsing and are more engaging than text-based tag clouds.

\end{abstract}
%

\section{Introduction}\label{sec:introduction}

Portable mobile phones with strong multimedia capabilities and
computational power are rapidly gaining popularity. As these devices
frequently also function as portable digital music players it is
important to investigate how music information retrieval systems can
be adapted to the unique challenges and opportunities they present. 
In this paper we describe GeoShuffle a music browsing application 
designed to address the challenge of limited screen real estate and 
to take advantage of the opportunity of location information 
that smart phones provide. 

Automatic music recommendation is an active topic of research. Such
systems can be based on collaborative filtering, expert annotations,
folksonomies, automatic content analysis and any of their
combinations. However, all these approaches suffer from the limitation
that their results are the same irrespective of the listening
context. The preferences of a listener change depending on where they
are and what they are doing.  For example the music a student would
like recommended when studying might be different from the music
desired when riding the bus. 

Location-aware devices based on technologies such as GPS are 
common. We propose that the quality of automatically generated
playlists can be improved by taking into account this newly available
location data. This information can be used to determine a user's
listening habits while in transit to common destinations, as people
often have daily routines such as return trips to work, school, social
activities, and so on.  It provides context to a user's listening
preferences beyond general ratings. A user providing a rating to a
song does not provide context about the conditions under which a user
would enjoys listening to that song.  For example, a high-energy song that a
user rates highly may never be desired when the user wants to relax.

Another unique characteristic of smart phones is their limited screen
real-estate. The size of personal digital audio collections is 
steadily increasing. Effective interaction with these large audio
collections poses significant challenges to traditional user
interfaces. Music management software typically allow users to select
artist, genres or individual tracks by browsing long sortable lists of
text. This mode of interaction, although adequate for small music
collections, becomes increasingly problematic as collections become
larger especially when screen estate is limited. A variety of
alternative ways of browsing music collections have been proposed
mostly in academic contexts. They typically rely on a combination of
audio signal analysis to automatically extract features followed by
visualization techniques to map the feature space to a 2D or 3D
representation for browsing and navigation. 

Tag clouds provide both an overview of the information space as well
as direct search support that is particularly suited for mobile phones
with small touch screens.  In this paper, we present content-aware
self-organizing tag clouds a technique that attempts to support
querying, browsing, and summarization using the familiar information
model of a tag cloud while taking into account automatic content
analysis information as well as location based information.



\section{Related Work}\label{sec:related_work} 
%%\subsection{Related Work}\label{sub:related_work}

Although there is existing work in location-based applications and
automatic/semi-automatic playlist generation there seems to be a lack
of published material on location-aware playlist generation. With
respect to intelligent playlist creation, Flexer et al. have proposed
using audio similarity based on Mel Frequency Cepstrum Coefficients
(MFCC) and Gaussian models to create a similarity matrix and select
songs that blend from and into a user-selected start and end track in
a playlist ~\cite{Flexer08}. Pampalk et al.  have proposed using user
behaviour based on track skipping to determine what artists, genres,
rhythms, etc., the user prefers to pass-over ~\cite{Pampalk05}. With
respect to location-aware playlist creation most existing work
simply associates particular pieces of music with specific
locations \cite{Eustice08}. 

The current generation of mobile phones feature decent
sized displays that also include touch functionality. Interfaces for
managing large audio collections based on long lists of scrollable
text are not particularly convenient in such displays. An alternative
that has mostly been explored in research literature is the use of
content-based visualizations of music collections \cite{cooper06}. 

Tagging systems allow users to add keywords, or tags, to resources
without relying on a controlled vocabulary and have
become ubiquitous in web-based systems. Tags are aggregated from many
users forming ``folksonomies'' which, although not as accurate as
well-designed ontologies, have the advantage of reflecting how users
perceive the data and how their vocabulary and perception evolve over
time. Tagging is simple and does not require a lot of thinking. Tags
form an essential part of personalized internet radio and music
community websites such as Last.fm
\footnote{\url{http://www.last.fm}}. Tag clouds are the most common
way of visualizing tags. They are two-dimensional stylized visual
representations of a list of words where the more prominent words are
typically assigned a larger font. They are useful for quickly giving
users the gist of a set of words. Tag clouds are in common usage on a
number of different social networks such as Flickr
\footnote{\url{http://www.flickr.com}} but trace their origins back at
least 90 years to Soviet Constructivist art \cite{viegas08}. 

There has been considerable research in recent years into the design,
use and effectiveness of tag clouds.  A historical look at tag clouds
is presented in Viegas and Wattenburg \cite{viegas08}, which looks at
the development of tag clouds since their inception a decade ago, and
speculates about their development in the future.  In the paper
``Seeing things in clouds'' \cite{bateman08}, an extensive evaluation
of different types of visual features in tag clouds, including font
size, font weight, intensity, number of characters and area were
investigated. Tag navigation in general has been examined in detail
with particular focus on ``Last.FM'', an online social community for
music \cite{mesnage09}.  A context aware browser for mobile devices
that uses tag clouds is presented in Mizzaro et al. \cite{mizzaro09}.

Islands of Music \cite{pampalk03} is a a content-based visualization
of music collections that uses Self-Organizing Maps (SOM) to generate
a two-dimensional representation of a collection of music.  MusiCream
\cite{goto05musicream} is an interface that allows users to interact
with a music collection using a dynamic visualization interface.
MusicRainbow \cite{goto06musicRainbow} is a similar system that uses
web-based labelling and audio similarity to visualize music
collections. Examples of visualizations for music discovery in commercial
and research systems can be found in the Visualizing Music blog
\footnote{\url{http://visualizingmusic.com/}}.





\section{System description}
\label{sec:systemdescription}

Our proposed system takes as input the user's location, the current
playing and associated metadata as well as content-based similarity
information between all tracks in a user collection. This information 
is stored in a database for organization and retrieval. The system 
processes these inputs to generate location-based information such as
common paths and make automatic recommendations based on them. 
Semantic information related to the generated playlists such as track
names, artists, genres, tags, playlists are rendered based on
self-organizing tag clouds that are computed based on automatically
extracted audio features. 


\subsection{Location and Path Logging}

We introduce the following terms to describe location information:
{\bf Paths} consist of a start and end location and a collection of
{\bf Path Sggments} which consist of a start, end, bearing and segment
speed. The {\bf Path Segments} are determined by a list of {\bf
  Location Points} which are instantaneous snapshots of what
song is playing and where. This includes a track's metadata (artist,
album, title, etc.), current coordinate and time, and whether a song started or skipped.

As a user's location or music changes and location points are
generated, the system interpolates the user's current line-of-travel
in real-time and generates a path segment consisting of a line between
start and end coordinates.  These path segments are then associated to
a path from the start location of the first path segment to the end
location of the last path segment.  These paths can then be profiled
by counting the songs that are played or skipped, the most listened to
genres or tempos, etc.; therefore, as the user builds up a path
history, it can be used to generate a more accurate representation of
the user's listening tastes. 

One of the challenges of determining path segments is that location
estimates vary in accuracy and are sampled irregularly.  In addition a
user following the same path in different days (for example taking the
bus to school) will not have exactly the same set of location
points. Therefore we have developed an algorithm for determining
determining path segments from a running list of location points. The
basic idea is to first determine the bearing between the first two
location points in a path segment. Subsequently the bearing between
the start point of the segment and subsequent points is determined.
If the new point has the same bearing as the original pair, the new
point becomes the end to the segment.  This continues until a
coordinate yields a bearing of the current segment's path.  This
basic algorithm works when travelling in very straight lines, and with
very accurate positioning hardware, but in real world usage will
generate segments between almost every pair of points, as any
deviation in bearing will result in a new segment being generated.

In order to account for the accuracy of the positioning system, an
algorithm was devised to allow for variation in the absolute location
based on the intrinsic accuracy of the mobile device.  Each absolute
position is reported as a box bounded by the accuracy of the device.
Consequently, any points in the bounding box are considered the same
absolute coordinate.  The same bounding box is used in calculating the
bearing for path segments.  



\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{map}
\label{fig:map}
\caption{Visualization of paths and location points on a map and
  schematic of path finding algorithm}
\end{figure}


These located segments are combined from a start location to an end
location in order to generate a path. Figure 1 shows a schematic
diagram of the algorithm and a map with paths and location points
overlayed. Currently, a path is started when the first change in a
user's location is sensed. A path is ended when a user stays at a
location for more than 15 minutes. Basic equations for finding
distances based on decimal degree coordinates for latitudes and
longitudes, and for finding the bearing between two coordinates are
based on the WGS84 world representation (currently used by GPS
systems).


\subsection{Audio Feature Extraction and Recommendations} 


The goal of audio feature extraction is to represent each track as a
vector of features that characterize musical content. First low-level
features such as the Spectral Centroid, Rolloff, Flux and the
Mel-Frequency Cepstral Coefficients (MFCC) are computed approximately
every 20 milliseconds. To capture the feature dynamics we compute a
running mean and standard deviation over the past M frames (the
so-called ``texture window'' typically around 1 second). The result is
a feature vector of 32 dimensions at the same rate as the original 16D
feature vector. The sequence of feature vectors is collapsed into a
single feature vector representing the entire audio clip by taking
again the mean and standard deviation across the 30 seconds (of the
sequence of dynamics features), resulting in the final 64D feature
vector per audio clip. A more detailed description of the features and
their motivation can be found in Tzanetakis and Cook \cite{TC02b}. For
the calculation of the self-organizing map described in the next
section all features are normalized so that the minimum of each
feature across the music collection is 0 and the maximum value is
1. This feature set has shown state-of-the-art performance in audio
retrieval and classification tasks for example in the Music
Information Retrieval Evaluation Exchange (MIREX) 2008 and was
computed using the free Marsyas audio processing framework
\footnote{\url{http://marsyas.info}}. Most audio feature sets
proposed exhibit similar performance so we expect that any audio
feature front end can be used.

Based on a distance matrix calculated between all pairs of tracks, 3
different recommendation algorithms are implemented. In the
naive similarity case, a random seed-song is selected, and playlists
of the ten most similar songs (based on pre-calculated Euclidean
distances) were created.  If the user skipped a song, a new seed is
selected and a new playlist is generated along with it.  In the
similary-with-history case, a profile is constructed based on songs
the user listened to at the same time and day of the week to recommend
similar songs.  A seed song is selected based on tracks that the user
enjoyed at similar times (current time $+/-$ an hour) in the past and
their three nearest neighbours.  If a user skipped a track, a new seed
based on their history is selected and a new playlist is generated.
Using location information, the system predicted a path that the user
is taking and selects a seed from a similar track that was listened to
on that path previously. Finally we provide interactive control to the
specificity of the generated playlists using the accelerometers
included in more mobile devices. Shaking the device at varying levels 
results in selecting seeds scuh that recommendations are more similar
if the shake is light and less similar if it is heavy. 


\subsection{Self-Organizing Maps}

For creating the visualization layout we utilized the self-organizing
map (SOM) which is a type of neural network used to map a high
dimensional feature space to a lower dimensional representation while
preserving the topology of the high dimensional space. This
facilitates both similarity quantization and visualization
simultaneously. The SOM was first documented in 1982 by T. Kohonen,
and since then, it has been applied to a wide variety of diverse
clustering tasks \cite{kohonen95a}. In our system the SOM is used to
map the audio features (64-dimensions) corresponding to each track
 to two discrete coordinates on a grid.

 The traditional SOM consists of a 2D grid of neural nodes each
 containing a $n$-dimensional vector, $ {\bf x(t)} $ of data. The goal
 of learning in the SOM is to cause different neighbouring parts of
 the network to respond similarly to certain input patterns. The
 network must be fed a large number of example vectors that represent,
 as closely as possible, the kinds of vectors expected during
 mapping. The data associated with each node is initialized to small
 random values before training. During training, a series of
 $n$-dimensional vectors of sample data are added to the map.  The
 ``winning'' node of the map known as the {\it best matching unit}
 (BMU) is found by computing the distance between the added training
 vector and each of the nodes in the SOM. This distance is calculated
 according to some pre-defined distance metric which in our case is
 the standard Euclidean distance on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding nodes
reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. The time-varying
learning rate and neighborhood function allow the SOM to gradually
converge and form clusters.



\subsection{Self-Organizing Tag Clouds} 

The technique of self-organizing tag clouds can be viewed as a fusion
of concepts from text-based visualization interfaces and more abstract
content-aware visualization interfaces. We use the term tag loosely to
denote any metadata associated with a track such as genre, artist or
year of release. Traditional systems based on long lists of sortable
text such as iTunes provide little support for browsing, discovery and
summarization. An alternative is visualization interfaces that are
based on automatic analysis of musical content. By mapping the music
collection onto a 2D or 3D representation they enable quick browsing
and navigation especially in the case of music that is not known to
the user or that has not been tagged. 

Tag-clouds provide a simple, familiar interface that partly overcomes
these limitations. For example they support both direct searching as
well as browsing and navigation. However they come with their own
problems. In order for a tag to assist search or browsing it is
necessary for the user to have some notion of its meaning. For example
a specialized term such as indie pop might be completely unfamiliar to
a particular listener while at the same time essential to
another. This problem becomes even more acute using the more
generalized notion of tags that includes information such as artist or
album.  As one of the goals for an effective interface of music
collection browsing is the discovery of new music by artists not known
to the listener, this is an important disadvantage. Simple tag clouds
do not provide the user with any information about the connections and
similarity relations between tags. A final problem with any system
based solely on tag information is that there is no way to access
music tracks that have not been tagged (the so-called ``cold start''
problem). By contrast content-based visualizations allow any track to
be accessed and do not require familiarity with the music explored.

\begin{figure}[t]
\centering
\includegraphics[width=35mm]{som_tag_cloud_before_msd.pdf}
\includegraphics[width=35mm]{som_tag_cloud.pdf}
\label{fig:sotag}
\caption{Self-Organizing tag cloud before and after mass-spring layout
  algorithm}
\end{figure}

We describe a new method for organizing music tag clouds that makes a
persistent map taking into account the musical similarity between
songs. Figure 2 shows an example of a self-organized
tag cloud. Each label (artist, genre, tag) is associated with a set of
tracks that have been annotated with it. As the tracks have been
mapped to feature vectors and subsequently to 2D grid coordinates by
the SOM, each tag is associated with a set of 2D
grid coordinates. The SOM process ensures that
neighboring points (tracks) will have similar high-dimensional audio
features and therefore similar musical content. The tags are placed on
the centroids of their corresponding set of 2D grid coordinates. Their
placement reflects the underlying musical content but results in
visual overlap between them. 

This initial layout contains many overlapping words, so the position
of each tag is repositioned using a mass, spring and damper
force-based algorithm for drawing \cite{ellson01}.  In our
implementation each tag is anchored to its original position using a
spring and an electrostatic-like force is applied between every pair
of tags that is proportional to the inverse of their squared distance.
Therefore tags that are close and overlapping will be pushed away
while still trying to remain close to their original location.  An
additional wall force term was added to keep all tags within the
designated window. The font size for each tag was determined by
counting the number of instances of that tag.  

There are some interesting characteristics of the resulting
visualization that we would like to highlight. The first is that 
tags that are not correlated with the acoustical content 
will correspond to tracks spread across the underlying
self-organizing map and therefore their placement will be in the
center. For example in Figure 2 the tags Male Singer, 
Singing and Female Vocal are near the center as they have a large 
variety of tracks that have been annotated with them. In contrast more
specialized tags such as Heavy Metal or Monks are more localized. 
The second important characteristic is that faceted browsing is
naturally supported. For example an artist name, that the user might
not be familiar with, located near the left corner will correspond to
the tag Monks. Finally a track for which there are no tag annotations 
will still be placed on the underlying self-organizing map and that 
way receive an implicit visual automatic tag annotation addressing 
to some extent the cold-start problem.

\subsection{Implementation}

The feature extraction, music similarity calculation and
self-organizing map training are performed using the Marsyas audio
processing framework. Our current prototype application GeoShuffle has
been implemented for Apple Inc.'s iPhone or iPod Touch devices. The
application dynamically generates music playlists that can be played
in the default iPhone/iPod Touch music player based on location, path
of travel, historical information and content similarity. To provide
feedback to the user on their preferences by path, as well as to test
the accuracy of the application, a Google Map generated map has been
embedded into the application (see Figure 1).  This map supports
annotations in the form of paths or absolute location points. The
device's positioning system provides real-time updates on the user's
absolute position. This allows the user to visually trace their daily
commutes and inspect their musical taste over each path.





\section{Evaluation}\label{sec:evaluation}


Evaluating a complex system and user interface such as the one
described in this paper is challenging due to its subjective
nature. We focus on two aspects of our work: 1) the use of
self-organizing tag clouds as a way to explore large music collections
that combines text and content information without requiring large
displays 2) the use of location information to improve music
recommendation.

For evaluation purposes we used a subset of the Magnatagatune dataset
consisting of 1141 tracks with each artist represented by at most 3
tracks. This was chosen as a large enough dataset to have considerable
variability while at the same time being manageable in the limited
storage of the iPod Touch used for development.  There are 341 artists
represented and also 14 top-level genre labels. In addition to the
regular meta-data information such as artist and genre, also includes tags
derived from the Tagatune Game with a purpose \cite{law09}. The
dataset has been made available to the scientific community for use in
research.

For evaluating the self-organizing tag clouds, 14 participants were
recruited from graduate Computer Science students. Three were female
and 11 were male. All subjects had normal or corrected-to-normal
vision, enjoyed listening to music and were experienced computer
users. None of the participants had previous knowledge of the
Magnatune dataset. The user study consisted of a 5-point system usability
survey (SUS) \cite{brooke96}.  

  The survey consisted of six questions, each rated on a five point scale,
  where ``1'' was labelled ``Strongly disagree'' and ``5'' was
  labelled ``Strongly agree''. The 6 questions were: 1) I
  thought the application was easy to use, 2) I needed to learn a lot
  before I could accomplish tasks with the application, 3) I think
  people would need technical support to learn how to use the
  application, 4) I think most people would learn to use the
  application very quickly, 5) Overall, accomplishing tasks using the
  self-organizing tag cloud was easy 6) Overall, accomplishing tasks
  using the self-organizing tag cloud was fun

  Results from survey are detailed in Table \ref{table:sus}. On
  average users rated Question 4 highest, which indicated that they
  thought most other people would be able to learn the application
  quickly. This question also had the lowest variance.  In Table
  \ref{table:sus} we detail all the responses from the participants. 
  We can see that two participants chose the middle check box, six 
  chose the next one to the right, and six chose the checkbox labelled
  ``Strongly agree''.

  In a similar vein, participants also rated questions 5 and 6 highly,
  although notably, two participants rated this question as one box to
  the right of ``Strongly Disagree''.  This shows that certain users
  found our interface easy to use and fit in well with their
  expectations of an interface to explore music collections, but for
  other users it did not. For Question 2, the average response was
  1.85, which implies that on average, users strongly disagree
  that they would have to learn a lot before accomplishing tasks with
  this application.  It is important to include negative examples on
  such a user study to ensure that participants are not just choosing
  answers to questions randomly;  this question performs this
  control function.

\begin{table}
\centering
\caption{System Usability Survey}
\begin{tabular}{ccccccccc} 
\hline
Question & 1 & 2 & 3 & 4 & 5 & Mean & Std \\  \hline
\\ 
1 & 0 & 1 & 3 & 8 & 2 & 3.79  & 0.8   \\
2 & 5 & 7 & 1 & 1 & 0 & 1.86  & 0.86  \\
3 & 5 & 3 & 3 & 1 & 2 & 2.43  & 1.45  \\
4 & 0 & 0 & 2 & 6 & 6 & 4.29  & 0.73  \\
5 & 0 & 2 & 1 & 4 & 7 & 4.14  & 1.1   \\
6 & 0 & 2 & 0 & 6 & 6 & 4.14  & 1.03  \\
\\ \hline
\end{tabular}
\label{table:sus}
\end{table}





For evaluating the location-aware music recommendation component it
was necessary to collect data over an extended period of usage. Usage
data was collected from only one subject. The subject used the system
over a period of three weeks through their daily routine. GeoShuffle
logged their musical preference over the time period and generated
sets of user paths (consisting of an origin, destination, and linear
path segments). The device switched between four modes of
recommendation without the user's knowledge (random, similarity,
similarity with history, similarity with location-awareness) and
logged which tracks were skipped throughout operation. These results
were then used to determine the amount of user skips in each mode of
recommendation without biasing the data.

Self-organizing tag clouds can also be used to visualize text
information associated with a playlist. Figure 3
shows the self-organizing tag cloud text associated with three
playlists (from left to right: random, similarity and path). The
figure clearly shows the increase in specificity and the content
distribution of the recommended playlists. 

\begin{figure}[t]
\centering
\includegraphics[width=25mm]{SOM_Tag_Cloud_Random.pdf}
\includegraphics[width=25mm]{SOM_Tag_Cloud_Similarity.pdf}
\includegraphics[width=25mm]{SOM_Tag_Cloud_Path.pdf}
\label{fig:so_screenshot}
\caption{Screen shot of playlist visualization using the
  Self-Organizing Tag Cloud}
\end{figure}


\begin{table}[h]
\centering
\caption{Number of skips and genres present in playlists created with different generators}
\begin{tabular}{|l|c|c|}
\hline
 		& Skips / Track Played & Genres in Playlist \\
\hline
Random	 	& 4.3		       & 12  	\\
\hline
Similarity 	& 1.7		       & 7	 \\
\hline
+ History 	& 1.2		       & 3	 \\
\hline
+ Path	 	& 0.3		       & 10	 \\
\hline
\end{tabular}
\label{tab:skip_results}
\end{table}


Table ~\ref{tab:skip_results} shows the analysis of skipping behavior
between different configurations of the system. We assume that
playlists that result in less skipping are better and show the results
as average number of skips per track played. The baseline of 4.3
corresponds to randomly selecting songs from the collection in similar
fashion to the iPod shuffle. The similarity configuration returns
tracks that are similar to all the tracks played in the logging
period. The history configuration in addition to similarity takes into
account the time of the day. The last configuration also takes into
account information about paths taken during the day and is the only
one that requires portable devices with location information. As can
be seen there is a significant reduction in the number of skips when
taking into account location information.




\section{Conclusions and Future Directions}

In this paper we describe our investigations in designing an interface
for content-aware music browsing, discovery and recommendation that is
designed based on the unique characteristics of modern smartphones. We
propose using location information to improve the quality of music
recommendations and introduce self-organizing tag clouds: a
visualization of metadata information such as genres, artists, tags
and playlists that takes into account automatically extracted musical
content information. The specificity of the music recommendation
algorithm can be interactively controlled using the
accelerometers. The resulting interface is particularly suited for
small screen real-estate and touchscreens. Our evaluation indicates that
self-organizing tag clouds are an effective and fun way of
exploring music collections and that location information can improve
the quality of music recommendations.

There are many directions for future work. We plan to explore
visualizing tag-based similarities as edges between tags with
proportional thickness. Another interesting direction is the addition
of social networking and collaboration features such as sharing
playlists for particular paths or comparison of collections between
different users. Several of the user study participants suggested
using the same interface for personalized tag annotation. Finally we
plan to conduct a wider ethnographic study where self-organizing tag
clouds and location-based recommendation are used in personal music
collections.

% \section{Acknowledgements}

% We thank the National Science and Research Council (NSERC) for providing part of 
% the funding necessary to do this research.  We would also like to thank
% all the participants in this study for their time and valuable comments.

\bibliography{ismir2010gtzan}


\end{document}
% -----------------------------------------------
% Template for ISMIR 2011
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2011,amsmath,cite}
\usepackage{graphicx,url}
\usepackage{subfigure}

% Title.
% ------
\title{A computational investigation of melodic contour stability in Jewish Torah Trope performance traditions}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
% Peter van Kranenburg
  {Author1} {Affiliation4 \\ {\tt author1@ismir.edu}}
% Daniel P. Biro 
  {Author2} {Affiliation5 \\ {\tt author2@ismir.edu}}
% Steven Ness, George Tzanetakis
  {Author3, Author4} {Affiliation1 \\ {\tt author34@ismir.edu}}

\def\taam{{\it ta'am}}
\def\teamim{{\it te'amim}}

\begin{document}

\maketitle
%
\begin{abstract}
   The cantillation signs of the Jewish Torah trope are of particular
   interest to chant scholars interested in the gradual codification
   of music performance into notation. Each sign, placed above or
   below the text, acts as a ``melodic idea'' which either connects or
   divides words in order to clarify the syntax of the text. Unlike
   standard music notation, the interpretations of each sign are
   flexible and influenced by regional traditions as well as
   improvisatory elements incorporated by a given reader. In this
   paper we describe our collaborative work in developing and using
   computational tools to assess the stability of melodic contours of
   cantillation signs from two different performance traditions. We
   also show that a musically motivated alignment algorithm obtains
   better results than the more commonly used dynamic time warping
   method for calculating similarity between pitch contours. Using a
   participatory design process our team, which includes a
   domain expert, has developed an interactive web-based interface that
   enables researches to explore aurally and visually chant recordings
   and explore the relations between sign, gesture and representation.


\end{abstract}

\section{Introduction}


In the last ten years there has been a growing interest in music
information retrieval (MIR). A variety of techniques for automatically
analyzing music based on both symbolic and audio representations have
been developed. In most cases the target user of MIR systems has been
the average music listener rather than the specialist. There is an
even longer tradition of computational musicology dating back to the
1950s of using mathematics, statistics and eventually computers to
study music. Most of this work in computational musicology has focused
on the symbolic domain and western music notation. More recently the
idea of Computational Ethnomusicology in which MIR techniques are used
to support research in musics from around the world has been proposed
\cite{sness2010}. Audio analysis techniques can be used for empirical
research on field recordings for which no transcription is available or
feasible. 

The study of religious chant is of particular interest to
musicologists as it can help understand the transition from oral
transmission to codified notation. Jewish Torah trope is ``read''
using the twenty-two cantillation signs of the {\it te'amei hamikra},
developed by the Masorite rabbis between the sixth to the ninth
centuries. The Masorite Rabbis concurrently inscribed the {\it
  te'amim} along with the vowels of the Hebrew letters in order to
ensure accuracy in future Torah reading, thereby altering the previous
mode of oral transmission. The melodic formulae of Torah trope govern
syntax, pronunciation and meaning and their clearly identifiable
melodic design, determined by their larger musical environment, is
produced in a cultural realm that combines melodic improvisation with
fixed melodic reproduction within a static system of notation.

Chant scholars have investigated historical and phenomenological
aspects of melodic formulas within the Jewish Torah trope to discover
how improvised melodies might have developed to become stable melodic
entities in given Jewish communities. In this paper we investigate how
computational approaches can be used to support research in this
area. More specific audio analysis is combined with content-based
similarity retrieval to explore the ways in which melodic contour
defines melodic identities. In particular the question of melodic
stability is investigated. Observing certain key {\it te'amim} such as
{\it etnachta} and {\it tipha} we investigate aspects of self similarity within
Torah trope within and across various Jewish communities (based on
recordings of Hungarian and Moroccan Torah trope).  This might give us
a better sense of the role of melodic gesture in melodic formulae in
Jewish Torah trope practice and possibly a new understanding of the
relationship between improvisation and notation-based chant in and
amongst these divergent traditions.




\section{Related Work} 


%%% gtzan and peter need to write 
%%% sness needs to help with citations and bibliography 
%%% add tmta article as well as cbmi (anonymized) 
%%% various other references from Daniel and tmta 


\cite{avenary78}
\cite{camachophd}
\cite{ness10}
\cite{vankranenburg03}


\section{Data Analysis} 


The {\it te'amim} consist of thirty graphic signs. Each sign,
placed above or below the text, acts as a ``melodic idea,'' which either
melodically connects or divides words in order to make the text
understandable by clarifying syntax. The signs serve to indicate the
melodic shape and movement of a given melody. Even though the notation
of the {\it te'amim} is constant, their pitches are variable.  Although the
thirty signs of the {\it te'amim} are employed in a consistent manner
throughout the Hebrew Bible, their interpretation is flexible: each
sign's modal structure and melodic gesture is determined by the text
portion, the liturgy, by pre-conscribed regional traditions as well as
by improvisatory elements incorporated by a given ``reader''. 

% XXXX Maybe add footnote from Daniel 

In the liturgical performance, the {\it ba'Äôal hakorei} (`the owner of
reading') embellishes the text with a melodic code, providing the
framework to decode the textual syntax of the read Torah text by the
reading religious community, for whom text, and not melody, is
primary. Since their inscription, the primary functionality of the
{\it te'amim}, to structure pronunciation and syntax, remained intact. But
as the Jewish people were dispersed throughout the world, secondary
levels of musical code were incorporated into the {\it te'Äôamim}.
Borrowed melodies and modal structures, taken from surrounding
musical cultures allowed not only for new melodic interpretations but
also for external {\it semiotic musical meaning} to permeate the
musical interpretation of the text.

For this small-scale study, we use the recordings of two readings of
the same Torah passage, one from the Hungarian (Ashkenazi) tradition
and the one from the Moroccan (Sephardic) tradition.  The two
recordings used in this study can be consulted at: \url{http://pierement.zoo.cs.uu.nl/meuk/cantillation/shircomparedfull.html}.
Both recordings have manually been segmented into the individual {\it
  te'amim} by the author who is a domain expert. Even though we
considered the possibility of creating an automatic segmentation tool,
it was decided that the task was too subjective and critical to
automate. Each segment is annotated with a word/symbol that is related
to the corresponding cantillation sign.





\subsection{Pitch Contour Representation} 

Each recording has been converted to a sequence of frequency values
using the SWIPEP fundamental frequency estimator \cite{camacho2007} by
estimating the fundamental frequency in non-overlapping time-windows
of 10ms. The frequency sequences have been converted to sequences of
real-valued MIDI pitches with a precision of 1 cent (which is 1/100 of
an equally tempered semitone, corresponding to a frequency difference
of about 0.06\%). For each of the recordings, we derive a melodic
scale by detecting the peaks in a non-parametric density estimation of
the distribution of pitches, using a Gaussian kernel. This can be
viewed as a smoothed frequency histogram. Clear peaks in the histogram 
correspond to salient pitches and can be used to form a discrete pitch
scale that is specific to the recording rather than any particular
tuning system. 

In a previous study \cite{ness10}, mean average precision values were computed
for each of the scales containing 1 to 13 pitches, taking all realizations of the same
{\it ta'am} as the query segment as relevant items, and using a distance measure
based on dynamic time warping. The finding was that quantizing
the melodic contours according to the scale containing {\it two} pitches resulted in the
highest mean average precision. Apparently, the two most prevalent pitches have
structural meaning. 

In the current study we use a different approach. Instead of quantizing the melodic
contours, we scale them linearly according to the two most prevalent pitches in the
entire recording. 
We denote the higher and lower of the
two prevalent pitches as $p_{high}$ and $p_{low}$, respectively. Each
pitch is scaled relative to $p_{low}$ in units of the difference
between $p_{high}$ and $p_{low}$. Thus, scaled pitches with value $<0$
are below the lowest of the two prevalent pitches and pitches with value $>1$
are above the highest of the two and pitches between 0 and 1 are
between the two prevalent pitches. As a result, different trope
performances, sung at different absolute pitch heights, are
comparable.

\subsection {A distance measure for melodic segments}

On the acquired scaled pitch contours we apply an alignment algorithm
as described in \cite{VanKranenburg2009}, interpreting the
alignment score as similarity measure. This approach is comparable to
the use of dynamic time warping in Ness et al, 2010, but the current
approach uses a more advanced scoring function for the individual
elements of the pitch sequences.

We use the Needleman-Wunsch global alignment algorithm \cite{Needleman:1990cs}. This algorithm
finds an optimal alignment of two sequences of symbols, which, in our
case, are sequences of pitches.  The quality of an alignment is
measured by the alignment score, which is the sum of the alignment
scores of the individual symbols. If we consider two sequences of
symbols ${\mathbf{x}}:x_{1},\dots,x_{i},\dots,x_{n}$, and
${\mathbf{y}}:y_{1},\dots,y_{j},\dots,y_{m}$, then symbol $x_{i}$ can
either be aligned with a symbol from sequence $\mathbf{y}$ or with a
gap. Both operations have a score, respectively the substitution score
and the gap score. The gap score is mostly expressed as penalty,
i.e.\ a negative score. The optimal alignment and its score are found
by filling a matrix $D$ recursively according to: 


\begin{equation}
  D(i,j)=\max\left\{ \begin{array}{ll}
    D(i-1,j-1)+S(x_{i},y_{j})\\ D(i-1,j)-\gamma\\ D(i,j-1)-\gamma\end{array}\right.,
\end{equation}


in which $S(x_{i},y_{j})$ is a similarity measure for symbols,
$\gamma$ is the gap penalty, $D(0,0)=0$, $D(i,0)=-i\gamma$, and
$D(0,j)=-j\gamma$. $D(i,j)$ contains the score of the optimal
alignment up to $x_{i}$ and $y_{j}$ and therefore, $D(m,n)$ contains
the score of the optimal alignment of the complete sequences. We can
obtain the alignment itself by tracing back from $D(m,n)$ to $D(0,0)$;
the standard dynamic programming algorithm has both time and space
complexity $O(nm)$.

The similarity measure for symbols, which returns values in the interval
$[-1,1]$, is in our case defined as:
\[
S(x_{i},y_{j})=\left\{ \begin{array}{ll}
1-4\left|sp_{1}-sp_{2}\right| & \textrm{if }\left|sp_{1}-sp_{2}\right|\leq0.5\\
-1 & \textrm{otherwise}\end{array},\right.\]
in which scaled pitch
\[
sp_{1}=\frac{p_{1}-p_{low,1}}{p_{heigh,1}-p_{low,1}},\]
and
\[
sp_{1}=\frac{p_{2}-p_{low,2}}{p_{heigh,2}-p_{low,2}}.\] in which
$p_{1}$ and $p_{2}$ are the pitches as sung, represented in continuous
midi encoding, in which the middle c is represented by value 60.0, c\#
by 61.0, d by 62.0, and so on. By allowing fractional pitches we have
a one-to-one correspondence to the frequencies, and a linear scale in
the pitch domain. We use a linear gap penalty function with
$\gamma=0.6$.

Since the score of an alignment depends on the length of the
sequences, normalization is needed to compare different alignment
scores. The alignment of two long sequences results in a much higher
score than the alignment of two short sequences. Therefore, we divide
the alignment score by the length of the shortest sequence. Thus, an
exact match results in score 1, which is the maximal score. The scores
are converted into distances by taking one minus the normalized score,
resulting in distances greater than or equal to zero.


\section{User Interface} 


\begin{figure*}[htb]
\begin{center}
\includegraphics[width=120mm]{cbmi2009-cantillion}
\end{center}
\caption{
Web-based \emph{Flash} interface to allow users to listen to audio, and to
enable interactive querying of gesture contour diagrams.}
\label{fig:cantillion} 
\end{figure*} 

We have developed a browsing interface that allows researchers to
organize and analyze chant segments in a variety of ways
(http://cantillation.sness.net). Each recording is manually segmented
into {\it te'amim}. The pitch contours of these segments can be viewed
at different levels of detail. They can also be rearranged in a
variety of ways both manually and automatically. The audio analysis
(pitch extraction and dynamic time warping) are performed using the
Marsyas audio processing
framework \footnote{\url{http://marsyas.sourceforge.net}} 
\cite{Marsyas}. 

The interface shown in Figure~\ref{fig:cantillion} has four main sections: a sound
player, a main window to display the pitch contours, a control window,
and a histogram window.  The sound player window displays a
spectrogram representation of the sound file with shuttle controls to
let the user choose the current playback position in the sound
file. The main window shows all the pitch contours for the song as
icons that can be repositioned automatically based on a variety of
sorting criteria, or alternatively can be manually positioned by the
user. The name of each segment (from the initial segmentation step)
appears above its F0 contour. The shuttle control of the main sound
player is linked to the shuttle controls in each of these icons,
allowing the user to set the current playback state either way.

When an icon in the main F0 display window is clicked, the histogram
window shows a histogram of the distribution of quantized pitches in
the selected sign. Below this histogram is a slider to choose how many
of the largest histogram bins will be used to generate a simplified
contour representation of the F0 curve. In the limiting case of
selecting all histogram bins, the reduced curve is exactly the
quantized F0 curve. At lower values, only the histogram bins with the
most items are used to draw the reduced curve, which has the effect of
reducing the impact of outliers and providing a smoother
``abstract'' contour.  Shift-clicking selects multiple signs; in this
case the histogram window includes the data from all the selected
signs. We often select all segments with the same word or trope sign;
this causes the simplified contour representation to be
calculated using the sum of all the pitches found in that particular
sign, enhancing the quality of the simplified contour representation.
Figure~\ref{fig:cantillion} shows a screenshot of the browsing interface.

In the current work we implemented a mode that allows the researcher
to sort the samples based on the alignment score from one
sample to the other.  The interface allows the user to select an
arbitrary gesture from the interface, and then perform a sorting of
all other gestures to it.  In the example shown in Figure
~\ref{fig:cantillion} the user has chosen a ``revia'', and has sorted
all the other gestures based on their DTW-based alignment distance
from this first revia.  One can see that the gesture closest to this
revia is another revia gesture from a different section of the audio
file.

We are currently developing an addition to the interface to
allow us to visualize subsets of signs at different quantization
levels, and to compare these to the original continuous contour.  This
interface uses a checkbox list to allow the user to select different
types of signs, and then displays these contours in the main interface
pane.  The user can select multiple quantization levels and can
compare them for many signs at once, which allows the user to quickly
perform an analysis similar to the full pair-wise comparison described
above, but interactively, and therefore using the knowledge and skills
of ethnomusicologists.


\section{Results and interpretation}


%pvk: FOLLOWING PARAGRAPH IS IMPORTANT. Should be included somewhere.

%% Melodic categories of these traditions can be examined and explicated
%% by employing large amounts of data with improved computer
%% technologies. For instance: the measure of repeatability within a
%% given {\it te'amim} would be a main indicator for melodic stability.  It is
%% also possible that some of the {\it te'amim} have precursors to music (for
%% instance basic syntactical divisions, exclamations and sentence
%% cadence structures).  The actual performance of the {\it te'amim} also point
%% to musical aspects that, as scholars have pointed out, were coming
%% from musical cultures outside of Judaism. 1 That which has been
%% historically studied, the relationship between Ashkenazi Torah trope
%% and plainchant, can now be tested in terms of musical data analysis. 2
%% By measuring the flexibility and variability of the {\it te'amim} we can
%% show how fixed musical structures and improvisation within these
%% traditions co-exist.


\begin{figure}[htb]
\centering
\includegraphics[width=0.7\columnwidth]{notrelated-crop.pdf}
\caption{Distribution of distances between unrelated segments.}
\label{fig:histogram-notrelated} 
\end{figure} 

\begin{figure}[htb]
\centering
\subfigure{\includegraphics[width=0.49\columnwidth]{metnachta-crop.pdf}}
\subfigure{\includegraphics[width=0.49\columnwidth]{hetnachta-crop.pdf}}
\caption{Distribution of distances between renditions of the {\it etnachta} in the
Moroccan interpretation (left) and the Hungarian interpretation (right).}\label{fig:histogram-etnachta}
\end{figure}

\begin{figure}[htb]
\centering
\subfigure{\includegraphics[width=0.49\columnwidth]{mtipha-crop.pdf}}
\subfigure{\includegraphics[width=0.49\columnwidth]{htipha-crop.pdf}}
\caption{Distribution of distances between renditions of the {\it tipha} in the
Moroccan interpretation (left) and the Hungarian interpretation (right).}\label{fig:histogram-tipha}
\end{figure}

\begin{figure}[htb]
\centering
\subfigure{\includegraphics[width=0.49\columnwidth]{msofpasuq-crop.pdf}}
\subfigure{\includegraphics[width=0.49\columnwidth]{hsofpasuq-crop.pdf}}
\caption{Distribution of distances between renditions of the {\it sof pasuq} in the
Moroccan interpretation (left) and the Hungarian interpretation (right).}\label{fig:histogram-sofpasuq}
\end{figure}

To investigate the stability in performance of the various \teamim{}, we use two approaches. Firstly, we compute the mean average precision for each of the \teamim{} based on the alignment-distance. Each segment is taken as query and all renditions of the same \taam{} are taken as relevant items. The higher the mean average precision, the higher the relevant items are on the ranking lists that are obtained by sorting all segments according to the distance to the query segment. The values are shown in Table~\ref{table:newprecisions}. Secondly, we show the distribution of distances between renditions of the same \taam{} by plotting histograms of those distances. Figure~\ref{fig:histogram-notrelated} shows the distribution
of alignment-based distances between unrelated segments. This histogram can be used
as reference for comparing distances between related segments.

% Sorry, the columns are switched...
\begin{table} 
\begin{center}
\begin{tabular}{|lr|lr|}
\hline
 Ta'am   &  Average  &  Ta'am   &   Average   \\
 (Morocco)   &  Precision   & (Hungary)   &   Precision       \\
    &   (Morocco)  &    &   (Hungary)       \\
\hline
sofpasuq†††&	0.550†& sofpasuq††&†	0.994†††\\katon†††&	0.399†††&	revia††&†	0.967†††\\tipha†††&	0.306††&etnachta††&†	0.945†\\mapah††&†	0.299†††&pashta††&†	0.683†††\\pashta†††&	0.269†††&	tipha†††&	0.673†††\\revia†††&	0.245††&katon†††&	0.562†††\\etnachta††&†0.234†&mapah†&††	0.550†\\zakef†††&	0.206†††&	merha††&†	0.530††\\merha†††&	0.158††&zakef†††&	0.231†††\\munach††&†0.147†&munach†&††	0.179†††\\kadma†††& 0.036††&kadma†††&	0.040†††\\
\hline
\end{tabular}
\caption{Mean average precision for different te'amim based on the
alignment distances.}
\label{table:newprecisions}
\end{center}
\end{table}

In our analysis, we focus on various key \teamim{}. Firstly, the {\it etnachta}.
Analyzing the distances between Moroccan renditions, as shown
in the left histogram in Figure~\ref{fig:histogram-etnachta}, one
finds increased melodic variation while the Hungarian interpretation
shows greater melodic stability. This is significant, as {\it etnachta} is
an example of a disjunctive {\it ta'am}, one which has a very clear
functionality as a syntactical divider within a given sentence. One
might discern from this data that it supports the argument that
notational traditions with clear melodic formulae
(i.e., christian plainchant) have considerably more influence on the rendition
of the {\it te'amim} in the Hungarian tradition than in the Moroccan counterpart.


Unlike the Hungarian interpretation of Torah trope the Moroccan
interpretation does not present a different, distinct melody for every
{\it ta'am}. That being said, in certain {\it te'amim}, like in the version of
{\it etnachta}, a greater amount of melodic variability is presented.  This
is not mirrored in the example of {\it tipha}, which usually acts as the
second part of the conjunctive pair {\it merha tipha}, which serves to
combine words to make a clear syntactical unit.  In both Hungarian and
Moroccan variants this {\it ta'am} shows a greater degree of stability.
This shows that certain conjunctive {\it te'amim}, which show greater
melodic stability, might also act as more stable syntactical anchors
in both traditions.  One might investigate if this is also true in
other traditions of Torah trope (Iranian, Yemenite and Lithuanian).

\begin{table} 
\begin{center}
\begin{tabular}{|lr|lr|}
\hline
 Gesture   &  Average  &  Gesture   &   Average   \\
 (Hungary)   &  Precision   & (Morocco)   &   Precision       \\
    &   (Hungary)  &    &   (Morocco)       \\
\hline
 tipha     &    0.662  &  katon     &  0.453  \\
 pashta    &    0.647  &  mapah     &  0.347  \\
 mapah     &    0.641  &  tipha     &  0.303  \\
 katon     &    0.604  &  sofpasuq  &  0.285  \\
 etnachta  &    0.601  &  pashta    &  0.242  \\
 sofpasuq  &    0.591  &  merha     &  0.251  \\
 merha     &    0.537  &  etnachta  &  0.150  \\
 revia     &    0.372  &  zakef     &  0.125  \\
 zakef     &    0.201  &  revia     &  0.091  \\
 kadma     &    0.200  &  kadma     &  0.043  \\
\hline
\end{tabular}
\caption{Average precision for different signs as reported in Ness et al 2010}
\label{table:precisions}
\end{center}
\end{table}





Table ~\ref{table:precisions} shows the average precision for
particular signs for two recordings of the same excerpt from the Torah
- one from Hungary and one from Morocco. Each recordings contains
approximately 130 realizations of each sign with a total of 12 unique
signs. Two pitch contours are considered relevant to each other if
they are annotated by the same sign. For each ``query'' contour we
return a list of results which are the pitch contours sorted by the
alignment cost of the DTW. Average precision emphasizes returning more
relevant contours earlier. It is the average of precisions computed
after truncating the list of returned results after each of the
relevant documents in turn. Unlike traditional retrieval systems where
the mean average precision can be used to characterize the overall
system performance in our cases we are more interested in the
individual difference in precision among different signs. These
differences show which signs have well-defined gestural
characteristics and which signs are not interpreted
consistently. Ultimately the numbers are only meaningful after careful
interpretation by an expert. For example based on Table
~\ref{table:precisions} one can infer that the performer in the
Hungarian version had more consistent interpretations of the signs
than the performer in the Moroccan version.



\subsection{Evaluation}
This distance measure is evaluated using evaluation measures from
information retrieval, notably the mean average precision, which is
the average precision of all relevant items for all queries, taking
each segment as query and taking all segments that are a rendition of
the same ta'am as relevant items.

The obtained mean average precisions are 0.644 for the Hungarian
rendition and 0.309 for the Moroccan one, which are improvements
concerning the results that were previously achieved in Ness et al.,
2010.

The findings are particularly interesting when observed in connection
with musicological and music historical studies of Torah trope. It has
long been known that the variety of melodic formulae in Ashkenazi
trope exceeded that of Sephardic trope renditions. The te'amim
actually entail more symbols than necessary for syntactical
divisions. Therefore it is clear that part of their original function
was representational. Such qualities might have been lost or
homogenized by later generations, especially in Sephardic communities,
in which many of the te'amim are identical in their melodic
structure. Simultaneously one can see how the Ashkenazi trope melodies
show a definite melodic stability. Observing the trope melodies for
sof pasuq and tipha in the Hungarian tradition, one can derive that
they inhibit a definite melodic stability. For the sof pasuq we obtain
a mean average precision as high as 0.994 and for the tipha 0.673 (for
comparison, the figures for the Moroccan performance are 0.550 and
0.306 respectively). This indicates that the 17 sof pasuqs are both
similar to each other and distinct from all other te'amim. The same
applies to a somewhat lesser extent to the 24 tiphas. Such a melodic
stability might have been due to the influence of Christian Chant on
Jewish communities in Europe, as is the thesis of Hanoch Avenary
(1978). Simultaneously, our approach using two structurally important
pitches also corresponds to the possible influence of recitation and
final tone as being primary tonal indicators within Askenazi chant
practice, thereby allowing for a greater melodic stability per trope
sign than in Sephardic chant.

\section{Future Work}

In the current study, we took the two most prevalent pitches for
scaling. There are reasons to assume that for various performance
traditions different numbers of pitches are of structural
importance. We will investigate this in future research.  The
presented method proves useful for the two recordings under
investigation. In a next stage, we will collect much more data, with
the aim to study stability and variation between and within
performance traditions of Torah trope on a large scale, integrating
the results into ongoing musicological and historical research on this
topic.

%% \section{Random emails} 


%% The histograms give summaries of the data, while Steven's interface
%% can be used to see what is going on on the level of individual chant
%% segments. These two views on the data should of course be
%% related. Maybe hypotheses that are raised by the one can be confirmed
%% or rejected by the other? It would be interesting to find a way to let
%% these two views on the data interact.

%% I'm still looking at it myself. An interesting case is the
%% sofpasuq. In the histogram of both renditions, there are two
%% spikes. Be aware that this combined histogram is not an addition of
%% the separated histograms (msofpasuq and hsofpasuq) but also shows the
%% distances between hungarian and moroccan sofpasuqs (the right
%% spike). This could mean that within the hungarian rendition as well as
%% within the moroccan rendition the sofpasuqs are quite similar to each
%% other (though within the hungarian rendition they are more similar),
%% while the hungarian rendition of the sofpasuq is in general dissimilar
%% from the moroccan rendition.

%% This is interesting. This would indicate that their is a high degree
%% of similarity in a "cadence" te'am (sof pasuq means "end of
%% sentence"). Now it would be good to compare this to, for example
%% etnachta, which is a syntactical divider.


%% \subsection{F0 Estimation}


%% After the segments have been hand-labeled and identified, the
%% fundamental frequency (``F0'' in this case equivalent to pitch) and
%% signal energy (related to loudness) are calculated for each segment as
%% functions of time. We use the SWIPEP fundamental frequency estimator
%% \cite{camachophd} with all default parameters except for upper and
%% lower frequency bounds that are hand-tuned for each example. For
%% signal energy we simply take the sum of squares of signal values in
%% each non-overlapping 10-ms rectangular window.

%% The SWIPEP algorithm \cite{camachophd} uses an algorithm that is
%% related to autocorrelation, and using a cosine as the kernel, performs
%% an integral transform of the spectrum.  Unlike autocorrelation, which
%% uses the square of the magnitude of the spectrum, SWIPEP uses the
%% square root of the magnitude of the spectrum.  SWIPEP also modifies
%% the cosine kernel in order to avoid some of the problems associated
%% with autocorrelation.  These involve first zeroing the first quarter
%% of the first cycle of the cosine, this allows it to avoid the maximum
%% value at zero lag that occurs when using autocorrelation.  It then
%% avoids the periodicity that autocorrelation experiences when analyzing
%% periodic signals by multiplying the kernel by a 1/f envelope.  To
%% force the width of the main spectral lobes to match the width of the
%% positive cosine lobes, it also normalizes the cosine kernel and
%% applies a pitch-dependant window size.

%% The SWIPEP algorithm is a powerful algorithm for estimating the
%% fundamental frequency of audio signals, and in a comparison against
%% twelve other leading F0 estimation algorithms it outperformed all of
%% them \cite{camachophd}.  It performs especially well when compared
%% against the traditional autocorrelation approach in that it makes less
%% errors, including less octave errors, a common problem encountered
%% when using traditional autocorrelation.



%% \begin{figure}[t]
%% \includegraphics[width=120mm]{f0contour}
%% \caption{F0 Contour : The top half of the graph shows the F0 contour
%%   as estimated by the SWIPEP algorithm.  The x-axis shows time in
%%   seconds from the start of the audio file, and the y-axis shows the
%%   pitch of the contour in MIDI note numbers.  The bottom half of the
%%   graph shows the signal energy of the audio, with the x-axis
%%   describes time in seconds, and the y-axis shows the energy of the
%%   audio in decibels.  }
%% \label{fig:contour}
%% \end{figure} 


%% \subsection{Quantization in Pitch}

%% Following the pitch contour extraction is pitch quantization, which is
%% the discretization of the continuous pitch contour into discrete notes
%% of a scale. Rather than externally imposing a particular set of
%% pitches, such as an equal-tempered chromatic (the piano keys) or
%% diatonic scale, we have developed a novel method for extracting a
%% scale from an F0 envelope that is continuous (or at least very densely
%% sampled) in both time and pitch. Our method is inspired by Krumhansl's
%% time-on-pitch histograms adding up the total amount of time spent on
%% each pitch \cite{krumhansl90}. We demand a pitch resolution of one
%% cent \footnote{One cent is 1/100 of a semitone, corresponding to a
%%   frequency difference of about 0.06\%}, so we cannot use a simple
%% histogram. Instead we use a statistical technique known as
%% non-parametric kernel density estimation, with a Gaussian kernel
%% \footnote{Thinking statistically, our scale is related to a
%% distribution given the relative probability of each possible
%% pitch. We can think of each F0 estimate (i.e each sampled value of
%% the F0 envelope) as a sample drawn from this unknown distribution so
%% our problem becomes one of estimation the unknown distribution given
%% the samples}. More specifically a Gaussian (with standard deviation
%% of 33 cents) is centered on each sample of the frequency estimate and
%% the Gaussians of all the samples are added to form the kernel density
%% estimate. The resulting curve is our density estimate; like a
%% histogram, it can be interpreted as the relative probability of each
%% pitch appearing at any given point in time. Figure ~\ref{fig:scale} shows this
%% method's density estimate given the F0 curve from Figure ~\ref{fig:contour}.

%% In quantizing the pitch, the size of the excerpt chosen can influence
%% both the number of peaks and the location of these peaks.  In the
%% current work, we have chosen to use the entire file as the dataset for
%% doing pitch quantization.  In the user interface presented below, we
%% allow the user to choose subsets of signs which can then be viewed at
%% different levels of quantization granularity.  This process is
%% necessary because of the two step nature of our analysis process,
%% where first the audio file is analyzed, and then this analysis is
%% presented to the user, who can then perform further analysis on the
%% audio.  We are in the process of developing a new interface that will
%% overcome this drawback, and will allow the user to directly interact
%% with the first analysis procedure.


%% \subsection{Scale-Degree Histogram}

%% We interpret each peak in the density estimate as a note of the
%% scale. We restrict the minimum interval between scale pitches
%% (currently 80 cents by default) by choosing only the higher peak when
%% there are two or more very close peaks. This method's free parameter
%% is the standard deviation of the Gaussian kernel, which provides an
%% adjustable level of smoothness to our density estimate; we have
%% obtained good results with a standard deviation of 33 cents. Note that
%% this method has no knowledge of octaves.


%% Once we have determined the scale, pitch quantization is the trivial
%% task of converting each F0 estimate to the nearest note of the scale.
%% In our opinion these derived scales are more true to the actual nature
%% of pitch-contour relationships within oral/aural and semi-notated
%% musical traditions. Instead of viewing these pitches to be deviations
%% of pre-existing ``normalized'' scales our method defines a more
%% differentiated scale from the outset. With our approach the scale
%% tones do not require ``normalization'' and thereby exist in an
%% autonomous microtonal environment defined solely on statistical
%% occurrence of pitch within a temporal unfolding of the given melodic
%% context. Once the pitch contour is quantized into the
%% recording-specific scale calculated using Kernel density estimation,
%% we can calculate how many times a particular scale degree appears
%% during an excerpt. The resulting data is a scale-degree histogram
%% which is used create simplified abstract visual contour
%% representations.




%% \begin{figure}[htb]
%% \centering
%% \includegraphics[width=60mm]{cantillion-30pashta-histogramlevels}
%% \caption{Melodic contours at different levels of abstraction (top:
%%   original, middle: quantized, bottom: simplified using 3 most
%%   prominent scale degrees}
%% \label{fig:contours_histogram} 
%% \end{figure} 


%% \subsection{Histogram-Based Contour Abstraction}

%% The basic idea of histogram-based contour abstraction is to only use
%% the most salient discrete scale degrees (the histogram bins with the
%% highest magnitude) as significant points to simplify the
%% representation of the contour. By adjusting the number of prominent
%% scale degrees used to represent the simplified representation the
%% researchers can view/listen to the melodic contour at different levels
%% of abstraction and detail. Figure ~\ref{fig:contours_histogram} shows
%% an original continuous contour, the quantized representation using the
%% recording-specific derived scale and the abstracted representation
%% using only the 3 most prominent scale degrees.



%% \begin{figure*}[t]
%% \centering
%% \subfigure[F0 Contour of 11 Pashta]
%% {
%%     \label{fig:sub:contour-11pashta}
%%     \includegraphics[width=2cm,height=2cm]{contour-11pashta.ps}
%% }
%% \hspace{1cm}
%% \subfigure[F0 Contour of 42 Pashta]
%% {
%%     \label{fig:sub:contour-42pashta}
%%     \includegraphics[width=2cm,height=2cm]{contour-42pashta.ps}
%% }
%% \hspace{1cm}
%% \subfigure[F0 Contour of 18 Sof Pasuq] 
%% {
%%     \label{fig:sub:contour-18sofpasuq}
%%     \includegraphics[width=2cm,height=2cm]{contour-18sofpasuq.ps}
%% }
%% \hspace{1cm}
%% \subfigure[F0 Contour of 11 Pashta Doubled]
%% {
%%     \label{fig:sub:contour-11pashta-doubled}
%%     \includegraphics[width=4cm,height=2cm]{contour-11pashta-doubled.ps}
%% }
%% \caption{
%% F0 contours of 4 different gestures from a Torah recitation recorded
%% in Hungary.  The first two show different versions of the pashta
%% gesture (11 pashta and 42 pashta) and the third shows the gesture for
%% sof pasuq (18 sof pasuq).  The last is a version of the first pashta
%% gesture (11 pashta) with each audio sample doubled, which effectively
%% stretches the contour by a factor of two.
%% }
%% \end{figure*}

%% \begin{figure*}[t]
%% \centering
%% \subfigure[DTW of 11 Pashta vs 11 Pashta]
%% {
%%     \label{fig:sub:dtw-11pashta-11pashta}
%%     \includegraphics[width=2cm,height=2cm]{dtw-11pashta-11pashta.ps}
%% }
%% \hspace{1cm}
%% \subfigure[DTW of 11 Pashta vs 42 Pashta]
%% {
%%     \label{fig:sub:dtw-11pashta-42pashta}
%%     \includegraphics[width=2cm,height=2cm]{dtw-11pashta-42pashta.ps}
%% }
%% \hspace{1cm}
%% \subfigure[DTW of 11 Pashta vs 18 Sof Pasuq] 
%% {
%%     \label{fig:sub:dtw-11pashta-18sofpasuq}
%%     \includegraphics[width=2cm,height=2cm]{dtw-11pashta-18sofpasuq.ps}
%% }
%% \hspace{1cm}
%% \subfigure[DTW of 11 Pashta vs 11 Pashta Doubled]
%% {
%%     \label{fig:sub:dtw-11pashta-11pashta-doubled}
%%     \includegraphics[width=4cm,height=2cm]{dtw-11pashta-11pashta-doubled.ps}
%% }
%% \caption{
%% Shown above are Similarity Matrices of the above four gestures
%% compared with the first pashta gesture.  Superimposed on the figures
%% is the Dynamic Time Warping curve showing the optimally matching path
%% between the two songs.
%% }
%% \end{figure*}

%% In the next section we show that these simplified abstract contour
%% representations result in better retrieval performance than the
%% original ``continuous'' pitch contours.


%% One of the main aspects in the studying of signs in the context of
%% chant and recitation is to what extent they convey gesture information
%% that is invariant with respect to the underlying text. To study this
%% question it was necessary to develop a method to compare the pitch
%% contours of different realizations from different parts of the audio
%% recording of the same sign.

%% To our knowledge, our use of the Histogram-Based Contour abstraction
%% is novel, the classical approach is that of an interval-based contour
%% abstraction of the pitch contour.  This interval-based method is a
%% commonly used method in Query-by-Humming experiments
%% (QBH)\cite{dannenberg2007}.  The interval-based contour abstraction
%% representation simplifies a contour by describing the relation of each
%% tone to the next in terms of how many scale degrees exist between one
%% note and the next.  For example, using the note sequence AED, there
%% would be a step of +4 between the A and E, and a step of -1 between
%% the E and D.  This representation can then be simplified by quantizing
%% it to a smaller number of step.  The simplest interval abstraction has
%% three levels, ``goes up'' (+1), ``goes down''(-1), and ``remains the
%% same'' (0).  One can then successively subdivide the upper and lower
%% ranges, giving 5 levels, 9 levels, 11 levels, and so on.  The result
%% of interval-based quantization is a string of the quantized interval
%% differences between one pitch value and then next.  The resulting
%% strings of values for each trope are then compared to one another
%% using the technique of Dynamic Time Warping, as described in the next
%% section.

%% \subsection{Dynamic Time Warping for Contour Similarity Calculation} 

%% Dynamic Time Warping (DTW) is a technique by which the similarity
%% between two different time sequences can be measured. It allows a
%% computer to find an optimal match between two sequences by performing
%% a non-linear warping of one sequence to the other. The technique of
%% dynamic programming is used for efficient implementation. An example
%% of DTW in Music Information Retrieval is to compare the tempo
%% variations between two different performances of a classical
%% symphony. The DTW algorithm would identify the parts of the two
%% symphonies that were played at the same tempo as a diagonal line, with
%% the line varying above and below the diagonal when the tempo was
%% different between the two pieces.

%% First the similarity matrix between the two pitch contours we are
%% comparing is calculated.  Based on the calculated similarity matrix
%% the DTW algorithm finds the optimal alignment path of the two sequences
%% and calculates the cost of that alignment.  When the contours are
%% similar the alignment cost will be small compared to when the contours
%% are dissimilar. The matching process is pitch shift invariant and
%% allows variations and tempo stretching. That way for any particular
%% sign (pitch contour) we can sort the sign (pitch contours) by
%% similarity.

%% \subsection{Plotting and Recombining the Segments}

%% To illustrate the technique we use the gestures of two separate
%% annotated recordings of a section of the Torah. One of these was
%% recorded in Morocco, and the other was recorded in Hungary. Figures
%% ~\ref{fig:sub:contour-11pashta}, ~\ref{fig:sub:contour-42pashta},
%% ~\ref{fig:sub:contour-18sofpasuq} and
%% ~\ref{fig:sub:contour-11pashta-doubled} show the F0 contour of the
%% sections of the audio file from a Torah recording from Hungary.
%% Figure ~\ref{fig:sub:contour-11pashta} shows a pashta sign, Figure
%% ~\ref{fig:sub:contour-42pashta} shows another pashta sign from
%% further along in the audio file.  Figure
%% ~\ref{fig:sub:contour-18sofpasuq} shows a sof pasuq gesture and Figure
%% ~\ref{fig:sub:contour-11pashta-doubled} shows the first pashta
%% gesture, but with the sample stretched by a factor of two.

%% The figures
%% \ref{fig:sub:dtw-11pashta-11pashta},\ref{fig:sub:dtw-11pashta-42pashta}
%% , \ref{fig:sub:dtw-11pashta-18sofpasuq} and
%% \ref{fig:sub:dtw-11pashta-11pashta-doubled} show Similarity Matricies
%% and the alignment paths computed using DTW for these four gestures
%% compared to the first pashta gesture. White areas are highly similar
%% and black areas have low similarity. In Figure
%% \ref{fig:sub:dtw-11pashta-11pashta} the first pashta gesture is
%% compared to itself.  The DTW curve is overlaid in black and is
%% basically a straight diagonal line from one corner to the opposite
%% corner, showing that the optimal path between the start and the end of
%% the file is a direct alignment of one file to the other.  Figure
%% \ref{fig:sub:dtw-11pashta-11pashta-doubled} shows a similar behavior,
%% except that the slope of the line is shallower.  Figure
%% \ref{fig:sub:dtw-11pashta-42pashta} shows the comparison of one pashta
%% gesture to another.  This path had a DTW cost of 23.8442.  Figure
%% \ref{fig:sub:dtw-11pashta-18sofpasuq} shows an alignment between the
%% pashta gesture and a sof pasuq gesture.  One can see that the line is
%% not only not diagonal, but that the line is often on dark areas which
%% denote high alignment cost.




%% We have also investigated the retrieval effectiveness of quantized
%% contour representations at different levels of abstraction using the
%% approach described above. In this case it makes sense to use Mean
%% Average Precision across queries to explore what is the best level of
%% abstraction for this task.

%% This first DTW analysis was conducted using the continuous pitch
%% values determined by the SWIPEP algorithm.  We then extended this
%% analysis by quantizing the pitch contours, calculating the pairwise
%% score between each contour and then calculating the mean average
%% precision recall.  We did this for all possible number of histogram
%% bins, from the maximum number of scale degrees of 13, down to only the
%% most popular histogram bin.  We then repeated this analysis with notes
%% from a western equal-tempered scale.  The total range of notes was
%% from the A2\# (the A\# two octaves below middle C) to C4 (middle C).
%% This gave a total of 16 semitones, of which we used the most common 13
%% scale degrees.  For all of these possible histogram bin numbers, we
%% converted all notes to these quantized values and did a pairwise DTW
%% comparison between all of them.  We then calculated the mean average
%% precision recall for each histogram bin quantization level.  These
%% results are presented in Figure \ref{fig:histogram-equal-continuous}.

%% From this graph and Table ~\ref{table:simplify}, we can see that the
%% optimal number of histogram bins is 2 when notes are quantized to our
%% derived scale.  The mean average precision recall at this level is
%% 0.493.  After this, the curve quickly drops, and then remains at a
%% steady state level of approximately 0.41. This is significantly better
%% than using the ``continuous'' contour mean average precision of
%% 0.2951.  The term continuous contour refers to when the original
%% contour of the song is quantized to the closest equivalent
%% equal-tempered, or MIDI, note.  When we quantize the notes to the
%% equal-tempered scale, the maximum value of 0.443 is also obtained with
%% 2 histogram bins.  It is important to note that the value of 0.493
%% that is derived when the data-driven approach of using the notes that
%% are actual chanted is higher than the value derived from using the
%% equal-tempered scale, and this can be easily understood by realizing
%% that the singers do not tune themselves to a western scale.  This
%% shows the fundamental utility of our method of deriving the quantized
%% scale from the notes that are actually sung.

%% These results are shown in a more intuitive way in Figure
%% \ref{fig:simplify-cont}.  In this figure three ``sof pasuq'' and three
%% ``pashta'' contours were chosen, and were quantized to the derived,
%% data-driven scale using the optimal value of 2 histogram bins.  One
%% can see that the ``sof pasuq'' contours have quite a different shape.
%% This visualization shows the utility of our approach.

%% In order to compare these results with those of the classical method
%% of Interval-based Contour Abstraction (IBCA), we first quantized the
%% continuous contour using either the data-derived scale mentioned
%% previously or to an equal tempered scale.  We then generated the
%% interval-based contour abstraction for each trope using first 3, then
%% 5, 7, 9, 11 and 13 different interval quantization levels.  These
%% strings of interval differences for each trope were then compared
%% against each other using the same Dynamic Time Warping (DTW) technique
%% used above.  Average precision-recall values were then generated for
%% each of the interval quantization levels.  The results of this are
%% shown in Table ~\ref{table:simplify} and are presented graphically in
%% Figure \ref{fig:histogram-equal-continuous}.

%% From these results, one can immediately see that our proposed method
%% of Histogram-based Contour Abstraction (HBCA) outperforms the
%% traditional method of Interval-based Contour Abstraction (IBCA) by a
%% large margin.  In addition, one can see a small improvement in the
%% IBCA approach when using a scale derived from the data, as opposed to
%% using an equal-tempered scale.  However, it must be stated that this
%% is a very small difference, and may not be statistically significant.
%% Further investigation in this area with larger sample sizes is
%% required.

%% \begin{figure}[htb]
%% \begin{center}
%% \includegraphics[width=80mm,angle=270]{results-histogram-equal-continuous}
%% \end{center}
%% \caption{ Mean average precision recall when quantizing the notes
%%   before DTW analysis.  Shown are the results for quantizing to a song
%%   specific scale (Histogram derived scale) versus an equal tempered
%%   scale (MIDI notes) for both Histogram-based Contour Abstraction
%%   (HBCA) and Interval-based Contour Abstraction (IBCA) approaches.  In
%%   addition, the mean average precision recall in the continuous case
%%   is also shown.}
%% \label{fig:histogram-equal-continuous} 
%% \end{figure} 


%% \begin{figure}[htb]
%% \begin{center}
%% \includegraphics[width=100mm]{simplify-cont}
%% \end{center}
%% \caption{Comparison of contour quantized to the two most prevalent
%%   scale degrees in a data-driven approach to the original continuous
%%   contour.  Shown are three examples of the signs ``sof pasuq'' and
%%   ``pashta''}
%% \label{fig:simplify-cont} 
%% \end{figure} 

%% \begin{table} 
%% \begin{center}
%% %\begin{tabular}{|r|r|r|} \hline
%% %Number &  Data   & Equal \\
%% %of Bins & Driven & Temperment \\  \hline
%% %  1  &  0.1931  &  0.26581  \\
%% %  2  &  0.4932  &  0.44356  \\
%% %  3  &  0.4479  &  0.35044  \\
%% %  4  &  0.4057  &  0.37572  \\
%% %  5  &  0.4097  &  0.39797  \\
%% %  6  &  0.4061  &  0.41386  \\
%% %  7  &  0.4026  &  0.41350  \\
%% %  8  &  0.3941  &  0.41791  \\
%% %  9  &  0.3953  &  0.41655  \\
%% % 10  &  0.3947  &  0.41931  \\
%% % 11  &  0.3948  &  0.41584  \\
%% % 12  &  0.3948  &  0.41594  \\
%% % 13  &  0.3948  &  0.41617  \\ \hline
%% %\end{tabular}

%% \begin{tabular}{|r|r|r|r|r|}
%% \hline
%% Number &  HBCA    & HBCA Equal &  IBCA    & IBCA Equal \\
%% of Bins & Data Driven & Temperment &  Data Driven   & Temperment \\  \hline
%%      1  &  0.1931  &  0.2658  &    &    \\
%%      2  &  0.4932  &  0.4435  &    &    \\
%%      3  &  0.4479  &  0.3504  & 0.1684   &  0.1663  \\
%%      4  &  0.4057  &  0.3757  &    &    \\
%%      5  &  0.4097  &  0.3979  & 0.1575   &  0.1456  \\
%%      6  &  0.4061  &  0.4138  &    &    \\
%%      7  &  0.4026  &  0.4135  & 0.1605   & 0.1490   \\
%%      8  &  0.3941  &  0.4179  &    &   \\
%%      9  &  0.3953  &  0.4165  & 0.1629   & 0.1522   \\
%%     10  &  0.3947  &  0.4193  &    &    \\
%%     11  &  0.3948  &  0.4158  &  0.1571  &  0.1503  \\
%%     12  &  0.3948  &  0.4159  &    &    \\
%%     13  &  0.3948  &  0.4161  & 0.1606   &  0.1488  \\
%% \hline
%% \end{tabular}


%% \caption{Table of mean average precision values when quantizing the
%%   notes before DTW analysis.  Shown are the calculated values for the
%%   Data-driven and Equal-temperment approaches using both the
%%   Histogram-based Contour Abstraction (HBCA) and Interval-based
%%   Contour Abstraction (IBCA) approaches}
%% \label{table:simplify}
%% \end{center}
%% \end{table}










\bibliography{ismir2011chantgtzan}

\end{document}

% -----------------------------------------------
% Template for ISMIR 2011
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2011,amsmath,cite}
\usepackage{graphicx}
\usepackage{url} 
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{tabularx}

% Title.
% ------
\title{Music Information Robotics: Coping Strategies for Musically Challenged Robots}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors	
{Steven Ness, Shawn Trail} {University of Victoria \\ {\tt sness@sness.net} \\ {\tt shawntrail@gmail.com}}	
{Peter Driessen} {University of Victoria \\ {\tt peter@ece.uvic.ca}}
{Andrew Schloss, George Tzanetakis} {University of Victoria \\ {\tt aschloss@uvic.ca} \\ {\tt gtzan@cs.uvic.ca }}

\begin{document}

\maketitle
%
\begin{abstract}
  In the past few years there has been a growing interest in music
  robotics. Robotic instruments that generate sound acoustically using
  actuators have been increasingly developed and used in performances
  and compositions over the past 10 years. Although such devices can
  be very sophisticated mechanically, in most cases they are passive
  devices that directly respond to control messages from a
  computer. In the few cases where more sophisticated control and
  feedback is employed it is in the form of simple mappings with
  little musical understanding. Several techniques for extracting
  musical information have been proposed in the field of music
  information retrieval. In most cases the focus has been the batch
  processing of large audio collections rather than real time
  performance understanding. In this paper we describe how such
  techniques can be adapted to deal with some of the practical
  problems we have experienced in our own work with music robotics. Of
  particular importance is the idea of self-awareness or
  proprioception in which the robot(s) adapt their behavior based on
  understanding the connection between their actions and sound
  generation through listening. More specifically we describe
  techniques for solving the following problems: 1) controller mapping
  2) velocity calibration, and 3) gesture recognition.

\end{abstract}

\section{Introduction}

There is a long history of mechanical devices that generate acoustic
sounds without direct human interaction starting from mechanical birds
in antiquity to sophisticated player pianos in the early 19th century
that could perform arbitrary scores written in piano roll
notation. Using computers to control such devices has opened up new
possibilities in terms of flexibility and control while
retaining the richness of the acoustic sound associated with actual
musical instruments. The terms music robots or music robotic
instruments have been used to describe such devices \cite{kapur05}.

We believe these new robotic instruments have a legitimate place with
potential to become part of an embedded conventional musical practice,
not just a research curiosity. While musical-robotics might seem niche
and esoteric at this point \cite{burtner04}, historic innovations such as
monophonic to polyphonic music, electrical amplification of the
guitar, or computers in the recording studio all brought skepticism,
but eventually became mainstay practices.

Although such music robots have been used in performance of both 
composed and improvised music as well as with or without human
performers sharing the stage, they are essentially passive output
devices that receive control messages and in response actuate sound
producing mechanisms. Their control is typically handled by software
written specifically for each piece by the composer/performer. 

Musicians through training acquire a body of musical concepts commonly
known as musicianship. Machine musicianship \cite{rowe01} refers to
the technology of implementing musical process such as segmentation,
pattern processing and interactive improvisation in computer
programs. The majority of existing work in this area has focused on
symbolic digital representations of music, typically MIDI. The growing
research body of music information retrieval, especially audio-based,
can provide the necessary audio signal processing and machine learning
techniques to develop machine musicianship involving audio signals.

The typical architecture of interactive music robots is that the
control software receives symbolic messages based on what the other
performers (robotic or human) are playing as well as messages from
some kind of score for the piece. It then sends control messages to
the robot in order to trigger the actuators generating the acoustic
sound. In some cases the audio output of the other performers is
automatically analyzed to generate control messages. For example audio
beat tracking can be used to adapt to the tempo played.

Self listening is a critical part of musicianship as anyone who has
struggled to play music on a stage without a proper monitor setup has
experienced. However this ability is conspicuously absent in existing
music robots. One could remove the acoustic drum actuated by a solenoid so
that no sound would be produced and the robotic percussionist will
continue ``blissfully'' playing along.

\begin{figure}[t]
\begin{center}
\includegraphics[width=75mm]{robotdrum_setup}
\end{center}
\caption{The experimental setup for our robotic based frame drum
  experiments. In the foreground, three frame drums are shown with
  solenoids placed to ensure optimal striking of the drum surface.  In
  the background of the picture, the control system is
  shown.}
\label{fig:robotdrum_setup} 
\end{figure} 

This work has been motivated by practical problems experienced in a
variety of performances involving percussive robotic
instruments. Figure ~\ref{fig:robotdrum_setup} shows our experimental
setup in which solenoid actuators supplied by Karmetik LLC. 
\footnote{\url{http://karmetik.com}} are used to excite different types of frame
drums.
% 

We show how the ability of a robot to ``listen''
especially to its own acoustic audio output is critical in addressing
these problems and describe how we have adapted relevant music
information retrieval techniques for this purpose. More specifically,
we describe how self-listening can be used to automatically map
controls to actuators as well as how it can be used to provide
self-adapting velocity response curves. Finally, we show how pitch
extraction and dynamic time warping can be used for high-level gesture
analysis in both sensor and acoustic domains.


\section{Related Work} 

An early example of an automated, programmable musical instrument
ensemble was described by al-Jazari (1136-1206) a Kurdish scholar,
inventor, artist, mathematician that lived during the Islamic Golden
Age (the Middle Ages in the west). Best known for writing the Book of Knowledge of
Ingenious Mechanical Devices in 1206, his automata were described as
fountains on a boat featuring four automatic musicians that floated on
a lake to entertain guests at royal drinking parties. It had a
programmable drum machine with pegs (cams) that bumped into little
levers that operated the percussion. The drummer could be made to play
different rhythms and different drum patterns if the pegs were moved
around, performing more than fifty facial and body actions during each
musical selection. This was achieved through the innovative use of
hydraulic switching. A modern example of a robotic musical ensemble is
guitarist Pat Metheny's Orchestrion which was specifically influenced
by the Player Piano
\footnote{\url{http://www.patmetheny.com/orchestrioninfo/}}.  Metheny cites his
grandfather's player piano as being the catalyst to his interest in
Orchestrions, which is a machine that plays music and is designed to
sound like an orchestra or band.

A seminal book in this field is ``Machine Musicianship''
\cite{rowe01}, in which one of the sections describes a comprehensive
system for the composition, creation and performance between humans
and robots.  Rowe describes improvisational and composition systems
that combine features of music feature extraction, musical analysis
and interactivity to generate engaging experiences for the audience.
In our work, the integration of machine musicianship and music
robotics has been used to develop a robotic percussionist that can
improvise with a human performer playing a sitar enhanced with digital
sensors \cite{kapur05a}. 

%It has also been used to create a robot that
%listens to live players, analyzes perceptual aspects of their playing
%in real-time, and uses the product of this analysis to play in a
%collaborative and improvisatory manner \cite{weinberg06}.

Another work closely related to ours is the \textit{Shimon}
human-robot based Jazz improvisation system \cite{hoffman10} that uses
a gesture based framework that recognizes that musicianship involves
not just the production of notes, but also of the intentional and
consequential communication between musicians \cite{hoffman10a}.
%% Another work closely related to this melodic percussion system has
%% also been described by Weinberg and Driscoll \cite{weinberg07}, in
%% which a detailed theoretical framework that combines visual and
%% acoustic information to provide for an improvisational framework
%% between humans and robots.  This work is relevant to ours in that it
%% provides a theoretical framework in which to design and evaluate
%% robotic musician systems.  
%Petersen et. al \cite{petersen08} describe
%another system that also uses a gesture based interface to enable the
%interaction of a robotic flautist with human performers.  This system
%uses a combination of visual and auditory input to create a hands free
%gesture based system that was described to meet the specific technical
%and idiosyncratic requirements of a professional musician.  

%The loop of input, configuration and
%control is crucial to the design of robot musicians, and this process
%is systematized in paper a by Singer et al. \cite{singer03} in which a
%string playing robot called the Guitarbot is described.  

Our system also uses these same basic building blocks, but adds the
power of machine learning and ``proprioception'' to the process, enabling
the robot itself to perform many of the time consuming mapping and
calibration processes that are often performed by hand in performance
situations.  In this context, a mapping refers to the process of
determining which controller output activates which solenoid.  In the
next section we describe how some practical recurring problems we have
experienced with robots in music performance robots have led to the
development of signal processing and machine learning techniques
informed by music information retrieval ideas.


\section{Motivation} 

Our team has extensive experience designing music robotic instruments,
implementing control and mapping strategies, and using them in live
and interactive performances with human musicians, frequently in an
improvisatory context. In addition two of the co-authors are
professional musicians who have regularly performed with robotic
instruments. One of the most important precursors to any musical
performance is the sound check/rehearsal that takes place before a
concert in a particular venue. During this time the musicians setup
their instruments, adjust the sound levels of each instrument and
negotiate information specific to the performance such as positioning,
sequencing and cues. A similar activity takes place in performance
involving robotic acoustic instruments in which the robots are set up,
their acoustic output is calibrated and adjusted to the particular
venue and mappings between controls and gestures are established. This
process is frequently tedious and typically requires extensive manual
intervention. To some extent this paper can be viewed as an attempt
to utilize techniques and ideas from MIR to simplify and automate this
process.  This is in contrast to previous work in robotic musicianship
that mostly deals with the actual performance. More specifically we
deal with three problems: automatic mapping, velocity calibration, and 
melodic and kinetic gesture recognition. 

The experimental setup that we have used consists of a modular robotic
design in which multiple solenoid-based actuators can be attached to a
variety of different drums. We use audio signal processing and machine
learning techniques to have robotic musical instruments that "listen"
to themselves using a single centrally located microphone. 

It is a time consuming and challenging process to setup robotic
instruments in different venues.  One issue is that of mapping, that
is, which signal sent from the computer maps to which robotic
instrument.  As the number of drums grows, it becomes more challenging
to manage the cables and connections between the controlling computer
and the robotic instruments. The system we propose performs timbre
classification of the incoming audio, automatically mapping solenoids
correctly in real-time to the note messages sent to the musically
desired drum. For example rather than sending an arbitrary control
message to actuator 40 the control message is addressed to the bass
drum and will be routed to the correct actuator by simply
``listening'' to what each actuator is playing in a sound-check
stage. That way actuators can be moved or replaced easily even during
the performance without changes in the control software. The same
approach is also used to detect broken or malfunctioning actuators
that do not produce sound.

When working with mechanical instruments, there is a great deal of
non-linearity and physical complexity that makes the situation
fundamentally different from working with electronic sound, which is
entirely ``virtual'' (or at least not physical) until it comes out of
the speakers. The moving parts of the actuators have momentum, and
changes of direction are not instantaneous. Gravity may also play a
part, and there is friction to be overcome. Frequently actuators are
on separate power supplies which can result in inconsistencies in the
voltage. The compositional process, rehearsal and performance of ``The
Space Between Us'' by by David A. Jaffe, in which Andrew Schloss was
soloist on robotic percussion, involved hand-calibrating every note of
the robotic chimes, xylophone and glockenspiel. This required 18+23+35
separate hand calibrations and took valuable rehearsal time. In this
paper we describe a method for velocity calibration, that is, what
voltage should be sent to a solenoid to generate a desired volume and
timbre from an instrument.  Due to the mechanical properties of
solenoids and drums, a small movement in the relative position of
these two can lead to a large change in sound output.  The most
dramatic of these is when during performance a drum moves out of place
enough that a voltage that at the start of the performance allowed the
drum to be hit now fails to make the drum sound.  Depending on the
musical context, this can be disastrous in a performance context. Good
velocity scaling is essential for a percussion instrument to give a
natural graduated response to subtle changes in gesture, e.g. a slight
increase in the strength (velocity) of a stroke should not result in a
sudden increase in the loudness of sound.


Issues like velocity calibration or control mapping seem quite
pedestrian, or even trivial until one has grappled with this problem
with real instruments. We believe that the ability of a robotic
instrument to perceive at some level its own functioning is important
in making robust, adaptive systems that do not require regular human
intervention to function properly. We refer to this ability as
``proprioception'' which in its original definition refers to the
ability of an organism to perceive its own status.

Finally we also describe some experiments recognizing melodic and
kinetic gestures at different tempi and with variations in how they
are performed. This can be viewed as an exchange of cues established
before the performance especially in an improvisatory context. This
allows higher-level gestures to be used as cues without requiring
exact reproduction from the human performer interacting with the
robotic instrument and enables a more fluid and flexible structuring
of performances.


  

\section{Experiments}

\subsection{Drum Classification for Automatic Mapping} 

%% For our experiments, we built a system that performs an automatic
%% mapping based on timbre classification. This was straightforward to do
%% using existing Marsyas feature extraction and classification
%% modules. These modules allow complex features to be calculated based
%% on input audio, some of these include Fast Fourier Transform (FFT),
%% Mel-Frequency Cepstral Coefficients (MFCC), Correlogram based
%% approaches \cite{slaney93}

We performed an experiment to investigate the performance of a audio
feature extraction and machine learning system to classify drum sounds
to perform automatic mapping. The audio features used were the well
known Mel-Frequency Cepstral Coefficients (MFCC) calculated with a
window size of 22.3ms.  These were then used as input to a Support
Vector Machine (SVM) machine learning system.  We collected a dataset
of audio with 4 different frame drums being struck by the robot with a
time of 128ms between strikes, then calculated all the MFCC of this
audio, and then found the 8 highest MFCC0 (roughly corresponding to
perceptual loudness) and marked these as onsets in the audio.  The
MFCC feature vectors corresponding to these onsets were used to train
the classifier. A separate test data set was also
collected. Percussive sounds can be challenging to classify as there
is not a lot of steady state spectral information. The results of this
experiment gave a classification accuracy of $66.38\%$, as shown in
the first line (Peak offset 0) in Table \ref{table:precision}.  We
then performed the same experiment but using instead different offsets
from the highest peak in window sizes of 22.3ms.  When we classified
all frames with the frame immediately after the highest peak, we
obtained a classification accuracy of $91.95\%$. We interpret this
result to mean that the resonance after the transient is clearly
distinguishable for different drums, whereas the transient at the
onset is fairly similar for different drums. This performance quickly
degrades as we move away from the onset.

This performance quickly degrades as we move away from
the onset. These results are for individual 22.3ms frames so it is
easy to get $100\%$ correct identification by voting across the entire
recording which can then be used for the automatic mapping. When we
setup the robotic instrument we actuate each solenoid in turn,
classify the audio and then set the appropriate mappings so that the
control software can address the actual frame drums rather than the
actuators.


\begin{table} 
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Peak & Percent & Peak & Percent \\
offset & correct & offset & correct \\
\hline
0 & 66.38 & 4 & 90.52 \\
1 & 91.95 & 5 & 86.49 \\
2 & 91.67 & 6 & 86.49 \\
3 & 91.95 & 7 & 77.59 \\
\hline
\end{tabular}
\caption{Classification accuracy of an SVM classifier The Peak offset
  is the offset from the time the drum is hit.}
\label{table:precision}
\end{center}
\end{table}




\begin{figure}[htb]
\begin{center}
\includegraphics[width=75mm]{bin-average}
\end{center}
\caption{Mapping from calibrated input velocities to output driving
  velocities for different distances}
\label{fig:calibration_mapping} 
\end{figure} 


\subsection{Timbre-Adaptive Velocity Calibration}

The acoustic response of a drum both in terms of perceived loudness
and timbral quality is non-linear with respect to linear increases in
voltage as well as to the distance of the solenoid to the vibrating
surface. In the past calibration was performed manually by listening
to the output and adjusting the mapping of input velocities to voltage
until smooth changes in loudness and timbre where heard. In this
section we describe how to derive an automatic data-driven mapping
that is specific to the particular drum. 

Our first objective is to achieve a linear increase in loudness with
increasing MIDI velocity for a given fixed distance between beater and
drumhead.  However, in practice, the beater may be mounted on a stand
and placed next to the drumhead mounted on a different stand.  Thus
the distance between beater and drumhead will vary depending on setup,
and may even change during a performance.  Thus a second objective is
to achieve a similar loudness versus MIDI velocity (corresponding to
voltage) curve over a range of distances between beater and drumhead.

To achieve these objectives we collected audio for all velocity values
and three distance configuration (near 1cm, medium 2cm, far 3cm). The
loudness and timbre variation possible is captured by computing MFCC
for each strike. More specifically for each velocity value and a
particular distance we obtain a vector of MFCC values. The frequency
of beating was kept constant at 8 strikes per second for these
measurements. The first MFCC coefficient (MFCC0) at the time of onset
is used to approximate loudness. Plots of MFCC0 for the distance
configurations are shown in \ref{fig:sub1:mfcc-values}.

In order to capture some of the timbral variation in addition to the
loudness variation we project our MFCC vectors to a single dimension
(the first principal component) using Principal Component Analysis
(PCA) \cite{jolliffe2002pca}. As can be seen in \ref{fig:sub1:values}
the PCA0 values follow closely the loudness curve. This is expected as
loudness is the primary characteristic that changes with increasing
velocity. However, there is also some information about timbre as can
be seen by the ``near'' plot that has higher variance in PCA0 than in
MFCC0.

Our goal is to obtain a mapping (from user input calibrated velocity
to output driving velocity) such that linear changes in input (MIDI
velocity) will yield approximately linear changes in the perceived
loudness and timbre as experessed in PCA0. We utilize data from all
the three distance configurations for the PCA computation so that the
timbrespace is shared. That way even though we get separate
calibration mappings for each distance configuration they have the
property that the same calibrated input value will generate the same
output in terms of loudness and timbre independently of distance.

In order to obtain this mapping we quantize the PCA0 values for each
distance configuration into 128 bins that correspond to the calibrated
input velocities. The generated mapping is the wrong way i.e from
output driving velocities to calibrated input velocities and is not an
injection (one-to-one function) so it can not be directly inverted. To
invert the mapping for each calibrated input velocity (or equivalently
quantized PCA bin) we take the average of all the output driving
velocities that map to it as the output driving value. This
calibration mapping is shown in Figure \ref{fig:calibration_mapping}.
Figures \ref{fig:sub1:mfcc-inverse-mapping} and
\ref{fig:sub1:inverse-mapping} show how changing the calibrated input
velocity linearly results in a linearized progression through the
timbrespace (PCA0) and loudness (MFCC0). In these graphs we show
directly the results of this calibration but it is also possible to
fit lines to them. In either case (direct calculated mapping or line
fit) the calibrated output changes sound more smooth than the original
output.

%% Another challenge is to achieve the largest possible dynamic range,
%% ideally large enough to be comparable to that of a human
%% performer. One possible approach is to have two actuators, one for
%% soft strikes and one for loud strikes. The soft striker would be
%% positioned very close to the drumhead, whereas the loud striker would
%% be positioned further away, so that it could develop some momentum
%% before striking the drumhead. Ideally we would need only one actuator
%% to cover the entire dynamic range, but that may not be possible.

%% The stiffness of the beater arm is an important parameter, it affects
%% the maximum speed of beating.  The beater arm will flex at a rate that
%% may resonate (or not) with the beating speed.  An analysis of beater
%% behaviour (loudness) versus beater arm material parameters, distance
%% from drumhead, speed and striking force remains to be done.



\begin{figure*}[t]
\centering
\subfigure[MFCC-values]
{
    \label{fig:sub1:mfcc-values}
    \includegraphics[width=60mm]{mfcc-values}
}
\hspace{1cm}
\subfigure[MFCC-inverse-mapping]
{
    \label{fig:sub1:mfcc-inverse-mapping}
    \includegraphics[width=60mm]{mfcc-inverse-mapping}
}
\\
\subfigure[PCA-values]
{
    \label{fig:sub1:values}
    \includegraphics[width=60mm]{values}
}
\hspace{1cm}
\subfigure[Calibrated PCA] 
{
	\label{fig:sub1:inverse-mapping}
    \includegraphics[width=60mm]{inverse-mapping}
}
\label{fig:fig1}
\caption{Velocity Calibration based on loudness and timbre} 
\end{figure*}








\subsection{Gesture recognition using Dynamic Time Warping}

Collaborating musicians frequently utilize high-level cues to
communicate with each other especially in improvisations. For example
a jazz ensemble might agree to switch to a different section/rhythm
when the saxophone player plays a particular melodic pattern during
soloing. This type communication through high level cues is difficult
to achieve when performing with robotic music instruments. In our
performances we have utilized a variety of less flexible communication
strategies including pre-programmed output (the simplest), direct
mapping of sensors on a performer to robotic actions, and indirect
mapping through automatic beat tracking. The final experiments
described in this paper show how high-level gesture recognition that
is robust to changes in tempo and pitch contour can be correctly
identified and used as a cue. Our system is flexible and can accept
input from a wide variety of input systems.  We show experimental
results with the radiodrum as well as melodic patterns played on a
vibraphone. There has been considerable work done in the area of using
Dynamic Time Warping for gesture recognition, including work done by
Akl and Valaee \cite{akl10} and Liu et al. \cite{liu09}.

For the first experiment, we used the most recent iteration of the
radiodrum system, a new instrument designed by Bob Boie that
dramatically outperforms the original radiodrum in terms of both data
rate and accuracy.  We instructed a professional musician to generate
8 different instances of 5 types of gestures, which were an open
stroke roll, a sweep of the stick through the air, a pinching gesture
similar to the pinch to zoom metaphor on touchscreens, a circle in the
air and a buzz roll.  We collected $(X,Y,Z)$ triplets of data from the
sensor at a sample rate of 44100Hz and then downsampled this data to
120Hz to allow us to compare gestures that were on average 1-2 seconds
in length while remaining within the memory limits of our computer
system.  We empirically determined that this rate captured most of the
information relevant to gesture recognition.

From this data, the similarity matrix of each gesture to each other
gesture is computed. Dynamic Time Warping \cite{sakoe78} is used to
compute an alignment score for each pair of gestures that correspond
to how similar they are. For each query gesture we return a ranked
list based on the alignment score and calculate the average precision 
for each gesture. As can be seen from Table ~\ref{table:precisions}
gesture identification is quite reliable in both cases. 

\begin{table} 
\begin{center}
\begin{tabular}{|lrr||lrr|}
\hline
\multicolumn{3}{|c||}{radiodrum} & \multicolumn{3}{c|}{Vibraphone} \\
\hline
Gestures   &  AP & P@1  & Gesture   &  AP  & P@1 \\
\hline
roll   &  0.866 & 1.0  &  pattern1 & 0.914  & 1.0  \\
sweep  &  0.980 & 1.0  &  pattern2 & 0.812  & 0.9  \\
pinch  &  0.837 & 1.0  &  pattern3 & 0.771  & 0.9  \\
circle &  1.000 & 1.0  &  pattern4 & 0.882  & 1.0  \\
buzz   &  0.978 & 1.0  &  pattern5 & 0.616  & 0.9  \\
\hline
MAP    &  0.931 & 1.0  &    MAP    & 0.799  & 0.94 \\
\hline
\end{tabular}
\caption{Average precision for different gestures on the radiodrum and
  vibraphone. The Mean Average Precisions (MAP) are 0.931 and 0.799.}
\label{table:precisions}
\end{center}
\end{table}


%% \begin{figure}[htb]
%% \begin{center}
%% \includegraphics[width=50mm]{pattern1_pattern1}
%% \end{center}
%% \caption{Similarity Matrix and best path calculate by DTW for pattern1 vs pattern1}
%% \label{fig:pattern1_pattern1} 
%% \end{figure} 

%% \begin{figure}[htb]
%% \begin{center}
%% \includegraphics[width=50mm]{pattern1_pattern2}
%% \end{center}
%% \caption{Similarity Matrix and best path calculate by DTW for pattern1 vs pattern1}
%% \label{fig:pattern1_pattern2} 
%% \end{figure} 



\section{Conclusions and Future Work}

We have shown how techniques from MIR can be adapted and used to solve
practical problems in music robotics. More specifically we show how
audio classification can be used for automatic mapping, principal
component analysis can be used for velocity/timbre calibration and
dynamic time warping for gesture recognition.  This system has not yet
been tried in performance, and we are currently working with musicians
to deploy this system in a live setting. In the future we plan to
extend this work utilizing more sensors including multiple microphones
on both the robot and the performers. To obtain the maximum possible
dynamic range we plan to have multiple actuators placed at different
distances on the same drum so that the ones that are far are used for
loud sounds and the ones that are near are used for soft sounds. The
proposed calibration method will be used to drive seamlessly both
actuators. We would also like to investigate how MIR techniques can be
used to ``teach'' the robot to play and recognize rhythmic and melodic
patterns.





\section{Acknowledgments}
We would like to thank Gabrielle Odowichuk and Anthony Theocharis for
help in collecting data.  We thank the National Sciences and
Engineering Research Council (NSERC) and Social Sciences and
Humanities Research Council (SSHRC) of Canada for their financial
support.


\bibliography{ismir2011drumgtzan}

\end{document}


%% ***Just to expand on my previous choice of title. Coping strategies
%% generalizes nicely to describe various approaches: ignore (if it fails
%% live with it), mask/hide (trigger samples), redundancy, etc (you
%% probably can think of some others).

%% We can then motivate self-awareness/embodiment/proprioception as an
%% unexplored coping strategy and maybe provide a proof of concept
%% experiment.

%% XXXXX Write something about the challenges of setting up robotic 
%% instruments in different venues XXXX 

%% XXX The paragraphs about the concerts need to be condensed and 
%% maybe abstracted. ISMIR submissions are anonymous which is going 
%% to make it hard to mention the concerts specifically XXXXX 


%% Recently, the MISTIC group performed two instantiations of the fifth
%% MISTIC concert, a concert that featured Dr. Andrew Schloss' new piece
%% ``sonicpair'', a duet between Dr. Schloss on radiodrum and Joanna
%% Hood, a classically trained violinist, accompanied by robotic
%% percussion and real-time audio DSP using a microphone amplification
%% system.  For this work, Schloss created a piece that invokes
%% variations for the Notomoton, a newly developed novel robotic drum
%% instrument and several other modular robotic frame drums. The
%% radiodrum is used to play the robotic percussion, and also manipulate
%% the audio from the microphone- which could be processing the viola or
%% Noto. On the viola, a variety of extended techniques were used,
%% including rubbing the bow gently on the strings, as well as, different
%% plucking and scratching techniques. Also performing at this concert
%% was Trimpin, the father of musical robots, along with Ajay Kapur,
%% Co-director of Calarts robot program, and composer/researchers Arne
%% Eigenfeld, Darren Miller and Steven Ness. The concert showcased a
%% broad variety of musical contexts in which robotic music can and will
%% inevitably be approached. ***Andy, would you like to offer a brief
%% synopsis of each composer's piece to illustrate diversity in this
%% context?


%% Another example of a musical context (for our current/proposed work)
%% stemming from a broader North American perspective is Pat Metheny‚Äôs
%% recent Orchestrion Project. Performing on a digitally extended solo
%% guitar, Metheny activates an entire stage full of acoustic instruments
%% that have been equipped with actuators. Initiated in 2007 and
%% premiered in 2010 the massive project then went on nearly a year long
%% world tour. Shawn Trail, a new interdisciplinary Ph.D. student with
%% MISTIC, has been on tour and in pre-production of the Orchestrion
%% Project with Metheny, working as control interface/robotics technician
%% trying to keep this massive technical set-up functioning properly
%% night after night of full-capacity use. This tour represented one of
%% the first times that a well known popular musician (17 time Grammy
%% winner, 32 nominations) featured music robots prominently in their
%% show. Trail recently also assisted Dr. Schloss as stage technician and
%% sound engineer at the Sonic Boom festival. Schloss was performing a
%% solo piece entitled``Variations for Notomoton'' composed by David
%% Jaffe for the Radio Drum and robotic percussion. We will use this
%% instance to contrast with the Orchestrion as both works similarly
%% feature a traditionally trained acoustic instrumentalist interfacing a
%% robotic ensemble using adaptations of the proprietary technics of
%% their respective instruments (Schloss- percussion, Metheny- Guitar),
%% yet in dramatically different contexts (andy- compositions designed
%% for true improv/pat- full thru-composed orchestrations with some
%% improvisation). Because both situations had very controlled and
%% systemized live technical support and preproduction they serve quite
%% well as parallels representing distinct, musically idiosyncratic,
%% expert musicians with finite realizations of very specific and
%% technically demanding (both musically and equipment) musical
%% performance scenarios- both using the traditional stage context- yet
%% dealing with spatial acoustic sound reinforcement differently as each
%% scenario required.
%% In this paper we will first discuss previous work related to this,
%% then we provide a broad overview of the system description of both the
%% Orchestrion and MISTIC concerts. We will then compare these two
%% similar but quite contrasting examples of the uses of robotics in
%% music. The MISTIC group focuses primarily in the field of Music
%% Information Retrieval \cite{orio06} and applies these technologies to
%% the production of music from a research and development
%% platform. Metheny's project, commercial by nature, was more focused on
%% a final, fully realized body of performance art. Both scenarios are
%% similar in that they rely heavily on novel electro-acoustic control
%% interface methods which we will discuss in detail. The Orchestrion has
%% provided us an additional intensive opportunity to observe what does
%% and doesn't work in a rigorous performance context. We also have
%% extensive experience with our own ensemble of instruments in live
%% performance (funded by SSHRC, NSERC, CCA) and have learned what works
%% and what doesn‚Äôt. 
%% At MISTIC we have also done work in the field of Computational
%% Ethnomusicology (****references- ***site peter biro, orchive), which
%% is generally described as the study of music with computational
%% modeling and simulation. By extending the application of Music
%% Information Retrieval (MIR) algorithms and systems to the archiving,
%% analysis, retrieval of and interactions with orally transmitted music
%% cultures we can design and build tools that help us organize,
%% understand and search large collections of music. Computational
%% Ethnomusicology encompasses a wide variety of ideas, algorithms,
%% tools, and systems that have been proposed to handle the increasingly
%% large and varied amounts of musical data available digitally. This
%% provides us with categorical access to various elements and points
%% throughout entire collections of field recordings, for instance. This
%% information also allows us to investigate the intrinsic qualities of
%% music related to a specific social function in order to generate novel
%% ways of interfacing robotic music instruments. A practical example of
%% this is found with the Gyil- the traditional xylophone of the Lobi
%% Nation from northwestern Ghana. A direct predecessor to the modern
%% conventional marimba, yet distinct, though, from other seemingly
%% similar West African xylophones (balaphone, marimba, etc) along with
%% other indigenous xylophone traditions because its technique, in which
%% a single musician sings, plays bass, comps chords, and improvises
%% simultaneously. The Gyil‚Äôs format of playing is related to the concept
%% of improvising used in jazz, and its repertoire is the most complex
%% and virtuosic of solo xylophone traditions. This important aspect of
%% the Gyil‚Äôs performance practice serves as a foundation for how we
%% might develop repertoire using digitally extended capabilities. The
%% Gyil's repertoire is quite foreign and abstract making it hard to
%% approach for many musicologists, which is one reason it remains under
%% researched. Through our system of MIR we can analyze the musical
%% content much more objectively and find patterns and distinct
%% structures and styles of playing much faster and more precisely-
%% making the information more accessible for understanding and using to
%% influence the development of new music.

%% Pat commissioned his instruments from the innovators of the field that
%% he specifically chose based on a preconceived conceptual vision of
%% what he wanted. Once they were fabricated and installed in his studio
%% he then pursued mastery over the ensemble. His work was more aligned
%% with that of a soloist with an orchestra performing a through
%% composition. MISTIC is more concerned with continually developing an
%% ever evolving modular, custom, DIY, open source, yet systematic
%% framework. In this sense our motivation is to develop the technologies
%% in a self-sufficient capacity. Musically speaking, our research has
%% followed closely distinct forms of indigenous music looking at forms
%% of improvisation, theoretical content, social functions of specific
%% musics, and instrumental technics proprietary to those culturally
%% specific instruments. It is not arbitrary or coincidental that we have
%% looked at the correlation to drumming music and social-cultural forms
%% of music as an impetus for our methodology. In our previous research
%% on new musical resources (funded by SSHRC, NSERC and CCA), we
%% concentrated on innovations in virtual controllers like the radiodrum
%% [Boie1989], or in enhanced electroacoustic percussion instruments like
%% the E-Drumset [Tindale2010]. The sound generation was either oriented
%% towards electronics (physical models of acoustic instruments played
%% through loudspeakers) or acoustics (various drums struck by a robotic
%% mechanism). To create new musical instruments, we need an
%% interdisciplinary group of designers, electrical engineers, computer
%% scientists, and craftsman capable of designing and building new
%% electroacoustic instruments, but also performers who understand what
%% works musically. Similarly, a Lobi Gyil master from Ghana would design
%% and build his xylophone, and then pursue mastery over it as a
%% performer [Chernoff1979]. As it takes many years to realize and master
%% an extended instrument, our goal is to develop the music technology
%% that we utilize self-sufficiently in order to gain and describe a
%% comprehensive understanding of what comprises the tools we use to
%% realize our art. Formalized theory is essential when implementing
%% complex technological performance systems. The utilization of
%% proprietary instrumental techniques for real-time control of the
%% extended digital functions effectively bridges musical human gestures
%% and complex computer interaction methods in a way that is intuitive to
%% both the performer and audience. This is why percussion remains a
%% pervasive model for control interfaces at MISTIC- because of the
%% direct relationship to sound and gesture. Pursuing this work, however,
%% requires the development of a system for analysis and the
%% standardization of design paradigms for interactivity in musical
%% instruments.


%% Although, as we demonstrate, that it is possible to run this system on
%% a single microphone based system using Audio analysis and
%% MachineLearning techniques, in some particularly loud surroundings for
%% example, a music concert, it may become difficult to correctly map
%% solenoid activations to sounds.  For this reason we also are working
%% on a system of optical pickups \cite{overholt05} that would directly
%% monitor the sonic output of that drum, theoretically providing an
%% ultra discreet signal with no sympathetic interference.  With this
%% system it would be possible to accurately map and calibrate a beater's
%% relationship to its respective drum.

%% Less important for the mapping and calibrating, but a very important
%% concept musically to the MISTIC group is the idea of Localized Sound
%% Reinforcement (LSR) \cite{eargle04}.  Because many drums, strings and
%% idiophones make sounds too quiet to be heard on stage, it is important
%% to project their sound through speakers.  Typically, the amplified
%% sounds from all the drums is projected through two speakers in a
%% stereo configuration, or more in a multichannel audio system. However,
%% the spatial non-proximity of the sound and the object producing the
%% sound invokes a mild form of cognitive dissonance \cite{festinger57}
%% where the mind tries to reconcile two different pieces of information.
%% In LSR, there is a speaker attached to each drum, and the amplified
%% sound appears to come directly from instrument itself. By placing the
%% amplification at the visual and primary auditory source of the drum
%% amplified sound is still projected from its source rather than
%% arbitrarily from loud speakers arranged accordingly around a venue.
%% Andy- radiodrum, Macbook- Max/MSP 4 audio outputs, Noto., 5
%% Solonoids/modular beaters, 5 Frame drums arranged spatially in a
%% cluster. The 2 lowest drums mic'd with Senn. to reinforce low
%% transients that are lost without amplication, and a 414 on the Noto
%% for processing, Elka footpedal to control Max Mapping Logic.

%% The robots were controlled using a Max/MSP \cite{puckette02} patch
%% that took input from the radiodrum and an octave of foot pedals and
%% sent OSC \cite{osc} messages to the drum controllers that were
%% controlled by the new NotomotoN system developed by Dr. Ajay Kapur at
%% CalArts \cite{ajay}. These drums had a series of solenoids attached to
%% them, with four solenoids of differing design attached to each head.
%% For the piece that Andy composed, he needed a wider sonic palette and
%% removed several of the solenoids and then attached them onto
%% microphone stands which he then placed to beat frame
%% drums. Calibrating this setup was time consuming and
%% difficult. Minimal thought was given to the sound reinforcement and so
%% the sound design of the piece suffered. However, simple but very
%% effective approach was used to deal with the loss of lower transients
%% from the two largest drums. By placing *senn. a proper mic for the
%% occasion, and oriented the two drums staggered- left/rightish
%% respectively and panning the signal on the board in rough
%% approximation, one could emulate the reinforced sound originating from
%% the general area of the source drum. Since the other drums were higher
%% frequencies they cut through the room noise and retained their spatial
%% characteristic, thus no further reinforcement was needed beyond the
%% Max audio which balanced nicely in stereo configuration.

%% - Use of NotomotoN (Kapur reference)
%% 	- Single drum with a number of beaters on the head to allow for
%% 		 rolls, flams and other such techniques
%% 	- Multiple kinds of solenoids for doing different kinds of
%% 		things.  Some are fast, some can hit hard- ajay actually said that wasn't the reason, it was more just because he had those already and used them to save on cost.
%% 	- Tuning the performance to the type of solenoid
%% 	- Drum has a USB plug and appears as a device in Max
%% 	- We found this sonically and artistically limiting so we removed
%% 		the solenoids and put them on mic stands and hit frame drums
%% - How did Pat do it, how did we do it	
%% 	- big budget / smaller budget
%% 	- lots of time for development / not much time for development 
%% 	- thousands of people at concert / 100-200 people for concert	
%% 	- large concert halls / small art gallery and academic recital hall
%% 	- robot drums were amplified / robot drums were minimally amplified
%% 	- robot drums all on stage / robot drums in the audience as well as on stage
%% 	- theatrical aspects / primarily sonic aspects

%% nightly setup and teardown- spell out the schedule- go get tour schedule- where? rigors- traveling roadshow

%% Mapping scenario:

%% -instruments get instructions from a variety of sources

%% Hexaphonic Guitar Pickup
%% -one to one mapping (notes he plays get played by bot- can control multiple instrument simultaneously)
%% -polyphonic

%% created very specific triggering environments for very specific musical pieces

%% pieces with improvisations-
%% ornet
%% live
%% first tune- one loop- using moog

%% M4L- (or Max) remapping notes from guitar to groups of percussion- selectable with foot controls

%% foot pedals have subsequent mapping logic running in Max

%% INSTRUMENTs:

%% Angeli! Picasso!

%% Rag West: (pneumatic)

%% Percussion Box- fathead
%% acoustic guitar- direct
%% ebass- direct

%% Interface

%% Bots- actuator/sound situation:

%% disklavier x 2 (1 baby grand- LDC, SDC, 1 upright)

%% lemur: AKG's- older C451's

%% piccolo snare drum- 2 solonoids, drumsticks. one mic-? small dia- dynamic

%% finger cymbals- single mic (dynamic or small diaphragm condenser) embedded natural metronome
%% hi hat- two solonoids mounted opposite to play double notes on closed hi-hat. 1 mic

%% floor tom- 2 beaters- mallet/stick- 1 akg

%% conga 1- 2 beaters- mallet/stick
%% conga 2- 2 beaters- mallet/stick
%% -both with small dynamic mics(senn. akg? sure?)

%% Bottles x 2= Ribbons? fatheads x2 each

%% cymbal x 3

%% 30" bass drum- 2 solonoids. senn.

%% rack tom- mallet/stick, plate- beater, snare brush stick with tip- senn.

%% cymbal- w sticks, wood block, plate- one mic

%% 18' bd- AT akg bd mic? one beater

%% marimba
%% glockenspeil- one overhead senn.
%% vibraphone
%% Guitarbot x 2

%% percussion

%% metal plate- 1solonoid, shaker box (rotary motor), shaker cylinder (rotary motor)- senn. x1 general
%% shaker group (rotary motor), bells (rotary motor), cymbal w 2 sticks/solonoids- senn x1

%% accordian- lavalier mic

%% - 100 inputs on board- audio?
%% - 2 hour concert- pieces? times?- 8-10 pieces

%% ***- different control interfaces (parameters, triggers)
%% pedals- how? arduino mega- copy group 499- talk about max patch, arduino code- example of why t his worked well- changed behavior of pedal on spot- led's, max logic... original versus final used... better if led's controlled by max

%% - 3000-4000 people at each concret- amount of shows

%% - just pat and the orchestrion

%% - solenoids, motors and pneumatics

%% - drums, zylophones, wind, string, idiophones

%% - Eric Singer from LEMUR built robots

%% - Rag West

%% - Control scenarios:

%% 	pitch to MIDI hexaphonic guitar pick-up embedded into his hollow body electric. 
%% 	3 custom footpedals (photo- selecting bots), developed by Trail, using a Max patch for 
%% 	switching logic.- compositionally embedded logic- 
%% 	Moog Taurus Pedal (tempo, looping, transport controls)- overuse- cumbersome
	
%% Roland Synth

%% House sound- wedges, monitors, bose, subs
%% yamaha board- rack with computer- monitors through computer to house back to stage

%% - How to mic the setup
%% - Differences and similarities
%% - DIY aspects- list for both
%% - commercial- list for both- fireface vs. pat- problems
%% %% - control scenarios

%% Problems:

%% Most of the robots in both the Metheny and MISTIC concerts use
%% solenoids to produce sound through striking an object.  Although
%% diverse and useful\cite{kapur07}, solenoids have a variety of
%% advantages but also have some drawbacks as well.  Many larger or older
%% solenoids produce a click when activated. Some performers use this to
%% great artistic effect \cite{kapur06}, however it is still a fact that
%% must be attended to since it isn't always desired or appropriate.
	
%% 	- why we need to look other places for robotic acoustic sound generation
%% 	- linear conversion problem. 
%% 	- velocity range
	
%% Other probs with solonoids

%% GuitarBot (photo):

%% The GuitarBot is a novel robotic string instrument. There are
%% currently only 3 in existance. Developed by Eric Singer, the initial
%% prototype incorporates electro-acoustic pickups. The other two were
%% commissioned by Pat Metheny for the Orchestrion Tour, and have optical
%% pickups. GB is comprised of four vertical strings tuned like a
%% cello. The pitch of each string is established by a separate mechanism
%% that runs along its length ‚Äòpinching‚Äô the specified pitch. The
%% mechanism is driven by a belt and is controlled by MIDI Pitch
%% CC‚Äôs. The string is excited by a rotary mechanism that spins at the
%% bottom of each string using the four picks attached to it. The plucks
%% are controlled by rhythmic sequencing in MIDI Rhythm CC‚Äôs. The
%% instrument is intonated using Max by defining the spot on the string
%% that represents the lowest note to be sounded. Each string is two
%% octaves. The highest pitch of that particular sting is defined by then
%% manually moving the ‚Äòpitch mechanism‚Äô until one finds the tonic of
%% that string. The rest of the pitches between are established by the
%% firmware on the instrument then running an interpolation based on the
%% chromatic scale between the highest and lowest note of each string
%% respectively. This is an extremely problematic process and is never
%% accurate due to organic variations in placement inevitably present
%% each time the mechanism selects a new pitch, even the slightest
%% variation will result in the desired pitch being out of tune with
%% other instruments (such as xylophones) that have fixed tunings.
%% HOW PAT SOLVED and IMPROVEMENTS TO BE MADE WITH MIR:

%% known intermittent short somewhere in the optical pickup of the low a
%% string of guitar bot 2, and the fact that the tuning was never
%% precise, it was presumed that the guitarbots could not be left to
%% their own fate on stage and therefore could not be trusted to
%% faithfully reproduce the written melodies in tune. to complicate
%% things Further other complications regarding consistency were created
%% by the marimba mallet shafts bending therefor recontextualizing the
%% preprogrammed velocity automations in the sequencing. Shifting of
%% beaters on other drums, the pneumatic instruments sometimes getting
%% ‚Äústuck‚Äù without indication of malfunctions, and problems with control
%% interfacing greatly compounded the pressure put on the ‚Äòperformer of
%% the bots‚Äô.

%% samples, sequencing, studio stems, to support bots irregularities
%% without compromising fidelity.

%% MIR solutions: smart real-time self-tuning of guitar bot, beater
%% self-realignment/calibration,

%% - IT SHOULD INTERPOLATE THE SOUNDS ITSELF AND TUNE ITSELF LISTEN TO
%% - EACH NOTE AS IT IS PLAYING AND DYNAMICALLY ADAPT ITSELF

%% - DISTANCE OF SOLENOID IN DRUMS PROBLEMS THAT THIS CAUSED IN
%% 	- PERFORMANCE, BOTH FOR PAT'S SETUP AND FOR OUR SETUP OCCURRED IN
%% 	- PRACTICE IN BOTH CONCERTS/CONTEXTS

%% - MARIMBA BARS WOULD BEND
%% 	- PRESET VELOCITY
%% 	- WOULDN'T EXECUTE THE NOTES
%% 	- PREPROGRAMMED VELOCITY CURVE NO LONGER RELEVANT (?)
%% 	- BAR IN DIFFERENT RELATION TO MALLET (?)
	
%% because these were prototypes we had no system to remember where the
%% original position was so the mallet instruments were wild most of the
%% time with unusable audio- samples were used to compensate for this. no
%% previous long term- nightly gigging- only big one-off shows, etc. with
%% much prep/breakdown time needed. had/need to be systimized.
	
%% HOW TO MOVE FORWARD

%% 	- motors for actuators to move themselves to correct positions
%% 	- OPTICAL PICKUPS- both audio and control data- discreet signal-
%% 	- new multichannel DSP potential
	
%% 	Other types of robots: - kalimba model - string model - peumatics
%%     - motors- types- where is that quote about pancake motors?
	
%% \section{Experimental}

%% For our experiments, we built a system that did an automatic mapping
%% based on timbre classification. This was straightforward to do using
%% existing Marsyas feature extraction and classification modules. These
%% modules allow complex features to be calculated based on input audio,
%% some of these include Fast Fourier Transform (FFT), Mel-Frequency
%% Cepstral Coefficients (MFCC), Correlogram based approaches
%% \cite{slaney93}

%% Basically there are (number) different drums each with a corresponding
%% robotic beater. The goal is for the system to recognize which beater
%% is beating which drum (including a silent drum for broken beaters)
%% without having to explicitly specify the mapping using a microphone.

%% 2) Automatic velocity calibration: Given a desired velocity acoustic
%% response that can be obtained either by the actual robot or by a human
%% performer re-adjust the current velocity response to match it. This
%% could be done with a simple regression from energy or loudness or
%% could incorporate multiple features to include timbral effects.

%% CRITICAL: For the experimental setup we will need a minimum of two
%% beaters and two drums for the first experiment. For the second ideally
%% we would like something that has a good dynamic range....
% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\begin{document}

\title{Use of symbolic representations of pitch contours for Orca call indexing using tools from Bioinformatics}
\subtitle{}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven Ness\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Canada}\\
       \email{sness@uvic.ca}
% 2nd. author
\alignauthor
Niko Rebenich\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Canada}\\
       \email{niko@uvic.ca}
\and  % use '\and' if you need 'another row' of author names
% 3rd. author
\alignauthor
Patrick Gorman\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Canada}\\
       \email{pgorman@uvic.ca}
% 4th. author
\alignauthor
Chris W. Pitkin@uvic.ca\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Canada}\\
       \email{cwpitkin@uvic.ca}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\date{2 April 2013}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
The Orchive is a large collection of over 20,000 hours of audio
recordings from the OrcaLab research facility located off the
northern tip of Vancouver Island.  It contains recorded orca
vocalizations from the 1980 to the present time and is one of the
largest resources of bioacoustic data in the world.  We have
developed a web-based interface that allows researchers to listen to
these recordings, view waveform and spectral representations of the
audio, label clips with annotations, and view the results of machine
learning classifiers based on automatic audio features extraction.

In this paper we investigate the use of symbolic representations of
pitch contours to allow tools from bioinformatics to be used to search
large databases of pitch information.  We examine the performance of
different types of symbolic representations on this dataset.
\end{abstract}

%% % A category with the (minimum) three required fields
%% \category{H.4}{}{}
%% %A category including the fourth, optional field follows...
%% \category{D.2.8}{}{}[]

\terms{Theory}

\keywords{Symbolic Approximation} % NOT required for Proceedings

\section{Introduction}
The Orchive is a large archive containing over 20,000 hours of
recordings from the Orcalab research station. These recordings were
made using a network of hydrophones and originally stored on analog
cassette tapes. OrcaLab is a research station on Hanson Island which
is located at the north part of Vancouver Island on the west coast of
Canada. It has been in continuous operation since 1980.  It was
designed as a land based station in order to reduce the impact on the
orcas under study, as the noise and disturbance from boats affects the
orcas in observable but currently unquantified ways.  In collaboration
with OrcaLab, we have digitized the tapes and have made these
recordings available to the scientific community through the Orchive
website (http://orchive.cs.uvic.ca).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{dm_orchive}
\caption{Annotated audio from from the Orchive}
\label{fig:dm_orchive}
\end{figure}

Over the past 5 years, a number of orca researchers using our website
have added over 18,000 clip annotations to our database.  A small
section of annotated audio from the Orchive is shown in Figure
\ref{fig:dm_orchive}.  These clip annotations are of two main types:
The first is clips that differentiate background noise from orca calls
and from the voice notes of the researchers that collected the data.
The second type of clip annotations classify orca vocalizations into
different calls.  Orcas make three types of vocalizations,
echolocation clicks, whistles and pulsed calls.  The pulsed calls are
highly conserved stereotyped vocalizations which have been classified
into a catalog of over 52 different calls by John Ford \cite{ford87}.
Of the 18,000 annotations currently in the Orchive, 3000 are of these
individually classified calls.  In addition, we have a curated call
catalog containing 384 different recordings of different calls
vocalized by a variety of different pods and matrilines.

Many parts of the recordings contain boat noise which makes
identifying orca calls both difficult and tiring. In addition, the
size of the Orchive makes full human annotation practically
impossible. Therefore we have explored machine learning approaches to
the task. One data mining task is to segment and label the recordings
with the labels background, orca, voice. Another is to subsequently
classify the orca calls into the classes specified in the call
catalog.

\section{Introduction}

Audio feature extraction is the first step in classifying audio using
machine learning algorithms. A commonly use audio feature that is
often used is an estimate of the fundamental frequency (F0) or pitch,
one well known algorithm for this is the Yin algorithm
\cite{cheveigne02}.  This algorithm is primarily an autocorrelation
based approach, which means that it takes the audio signal and
convolves it with itself.  The peaks in this convolution then
correspond to harmonics in the signal, and with noise free data with
harmonics that strictly decrease, the lowest peak is the fundamental
frequency.  With audio that does not fit this strict definition, there
are many cases where the lowest peak is not the fundamental frequency,
one example is if odd harmonics are systematically lower than even
harmonics, and another is if there is substantial noise in the data.
The Yin algorithm makes several modifications to simple
autocorrelation to overcome these issues.  Another modern pitch
detector is SWIPEP \cite{camachophd}.  

Mel-Frequency Cepstral Coefficients \cite{Logan00melfrequency} (MFCC)
have been widely used for this purpose.  MFCCs have also been used in
bioacoustics, and have been used to classify insect sounds
\cite{leqing11}, birds \cite{changhsing07} and orca calls
\cite{ness08}.  We have investigated the use of MFCC values for
classifying calls from the orca call catalog, and results using these
with a variety of machine learning techniques are described below.  In
future work we would like to try to use MFCC or other forms of
spectral data as input to our symbolic approximation algorithm.

Another type of audio feature extraction that is promising is features
based on models of the auditory cortex \cite{lyon82}.  These
algorithms model the properties of the cochlea and peripheral nervous
system\cite{lyon10}, and have at their core an adaptive filterbank
coupled to a triggered pulse model \cite{waltersphd}.  However, one
issue with these systems is that instead of a 1-dimensional (waveform)
or 2-dimensional (spectral), they lead to a 3-dimensional dataset in
which 2-D audio images change over time, which leads to a very large
amount of data.  Approaches using vector quantization could be used as
an input to our symbolic approximation algorithm in future work.

\section{Time Series Quantization}

The transcription of time series data into a character sequence that
can be consumed by bioinformatics sequence alignment tools is a
nontrivial task.  An attempt towards this goal is Symbolic Aggregate
Approximation (SAX) which was proposed by Lin et al. in
\cite{Lin2003}. The underlying idea of SAX is to parse a time series
using a sliding window and to generate a character sequence that
approximates the signal's normalized slope using a technique called
Piecewise Aggregate Approximation (PAA).  SAX is most useful in cases
where the data is not on an absolute scale.  In order to investigate
the utility of SAX on this dataset, we plotted the fundamental
frequency curves of a number of examples of calls to each other, two
of these plots are shown in Figure \ref{fig:pitch-N04} and Figure
\ref{fig:pitch-N05}.  From these we can see that the absolute pitch of
these calls is well conserved, a result that has been previously
observed \cite{ford87}.  It is of interest to note that the N05 call
voiced by the A35 matriline is of a lower pitch than the others, this
matriline is in the A4 pod, while the calls by the A12 and A36
matrilines are of more similar pitch to one another.  This information
could be used to help classify which pod is vocalizing a particular
call.

\begin{figure}[h]
\centering
\includegraphics[width=90mm]{pitch-N04}
\caption{F0 contour for 21 examples of the N04 call.}
\label{fig:pitch-N04}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=90mm]{pitch-N05}
\caption{F0 Contour for 10 examples of N05 call.}
\label{fig:pitch-N05}
\end{figure}

This approach is certainly valid if slope changes within a given
window characterize the signal well and the signal is fairly free of
noise. We therefore have examined SAX for a possible avenue to
transcribe Orca vocalizations. We determined, however, that
considering windowed slope fragments of Orca vocalizations does
produce sequences that are of lower quality than those produced by our
transcription technique which we refer to as FTSQ (Fundamental
Frequency Time Series Quantization).  The FTSQ procedure is outlined
in detail below. We do not provide classification results for SAX
sequences because the majority of SAX sequences were too long in order
to be processed by our dynamic programming based alignment algorithm
and hence a direct comparison can't be impartial. Further, we observed
that the fundamental frequency of Orca vocalizations carries a lot of
information, that is, it seems to be stable, and within a certain
range that is unique to a large portion of the vocalizations in our call
catalog. Since SAX is normalizing the signal slope within a sliding
window it cannot take advantage of this important feature.

\subsection{Signal Pre-processing and FTSQ}

Fundamental Frequency Time Series Quantization is a time series
quantization approach that transcribes an audio time series based on
an estimate of it's fundamental frequency ($F_0$).

In general it is not feasible to analyze Orca vocalizations as a raw
time series, since these signals are a complex mixture of sinusoids and
noise (see Figure \ref{fig:audio_raw}).  Figure \ref{fig:freqspec}
shows a spectrogram representation of the N01 and N09 Orca
vocalizations.  When comparing Figures \ref{fig:audio_raw} and
\ref{fig:freqspec} it is easy to see that the frequency components of
the audio signal reveals much more about the structure of the signal
than a raw audio time series by itself.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\columnwidth]{audio_raw}
\caption{Raw audio signal of Orca vocalization N01.}
\label{fig:audio_raw}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.60\columnwidth]{freq_N01N09.png}
\caption{Frequency spectrum of Orca vocalizations N01 and N09.}
\label{fig:freqspec}
\end{figure}

Quantizing the fundamental frequency of an audio signal is am approach that makes 
use of the underlying structure in the frequency bands of the audio signal.
Loosely speaking one may think of the fundamental frequency as the pitch of a sound signal.
For our FTSQ technique we use Yin \cite{Cheveigne2002} a robust fundamental frequency 
estimation algorithm developed by Cheveigne et al.
Processing the audio waveform from Figure~\ref{fig:audio_raw} with Yin yields the 
fundamental frequency (in octaves relative to 440 Hz) shown in the top plot of Figure \ref{fig:yin}.
The two plots below the fundamental frequency show the aperiodicity and the
period-smoothed instantaneous power of the signal; they give estimate in the 
confidence on the approximation of $F_0$ and are used to identify meaningful
regions of interest within the $F_0$ signal.
\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{yin.pdf}
\caption{Frequency spectrum of Orca vocalizations N01 and N09}
\label{fig:yin}
\end{figure}


A further issue that is encountered commonly in the process of
determining a pitch contour for an audio recording is the optimization
of all the parameters of the pitch detection algorithm to perform well
on the particular dataset that is being used.  There are many such
parameters for each different algorithm, the important ones in the Yin
algorithm include the window size and hop size of the FFT, the high
and low frequency cutoffs, the high and low frequencies around which
the pitches will be wrapped using modulo arithmetic, the tolerance of
the Yin algorithm which determines which peaks in the autocorrelation
will be used for the pitch determination, the window size for the
median filter, and the number of histogram bins in which to divide the
frequency range into.  In our previous work, this was done by hand by
plotting points using MATLAB or Gnuplot, which can be a long and
labour intensive process.

For this project, we added custom visualization tools to our OpenMIR
platform to view the original audio as a spectrogram, to allow the
user to listen to this audio, to show a pitch contour with dynamic
controls over these various parameters, and an energy display that
shows the RMS energy of the audio signal.  This interface is shown in
Figure \ref{fig:openmir}.  This software allowed us to quickly
iterate over a large number of parameters, and to determine the
optimal parameters for the Yin algorithm.  The most important of these
was the median filter size, and a median filter of size 11 is shown in
Figure \ref{fig:openmir}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{openmir}
\caption{Spectrogram, pitch and RMS view of a small portion of the Orchive
catalog viewed with the OpenMIR interface. }
\label{fig:openmir}
\end{figure}

\subsubsection{Octave Errors}
The output of Yin does not always give a proper estimate of $F_0$.
Indeed, the $F_0$ signal in Figure \ref{fig:yin} displays several
estimation errors that we herein refer to as \emph{octave errors}.
Octave errors manifest themselves as sudden jumps in a multiple of the
core $F_0$ frequency. Figure \ref{fig:yin} displays multiple octave
errors the first one at about 0.1 seconds. When quantizing the $F_0$
signal into a letter representation over a finite alphabet. Octave
errors result in mis-mapped letters in the output sequence. To a
certain degree we are able to handle octave errors by crafting a
custom substitution matrix in our sequence alignment
algorithm. However, doing so ultimately lowers the confidence in our
alignment score and might confuse legitimate frequency jumps with
octave errors and as a result would not penalize mismatches
appropriately. Therefore, we have developed a technique that can
automatically correct for the majority of octave errors and as a net
effect produces a more accurate letter representation.

We first run the $F_0$ signal obtained from Yin through a median filter, 
which smoothes the signal and eliminates small transients that occur due
to noise in the original signal. We then threshold the aperiodicity and the
period-smoothed instantaneous power to obtain $F_0$ signal regions at which
the $F_0$ signal is estimated with high confidence. After down-sampling the
signal we obtain the representation shown in Figure
\ref{fig:clean_yin}.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{yin_clean.pdf}
\caption{Thresholded and down-sampled $F_0$ signal of a N01 call.}
\label{fig:clean_yin}
\end{figure}

The green line represents the median of the original $F_0$ signal
drawn in blue. The octave errors are clearly identifiable as such in
Figure \ref{fig:octave_yin} which shows versions of the $F_0$ signal
one octave above (purple) and on octave below (red) the original
(blue).  $F_0$ signal. To correct for octave errors we simply piece
the portions of the three versions of the $F_0$ signal together
according to which ever signal is closest to a slightly upwards biased
median of the original signal (blue). This yields the black $F_0$
curve wich shows the now automatically corrected version of the $F_0$
signal.
\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{octave_fix}
\caption{Octave errors within a $F_0$ signal of a N01 call.}
\label{fig:octave_yin}
\end{figure}

\subsection{Log-Normal Signal Quantization}
When transcribing the $F_0$ signal into a letter representation suitable 
for sequence alignment tools it is natural to ask how these letters should be
assigned over the range of possible $F_0$ frequency values. A naive approach
would be to map $F_0$ frequencies to letters in a linear fashion. However, while
this approach does work well in practice, we investigated if a non-linear mapping
might potentially be more appropriate. For this purpose we plotted
the distribution of $F_0$ frequencies over time for the whole catalog of
Orca vocalizations in Figure \ref{fig:freq_dist}. The dynamic range of
$F_0$ incorporates frequencies from about 80 to 2400Hz, with three high density bands
at around 250, 700 and 1200Hz. Ideally one would therefore quantize the $F_0$ signal
using a three-modal distribution. However, for simplicity we chose a log-normal 
distribution (see Figure \ref{fig:log_norm}), which provides us with a fine resolution
for the more common $F_0$ frequencies and quantizes high $F_0$ frequencies at a coarser level.
\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{freq_dist}
\caption{Distribution of $F_0$ frequencies of call catalog.}
\label{fig:freq_dist}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\columnwidth]{log_norm}
\caption{Log-normal distribution for quantization}
\label{fig:log_norm}
\end{figure}

The result of log-normal quantization of the $F_0$ trace in Figure
\ref{fig:octave_yin} is provided in Figure
\ref{fig:letter_curve}. Finally, the resulting letter sequence is
given by \texttt{LLHHJJKKKKKKKKKKKKJJJJJJJII--IIIIIHHHHHHHGGGGGFFFFFF}
which can readily be consumed by a dynamic programming sequence
alignment algorithm.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{letter_curve}
\caption{$F_0$ signal of N01 call quantized to a finite alphabet}
\label{fig:letter_curve}
\end{figure}

\begin{table}
\centering
\begin{tabular}{|c|c|} 
\hline
Data Set                         &  Global Accuracy  \\
\hline
Original                         &              64\%  \\
Octave Removal - Linear          &              81\%  \\
Octave Removal - NonLinear 900   &              80\%  \\
Octave Removal - NonLinear 1200  &              81\%  \\
J48                              &              58\%  \\
Naive Bayes                      &              65\%  \\
SVM                              &              75\%  \\
\hline
\end{tabular}
\caption{Global accuracy for different methods of converting a Yin
  pitch contour into an alphabet using FTSQ}
\label{table:performance}
\end{table}


\begin{table}
\centering
\begin{tabular}{|c|c|} 
\hline
 Call  &  Accuracy  \\
\hline
 N47   &     0.400  \\
 N01   &     0.969  \\
 N12   &     0.583  \\
 N05   &     0.785  \\
 N04   &     1.000  \\
 N03   &      0.75  \\
 N09   &     0.772  \\
\hline
\end{tabular}
\caption{Global accuracy for different call types using Octave Removal
  NonLinear 1200 parameters with the FTSQ algorithm.}
\label{table:performance}
\end{table}

\section{Alphabetic Sequence Representation}

In order for sequence comparison algorithms to work well, it is
necessary that the combination of the letters and the scoring matrix
are compatible and output meaningful results.  In the two sections
below, we show the result of quantizing the signal using the nonlinear
histogram approach for two of the more common calls vocalized by
A-clan Northern Resident whales.  The first is the very common N4
call, which consists of a quick up swing in pitch, followed by a
downswing and then a constant tone.  In it we can clearly see long
regions of the repeated letters, P,O,N,M,L,K.  Even from a quick
visual inspection these repeated letters show that our method of
converting frequencies into a discrete alphabet is promising.

{\tiny
\begin{verbatim}
N04 A04 HIKNOOOOOPPPPOOOOOOOOOOOONNNNNNNNNNNNMMMMMMMMMMMMLLLLLLLLLLLLLLLLLLKKKKKKKKKKKKKKK
N04 A04 IJNOPPPPPPPPPPPOOOOOOOOONNNNNNNNMMMMMMMMMMMMMMLLLLLLLLLLLLLLLLLLLLLLKKKKKKKKKKKKKK
N04 A05 NLKLMNOOOOOOOOOOOOOOOOOOOOOOOOOONNNNNNNNNNNNNNNNNNNNMHGQ
N04 A08 RILMNNOOOOOOOOIIOOOOOOOOOOONNNNNNNNNNNNNNNNNNNNNMMMMMMMMMMMMMMMMMMMMMLLLLLLLMLLLLL
N04 A08 SIIJJKNPQQRPIIOOOOOOOONNNNNNNNNNNNNNMMMMMMMMMMMMMMMMMMMMMMMMMMDCCCMMM
\end{verbatim}
}

Even more promising were the results for another call, N5, which
consists of a constant tone.  These calls are from the A36 matriline
of orcas, which now consists of only two whales, the brothers A37 and
A46, although the grandmother whale, A12 often now associates with
this matriline after A34, her daughter's matriline, grew large in
size.  From these calls, we can see that the frequency represented by
the letter L is very constant throughout the entire call, and is a
clear indication that this is an N5 call.

{\tiny
\begin{verbatim}
N05,A36,A36-N05-070806-D012-13913,IIJJJKKKKKLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
N05,A36,A36-N05-070806-D012-13917,JJJKKLLFLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
N05,A36,A36-N05-070806-D012-13921,IIJJKKKKKKKKKLLLLLLLLLLLLGLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
N05,A36,A36-N05-071506-D017-11311,IQHJKLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
\end{verbatim}
}


\section{Future Work}

From these promising results from using Yin for pitch estimation, a
nonlinear histogram binning procedure, and a non-linear histogram
binning approach, we would like to investigate changes to all three of
these sections of our algorithm.

We first wish to investigate the use of other pitch determination
algorithms, primarily the SWIPEP algorithm \cite{camachophd} which was
used in previous work with audio from chant traditions \cite{ness08}.
There are many other pitch determination methods that we wish to
investigate as well, including a convolutive probabilistic method from
Benetos and Dixon \cite{benetos11}, and a multi-tier approach by Sun
\cite{sun02}.

We also wish to investigate the use of histogram based methods for
deriving a histogram directly from the audio, as investigated in
\cite{ness08}.  This approach uses a series of gaussians to build up
an approximation of the frequency content of the sound directly from
the fundamental frequencies in a range of audio.  From Figure
\ref{fig:freq_dist}, we can clearly see three different lobes of
frequency, and a quantization approach that takes this into account
would be very useful.

In early work on this paper, we investigated another method of turning
audio into an alphabet, namely, clustering Mel-Frequency Cepstral
Coefficients (MFCC) into distinct clusters and using these clusters as
the letters for our alphabet.  Unfortunately the time complexity of
clustering algorithms depends heavily on the size of the number of
clusters, and thus the size of the alphabet.  For k-means clustering
the complexity is $O(n^{dk+1} log(n))$, where $d$ is the size of the input
vector, $k$ is the number of clusters, and $n$ is the number of input
vectors.  The MFCC algorithm typically gives vectors of size 13, but
can give a vector of size 26 or more.  For an alphabet of size 20, the
exponent is of order $13*20+1=261$, which caused problems in terms of
length of time.  With further reflection we may be able to find
another way of doing this clustering, either with a reduced set of
input vectors, or by finding some way to reduce the size of this
exponent.  

Alternatively, we could use algorithms from multidimensional
sequential pattern mining, such as that described by Pinto et
al. \cite{pinto01}.  These algorithms take as input a multidimensional
representation of a sequence, containing many parallel sequences, and
look for patterns in this sequence.  We feel this might be a good
approach for mining data directly from MFCC or other related spectral
representations of audio.

We also want to investigate the use of other tools from string mining
and bioinformatics in this problem domain.  The first algorithm that
we wish to try is the BLAST \cite{altschul90} algorithm, however we
have doubts about the applicability of this algorithm to our problem
domain, because instead of looking for small sequence fragments that
are identical or closely related, we are more looking for large
regions of similar sequence.

Instead, we see great hope in the use of other algorithms for mining
data in sequential patterns, such as those described by Dietterich
\cite{dietterich02}.  The PrefixSpan algorithm \cite{pei04} is also a
relevant algorithm for this work which mines sequential patterns.  The
SPAM algorithm \cite{ayres02} uses a bitmap representation for
sequences, and would also be useful to investigate for this dataset.

\section{Conclusion}

In this work we investigated the possibility of converting audio of
the vocalizations of Orcas into a discrete alphabet, thus making it
amenable to be studied by the large variety of bioinformatics and
string comparison tools extant in the scientific community.

In the first section of this work, we used the Yin algorithm to
determine the fundamental frequency of the audio of a curated call
catalog of orca vocalizations.  By adjusting various parameters of the
Yin algorithm using software custom designed for this project in the
OpenMIR suite, we were able to obtain pitch contours of this audio.

We then took the extracted pitch contours and used an algorithm that
first did a median filtering on the audio followed by a novel
algorithm that replaced parts of the pitch contour that were subject
to octave errors and output a pitch contour that had many fewer such
octave errors.  

A series of experiments looking at different ways of dividing the
frequency range of this audio into histogram bins was then performed.
We first tried a naive approach that simply divided the entire
frequency range into equal sized bins, but found that we obtained
improved performance when we used a non-linear frequency histogram.

This output of this histogram binned pitch contour was turned it into
letters, and used a local alignment algorithm to align these.  In
other work Dynamic Time Warping is typically used, an algorithm that
takes in real valued numbers and does a global alignment.  We found
that for this data, a local alignment outperformed a global
alignment. From this local alignment, we obtained quite satisfactory
classification results using an accuracy-at-top-1 approach of
approximately 80\%.

In conclusion, we see the technique of using the tools for
bioinformatics on the audio data of orca calls to be promising.  We
have converted the entire 18,000 hours of data currently in the
Orchive into pitch contours and are working on evaluating new string
mining algorithms on this dataset.


% Bibliography
\bibliographystyle{abbrv}
\bibliography{kdd2013}
\end{document}
% Created 2013-04-02 Tue 14:24
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}


\title{Original data}
\author{Steven Ness}
\date{02 April 2013}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents



\begin{tabular}{llrrrrrrrr}
 Data Set                         &  Params        &  Global  &    N47  &    N01  &     N12  &    N05  &    N04  &    N03  &    N09  \\
 Original                         &  Unweighted 1  &   0.642  &  0.250  &  0.878  &  0.4375  &  0.785  &  0.857  &  0.231  &  0.523  \\
 Octave Removal - Linear          &  Weighted 2    &   0.813  &  0.700  &  0.969  &  0.4375  &  0.857  &  1.000  &  0.667  &  0.782  \\
 Octave Removal - NonLinear 900   &  Weighted 3    &   0.806  &  0.300  &  0.939  &   0.750  &  0.714  &  1.000  &  0.667  &  0.818  \\
 Octave Removal - NonLinear 1200  &  Unweighted 1  &   0.814  &  0.400  &  0.969  &   0.583  &  0.785  &  1.000  &   0.75  &  0.772  \\
\end{tabular}



Table 1: Global Accuracy


\begin{tabular}{lr}
 Data Set                         &  Global Accuracy  \\
                                  &                   \\
 Original                         &              64%  \\
 Octave Removal - Linear          &              81%  \\
 Octave Removal - NonLinear 900   &              80%  \\
 Octave Removal - NonLinear 1200  &              81%  \\
 J48                              &              58%  \\
 Naive Bayes                      &              65%  \\
 SVM                              &              75%  \\
\end{tabular}



Table 2 : Accuracy per call (Octave Removal - NonLinear 1200)

\begin{tabular}{lr}
 Call  &  Accuracy  \\
       &            \\
 N47   &     0.400  \\
 N01   &     0.969  \\
 N12   &     0.583  \\
 N05   &     0.785  \\
 N04   &     1.000  \\
 N03   &      0.75  \\
 N09   &     0.772  \\
\end{tabular}











\end{document}
% IEEE Paper Template for US-LETTER Page Size (V1)
% Sample Conference Paper using IEEE LaTeX style file for US-LETTER pagesize.
% Copyright (C) 2006-2008 Causal Productions Pty Ltd.
% Permission is granted to distribute and revise this file provided that
% this header remains intact.
%
% REVISION HISTORY
% 20080211 changed some space characters in the title-author block
%
\documentclass[10pt,conference,letterpaper]{IEEEtran}
\usepackage{times,amsmath,epsfig}
\usepackage{subfigure}
\usepackage{url}
%
\title{Strategies for orca call retrieval to support collaborative 
annotation of a large archive}
%
\author{%
% author names are typeset in 11pt, which is the default size in the author block
{Steven R. Ness{\small $~^{\#1}$}, Alex Lerch{\small $~^{*2}$}, George Tzanetakis{\small $~^{\#3}$} }%
% add some space between author names and affils
\vspace{1.6mm}\\
\fontsize{10}{10}\selectfont\itshape
% 20080211 CAUSAL PRODUCTIONS
% separate superscript on following line from affiliation using narrow space
$^{\#}$\,Computer Science, University of Victoria\\
Canada \\
\fontsize{9}{9}\selectfont\ttfamily\upshape
%
% 20080211 CAUSAL PRODUCTIONS
% in the following email addresses, separate the superscript from the email address
% using a narrow space \,
% the reason is that Acrobat Reader has an option to auto-detect urls and email
% addresses, and make them 'hot'.  Without a narrow space, the superscript is included
% in the email address and corrupts it.
% Also, removed ~ from pre-superscript since it does not seem to serve any purpose
$^{1}$\,sness@sness.net, gtzan@cs.uvic.ca\\
% add some space between email and affil
\vspace{1.2mm}\\
\fontsize{10}{10}\selectfont\rmfamily\itshape
% 20080211 CAUSAL PRODUCTIONS
% separated superscript on following line from affiliation using narrow space \,
$^{*}$\,zplane.development inc. \\
Germany\\
\fontsize{9}{9}\selectfont\ttfamily\upshape
$^{2}$\,lerch@zplane.de
% 20080211 CAUSAL PRODUCTIONS
% removed ~ from pre-superscript since it does not seem to serve any purpose
%$^{2}$\,second.author@second.com
}
%
\begin{document}
\maketitle

% INCLUDES COPYRIGHT NOTICE: one of three copyright notice should be included. Uncomment the appropriate line below, according to the authors affiliation:
\begin{figure}[b]
\parbox{\hsize}{\em
%information about the event:
%copyright notice: one of three copyright notices below should be included. Uncomment the appropriate line, according to the authors affiliation:
% ???-?-????-????-?/10/\$??.?? \ \copyright 2010 IEEE. 
}\end{figure}


\begin{abstract}
  The Orchive is a large audio archive of hydrophone recordings of
  Killer whale (\textit{Orcinus orca}) vocalizations. Researchers and
  users from around the world can interact with the archive using a
  collaborative web-based annotation, visualization and retrieval
  interface. In addition a mobile client has been written in order to
  crowdsource Orca call annotation. In this paper we describe and
  compare different strategies for the retrieval of discrete Orca
  calls. In addition, the results of the automatic analysis are
  integrated in the user interface facilitating annotation as well as
  leveraging the existing annotations for supervised learning.  The
  best strategy achieves a mean average precision of 0.77 with the
  first retrieved item being relevant 95\% of the time in a dataset
  of 185 calls belonging to 4 types.
\end{abstract}


% NOTE keywords are not used for conference papers so do not populate them
% \begin{keywords}
% keyword-1, keyword-2, keyword-3
% \end{keywords}
%

% Copyright Section: choose the appropriate copyright notice for MMSP'09, acording to the authors' affilination:



\section{Introduction}
%
In recent years there has been increasing research activity in the
areas of multimedia learning and information retrieval.  Most of it
has been in traditional domains, such as sports video, news video, and
natural images \cite{hauptman97}.  There is broad interest in these
domains and in most cases there are clearly defined objectives such as
highlights in sports videos, explosions in news video or sunsets in
natural images. Some of the important research trends in multimedia
retrieval research have been the use of large collections for
supervised learning, the integration of the user interface and the
annotation/retrieval system, and the shift from single user system to
collaborative web-based interfaces which enable client-cloud
architectures.

Our focus in this paper is applying similar ideas to the Orchive
\cite{tzanetakis07}, a large archive of audio recordings of killer whale (\textit{Orcinus orca})
 vocalizations from the Northern resident community of British
Columbia. It has been shown that different killer whale communities
use distinct vocal signals \cite{ford00}. Pods are stable kin
groups and have unique vocal repertoires consisting of 7-17
distinct calls. Related pods often use structurally distinct versions 
of the same class type. 


Currently the Orchive contains approximately 10000 hours of audio data
digitized from the original analog cassettes with a projected total
size of 20000 hours (it would take approximately 6 years listening 8
hours every day to cover the entire archive). Traditionally
researchers had to digitize the analog cassettes and process the
resulting files individually on their computers. This tedious process
has inhibited researchers from analyzing the amounts of data that are
available. Harnessing the large amounts of data provided in the
archive holds enormous potential in advancing our understanding of how
these animals communicate. However, in order for the data to be
effectively analyzed it needs to be annotated and automatic retrieval
tools need to be developed. We have developed a collaborative
web-based interface that is enhanced with retrieval and classification
capabilities which are used to support the annotation process.

Unlike other areas of multimedia retrieval such as internet videos and
images for which annotations are easily obtained either by text
analysis or by manual entry from users, annotation is major challenge
in our application domain. Correct identification of the different
types of discrete Orca calls requires training and in the most
difficult cases can only be performed by an expert. The main challenge
that motivated the work described in this paper has been to obtain 
high quality annotations for this large specialized audio archive. 
We have followed a multi-pronged strategy in order to address this
challenge: using a client-cloud web-based collaborative interfaces we
can utilize volunteers from all around the world to perform the
annotation, using automatic similarity retrieval we assist the
annotation process especially for inexperienced users without 
affecting the quality of the resulting annotations, finally we use the
annotated data to build machine learning algorithms to further
annotate the data. 

Most of existing work in the automatic analysis for Orca calls has
focused either on the automatic detection of calls either in real-time
\cite{Luke2010771} or offline \cite{ness08} or on their classification
\cite{brown07_orca_dtw}. In contrast our focus is on similarity-based
retrieval. There are several reasons why retrieval is more important
than classification in our situation. Retrieval provides more fine
grained information than classification and supports the study of
variation within a particular call type. In addition it can deal with
calls of an unknown type or with classes that have a very small number
of examples. Users of our interface fall into two categories: experts
and volunteers. In many cases volunteers might not be able to classify
a call type by listening to it but can easily identify to which call
it is more similar from a limited set of examples. That way the call
can be indirectly classified. Similarity retrieval can provide this
limited set of examples. There have been several strategies and
representations proposed in the literature for the classification
and retrieval of Orca calls, but to the best of our knowledge they have
mostly been evaluated on small amounts of data and have not been
compared directly on the same data using retrieval effectiveness
metrics.

The main contributions of this work are: 1) a collaborative web-based
interface that integrates automatic similarity retrieval to enhance
the annotation process 2) a description of different strategies for
the retrieval of Orca calls 3) an experimental evaluation of these
different strategies on a large dataset using established retrieval
effectiveness metrics and 4) a post-processing step for denoising 
and exact boundary identification for presentation of the Orca calls. 




\section{Related Work} 
\label{sec:related} 

The main motivation behind our work has been creating better
interfaces for interacting with the Orchive \cite{tzanetakis07} a
large archive of audio recordings of Orca vocalizations.  There are
stable resident populations of \textit{Orcinus orca} in the
northwest Pacific Ocean, and some of these populations \cite{ford00}]
are found near Hanson Island, off the north tip of Vancouver Island in
Canada. Orcalab is a research station that has been recording audio of
these Orca populations since 1972
\cite{deecke99_quantifying_orca}. They have amassed a huge archive of
more than 20,000 hours of audio recordings collected via a permanent
installation of underwater hydrophones.

Most of existing work in the automatic analysis of Orca calls has
focused on detection and classification rather than retrieval. A
real-time system with low computational requirements for the detection
of Orca vocalizations is described in \cite{Luke2010771}. Annotation
bootstrapping is a technique used to classify/segment hydrophone
recordings into three broad categories: voiceover, background, and
vocalizations \cite{ness08}. 

Our work was influenced by two publications that described different 
representations and methodologies for the classification of Orca
calls. Orca vocalizations consists of well-defined, discrete calls
with tonal signal components. They can be characterized by the pulse
rate contour of the goal which can be viewed as analogous to the pitch
contour of a speech or monophonic music signal. A method for computing 
acoustic similarity between pulse rate contours (normalized so that
they all have the same duration) using the discriminative error of a
Artificial Neural Network is described in
\cite{deecke99_quantifying_orca}. 

Dynamic time warping (DTW) is a technique for measuring the similarity
of two sequence that many vary in time. It is mostly known in the context
of speech recognition \cite{sakoe78} but it has found applications in
many areas including video, motion and DNA sequence analysis. The use
of DTW to compute the similarity between two pulse rate contours in
the context of Orca calls has been explored in
\cite{brown07_orca_dtw}. In that work, a similarity matrix is
calculated containing all the DTW alignment costs between pairs of
pulse rate contours. This similarity matrix is then subsequently used
to calculate clusters which are then compared the ground truth call
labeling to assess the feasibility of call classification using this
approach.  Figure ~\ref{fig:dtw} shows two similarity matrices and the
corresponding alignments betwen two instances of the same call and two
different calls. As can be seen the alignment of the two calls of the
same type is much closer to a diagonal and the total score is lower. 

\begin{figure*}[t]
  \begin{center}
\subfigure[N1 versus N1]{
\includegraphics[height=55mm]{n1n1}
}
\subfigure[N1 versus N4]{
\includegraphics[height=55mm]{n1n4}
}
\label{fig:dtw} 
\end{center} 
\caption{A representation of the output of the Dynamic Time Warping (DTW) algorithm which is used to calculate the path of best similarity between two sequences of pitch values.  Shown here are similarity matricies, one is between two different instances of the N1 call, and the other is the N1 call compared to an N4 call.  The black line shows the path of best match as determined by the DTW algorithm.  The N1 call compared to itself had a DTW score of 0.60333, while the N1 call compared to the N4 call had a worse DTW score of 3.35097.    } 
\end{figure*} 


In this paper we focus on retrieval rather than classification and
only use the ground truth labels as a way to measure retrieval
effectiveness. We compare different strategies over a large dataset
(185 calls, 4 classes) using well established retrieval effectiveness
measures. To the best of our knowledge this is the first systematic 
evaluation of these different design choices over a data set that is
significantly larger than the ones used in the existing literature. 
It also the first time a full collaborative web-based client-cloud
system has been developed to support research in bioacoustics which 
is typically performed individually using desktop applications. 

\section{System Description}

\subsection{Contour Extraction and Retrieval Strategies}  

The discrete calls of killer whales are pulsed signals in which a tone
(of a certain tonal frequency) is not emitted continuously but in
pulses given by the pulse-repetition rate. Unlike the tonal signals of
many birds and other delphinids, the highest energy is not always
contained in the first or second harmonic
\cite{deecke99_quantifying_orca}. The high levels of background noise
and variety of recording conditions compound the difficulty of
obtaining pulse rate contours. The pulse rate contour is used as the
primary representation for Orca calls because it is more robust as
compared to spectral features to levels of background noise typical in
field recordings.

We have compared three pitch extraction methods for obtaining the
pulse rate contour. The first method ({\bf PRAAT}) is based on time-domain
autocorrelation and is similar to the pitch extraction algorithm
implemented in Praat \cite{boersma93}. It is based on calculating the
time-domain autocorrelation of the signal: 
\begin{equation} 
R(\tau) = \frac{1}{N} \sum_{n=0}^{N-1-m} x[n] x[n+m] \;\;\;\; 0 \leq m
< M 
\end{equation} 

The peaks of the autocorrelation function correspond to the lags 
in which the signal is self-similar. The signal is processed in
windows and the autocorrelation of the windowed signal $R_{xw}$ is divided 
by the autocorrelation of the window $R_{w}$ providing better robustness to 
noise and better accuracy. 

\begin{equation}
R_{x}(\tau) = R_{xw}(\tau) / R_{w}(\tau)
\end{equation} 


The second method is based on the {\bf YIN} pitch extraction method.  The YIN method is 
based on the difference function which is similar to the
autocorrelation: 

\begin{equation}
d{t} = \sum_{n=0}^{N-1} (x[n] - x[n+\tau])^{2}
\end{equation} 

The dips in the difference function correspond to periodicities. 
In order to reduce the occurrence of subharmonic errors, YIN employs a
cumulative mean function which de-emphasizes higher period dips 
in the difference function. 

The third method ({\bf SACF}) is based on the multipitch detection algorithm
described by Tolonen and Karjalainen \cite{tolonen00}.  In this algorithm, the
signal is decomposed into two frequency bands (below and above 1000
Hz) and amplitude envelopes are extracted for each frequency band. The
envelope extraction is performed by applying half-wave rectification
and low-pass filtering.  The envelopes are summed and an enhanced
autocorrelation function is computed so that the effect of integer
multiples of the peak frequencies to multiple pitch detection is
reduced.



We explore three retrieval strategies/representations. Statistical
features characterizing the entire pulse rate contour are computed and
each call is characterized by a single vector of features. The
features are normalized by max-min normalization so that they range
from 0 to 1 over the entire dataset. Similarities are then computed by
taking the Euclidean distance in the normalized space. This strategy
is used as reasonable baseline. The features used in this work are the
mean, median, standard deviation, min and max of the pulse rate
contour. The second strategy consists of resampling the pulse rate
contour using linear interpolation to a fixed number of points. This
strategy is similar to the one used in Deecke
\cite{deecke99_quantifying_orca}. Essentially it assumes that the
duration of the call does not play a major role in its
characterization and temporal scaling is applied uniformly across the
contour. The third strategy utilizes dynamic time warping to align the
pulse rate contours. The alignment cost is used to measure the
similarity between calls. The two sequences to be matched are arranged
on the sides of a grid. To find the best match between the sequences
we can find a path through the grid that minimizes the total distance
between them. More details can be found in \cite{sakoe78}.



\begin{figure*}[t]
\begin{center}
\includegraphics[width=150mm]{orchive}
\label{fig:orchive} 
\caption{
Orchive collaborative web-interface showing how annotation can support
retrieval. The automatically suggested call is shown on the right and
can be used for immediate annotation if the user agrees that the
selected call is of the same type after listening to it.} 
\end{center} 
\end{figure*} 


\subsection{Collaborative web-interface} 

Collaborative web-based interfaces have significant advantages
compared to desktop applications especially in the context of
research in bioacoustics. In the most common traditional setup each
researcher has access to their own data collection and performs
annotations using desktop applications such as audio editors and 
general purpose software applications such as spreadsheets. In such a
context it is hard to leverage annotations from multiple users or
share large data-sets such as the Orchive. By moving into a
client-cloud mode of interaction we are able to have several users
from around the world access the Orchive simultaneously and can store 
and analyze their annotations centrally. The entire archive is also 
regularly backed up and replicated as more files are digitized and 
more annotations are added. The flexibility of the architecture
enables the easy creation of new interfaces such as the mobile game 
client described later in this paper. 


Figure ~\ref{fig:orchive} shows a screen-shot of the collaborative
web-interface. The user has selected a region corresponding to call
that needs to be annotated. Under the spectrogram a call catalog
is provided to help the user with annotation. The interface uses the
similarity retrieval described in this paper to suggest the most
likely call type to the user by showing it zoomed in the window to the
right. The call is then correctly annotated as of N4 type. The
interface is written in the Django web development framework
\footnote{\url{http://www.djangoproject.com/}} and interfaces directly
with the Marsyas open source audio analysis software
\footnote{\url{http://marsyas.info}} through Python bindings. The entire
interface runs inside a web browser and can be accessed at
{\url{http://orchive.cs.uvic.ca}}. Viewing is public but annotation is
restricted to registered users. 









\begin{figure}[htb]
\begin{center}
\includegraphics[width=50mm]{SAM_1388}
\label{fig:mobile} 
\caption{
Mobile client interface for Orca call annotation.}
\end{center} 
\end{figure} 


\subsection{Mobile client} 

A game with a purpose (GWAP) \cite{gwap} is a type of computer game that can be
used to collect interesting data about a task while at the same time 
being entertaining to the players. The most well known example is the
ESP game \cite{esp} in which two players attempt to assign labels to images and
in the process provide a wealth of annotation data. We have developed
a similar game for classifying Orca vocalizations. The player is
presented with a call that needs to be annotated and the two closest 
examples from a labeled dataset based on content-based similarity
retrieval with the constraint that the two examples come from
different classes. The player then selects the call that is more
similar and the label is sent to the database for storage. By probing
the user with labeled samples we can assess their skill in
annotation and decide whether to use their labels or not. The game and
client is written for the Android mobile operating system. 



\subsection{Usage statistics} 

Since launch in July 2008, there have been just over 5000 visitors to the site from
74 countries.  These users have viewed just under 30,000 pages, with
an average of 5.73 pages per visit.  Each visitor spent an average of
3 minutes 44 seconds on the website.  24 researchers have been granted
annotation access to the Orchive, and amongst them they have entered
5708 annotations of orca calls using our web based interface.  These
annotations are on only about 1000 recordings, which means the vast
majority of the 14862 recordings in the database have yet to be
annotated.  These recordings are 45 minutes long, which means a total
of almost 700,000 minutes of data currently is in our archive, with
more data being digitized constantly.


\begin{figure*}[t]
  \begin{center}
\subfigure[Before]{
\includegraphics[height=40mm]{A34_N10}
}
\subfigure[After]{
\includegraphics[height=40mm]{A34_N10_snip}
}
\label{fig:snip} 
\end{center} 
\caption{Post-processing of Orca calls for presentation. The selected 
call on the left is processed in order to remove the noisy hydrophone
parts preceding and following it as well as enhancing the tonal
components.} 
\end{figure*} 


\subsection{Post-processing} 

The annotation process by users is not very accurate and frequently
includes noisy background parts of the signal. A spectral peak
picking approach is used to identify the tonal components of the input
signal. These components, separated from the noise-like signal parts
are resynthesized to provide a more clear sounding call that still
retains its identifying characteristics. A result of this denoising
compared to traditional denoising based on spectral profiling can be
found and heard at: \url{http://orchive.cs.uvic.ca/main/tour}. 





	
In a first processing step, peaks are pre-selected as candidates for
tonal components by ensuring that a) their amplitude is the largest
local maximum within small blocks of neighboring frequency bins, b)
their frequency is in a pre-defined frequency range and c) their
amplitude is above a threshold that is computed relative to the
overall spectral maximum, the overall spectral energy and a smoothed
version of the magnitude spectrum.  Then, a ``tonal probability'' is
computed for every peak candidate and the candidates with the highest
probability are selected as tonal components. The probability is
computed as the arithmetic mean of a) the Gaussian of the distance of
the peak's bin frequency and its instantaneous frequency, b) the
normalized logarithmic distance between the peak's amplitude and a
simultaneous masking threshold computed according to the
psycho-acoustic model in ITU recommendation BS.1387, and c) the
amplitude of a peak spectrogram smoothed in both time and frequency
domain so that sporadic peaks have low amplitude and ``steady'' peaks
have higher amplitude. This tonal probability is also used to remove
the parts of the annotation that contain background noise as shown in
Figure \ref{fig:mobile}. It is hard to evaluate quantitatively the
effectiveness of the post-processing step as there is no ground-truth
for what the clean Orca calls would sound like. In all the cases that
we tried the boundaries of the call were correctly identified and the
perceived quality of the call is much better.





\section{Experimental Evaluation}

In order to systematically explore the different strategies for Orca
call retrieval we utilized a dataset consisting of 185 recordings of
vocalizations. They have been annotated using the Orchive
collaborative user interface and classified into 4 discrete call types
by volunteers. The ground truth labels have been verified by
experts. Table ~\ref{table:dataset} shows the composition of the
dataset used for evaluation. We use two established evaluation metrics
that measure the retrieval effectiveness. Precision at 1 is simply the
number of queries for which the first retrieved call has the same
class as the query. The mean average precision (MAP) is the most
frequently used summary measure of a ranked retrieval run. Average
precision of a single query is the mean of the precision scores after
each relevant document has been retrieved. The value for the run (a
set of queries) is the mean of the individual average precision
scores. MAP combines aspects of both precision and recall and rewards
returned relevant items higher in the list.


\begin{table} 
\begin{center}
\caption{Dataset composition and MAP scores for best configuration
  (Hertz frequency scale, SACF pitch extractor and DTW matching) } 
\begin{tabular}{|l|c|c|c|c|}
\hline
Call Type     & N1      & N3      & N4 & N47 \\ 
\hline 
Instances     & 36      & 56      & 60  &  33 \\ 
\hline 
MAP            & 0.63   &  0.94   & 0.78  & 0.58  \\ 
%  Precision@1 &      &       &      &       \\ 
\hline 
\end{tabular} 
\label{table:dataset}
\end{center}
\end{table} 


Table ~\ref{table:dataset} shows the best MAP scores achieved for each
type of call. As can be seen there is large variance in the MAP score
for different types of calls. For example retrieval of N3 calls is
very robust but retrieval of the N47 calls is not as much. These
differences are also observed in the human classification of these
calls. 

Table ~\ref{table:dataset_map} shows the MAP scores and
average precision score at 1 over the entire dataset for combinations
of different representations and pulse rate extraction strategies. As
can be seen, the SACF pitch extractor is the best performing pitch
extraction method independently of the retrieval strategy. The DTW
matching is also the best performing retrieval strategy. It is hard to
draw any conclusions with respect to SACF performing better than the
other two pitch extractors. The better results obtained using the DTW
retrieval strategy indicate there is important non-uniform timing
variation in the structure of these calls. 

We have also conducted experiments with different frequency scale
representations such as the Bark-scale \cite{zwicker1961} and
logarithmic frequency but in all configurations they performed worst
than the default linear frequency representation in Hertz.



\begin{table} 
\begin{center}
\caption{Mean Average Precision Scores for different pitch extraction 
and retrieval strategies} 
\begin{tabular}{|l|c|c|c|}
\hline   
              & Features           & Contour & DTW \\
\hline 
PRAAT     & 0.38       & 0.52         & 0.67 
   \\ 
\hline 
YIN          & 0.50        & 0.51         & 0.72  \\ 
\hline 
SACF        & 0.63     & 0.66          &  0.77 \\ 
%  Precision@1 &      &       &      &       \\ 
\hline 
\end{tabular} 
\label{table:dataset_map}
\end{center}
\end{table} 


\begin{table} 
\begin{center}
\caption{Average Precision at 1 scores for different pitch extraction 
and retrieval strategies} 
\begin{tabular}{|l|c|c|c|}
\hline   
              & Features & Contour & DTW \\
\hline 
PRAAT     &    0.38      &       0.40       &  0.4      \\ 
\hline 
YIN          &    0.77         &   0.72          &       0.95 \\ 
\hline 
SACF        &    0.79   &    0.82    &  0.95 \\ 
%  Precision@1 &      &       &      &       \\ 
\hline 
\end{tabular} 
\label{table:dataset_prec1}
\end{center}
\end{table} 




\section{Conclusions and Future Work} 

We describe a web-based collaborative web interface for retrieval and
annotation of a large archive of Orca vocalizations. A number of
different strategies for pulse rate extraction and retrieval were
experimentally compared. Excellent results were obtained by a
combination of a computationally efficient pitch extractor based on
the summary autocorrelation function and a matching strategy based on
dynamic time warping. The best configuration achieves a mean average
precision of $0.77$ and the first retrieved call is relevant $95\%$ of
the time. In the future we plan to expand our dataset both in terms of
instances and call types.  In addition we plan to investigate more
thoroughly different design choices especially in the pitch contour
extraction possibly taking into account information about the hearing
system of Orcas \cite{nummela99_orca_ear_anatomy}. Finally we plan to
expand our prototype annotation game with various types of
challenges. Although the community of users interested in Orca
vocalizations is small their are passionate and eager to help. 









\section{Acknowledgments}

We would like to thank Paul Spong and Helena Symonds of Orcalab for
providing the data and inspiration for this project. We would also
like to thank the National Sciences and Engineering Research Council
(NSERC) for their financial support. This work was partly supported by
a fellowship within the Postdoc-Programme of the German Academic
Exchange Service (DAAD).

\pagebreak

\bibliographystyle{IEEEtran}

\bibliography{IEEEabrv,mmsp2011gtzan}

\end{document}
\startchapter{Glossary}
\label{chapter:glossary}

ANOVA (Science) - Analysis of Variance between groups - A group of statistical techniques to test hypotheses based on experimental results.  It tests the distribution of the means of variables to see if they are the same, assuming the variables are normally distributed.
\\

Computational Ethnomusicology (Musicology) - a new field that uses the computational techniques commonly used in Music Information Retrieval (MIR) to analyse music and audio from social and cultural traditions from around the world.
\\

Cross-validation (Computer Science) - A method used to test the accuracy of machine learning classifiers by separating a dataset into examples used to train the classifier and examples used to test the classifier.
\\

Ethnomusicology (Musicology) - a sub-discipline of Musicology that focuses on the study of the socio-cultural aspects of music in societies around the world.
\\

Folksonomy (Computer Science) - a system of classification that comes from a group of users collaboratively creating and managing tags in an effort to annotate and categorize content
\\

Flux (MIR) - In this work, is the norm of the difference vector between two successive magnitue/power spectra.
\\

Gesture (Musicology) - In it's simplest definition, a gesture is simply the pitch contour of a sung musical phrase.  However, this word has become imbuded with subtle meanings in Ethnomusicology, and can be thought of as not just the pitch contour, but is also related to the gesture of the conductor of the piece of music and the intention of the performer.
\\

Hermeneutic (Musicology) - The study of interpretation, particularily the interpretation of the Bible and Torah.
\\

Heterophonic (Musicology) - Two voice singing a melody (or harmonies of that melody) at one time
\\

MFCC coefficients (MIR) - A way to transform a standard spectrum into one that more closely approximates how the human ear perceives sound.
\\

Monophonic (Musicology) - One voice singing a melody at one time
\\

Music Information Retrieval (Computer Science) - also known by the acronym MIR, a new field of study where one applies tools from areas such as Digital Signal Processing, Audio Feature Extraction and Machine Learning to help people understand and retrieve information from music or audio.
\\

Ontology (Computer Science) - a formal representation of ideas or concepts and the relation between them.  This Computer Science definition of ontology is a subset of the broader philosophical idea of ontology, which is a study of what exists and what can exist.
\\

Open Sound Control (Computer Science + Music) - A new content format for sending musical data between computers
\\

Paradigmatic (Musicology) - In a text or a song, the relationships between symbols, and the analysis of how symbols relate to each other both in one text and amongst a group of texts.
\\

Pashta (Musicology) - One of the cantillation signs from the Torah - It means ``Stretching out'', because its shape leans forward.
\\

Phase Vocoder (Computer Science + Music) - A type of digital audio filter which can scale audio in both time and frequency
\\

Rolloff (MIR) - A measure of the steepness of falloff in an audio spectrum
\\

Semiotics (Musicology) - The study of signs and signifiers
\\

Self-Organizing Map (Computer Science) - A technique that maps a high dimensional feature space to a lower space.  It is a similar technique to artifical neural networks.

Siratok (Musicology) - A form of lament song found in Hungary
\\

Sof Pasuq (Musicology) - One of the cantillation signs from the Torah.  It means ``End of verse''
\\

Spectral Centroid (MIR) - A measure of the ``center of mass'' of a spectrum.
\\

Syntagmatic (Musicology) - In a text or song, the symbols and the way that they can be joined together.  The surface structure of a text or a song.
\\

Tag Cloud (Computer Science) - a two-dimensional graphical representation of words related to a topic where words are typically arranged alphabetically or randomly.  Words that are of higher relevance are usually shown in a larger font.
\\

Wii remote (Computer Science) - The Wii is a new computer gaming system developed by Nintendo, which has a novel input controllers containing not just buttons, but a 3D accellerometer, an infrared camera, a speaker and a vibration generator.  The Wii remote is wireless, communicating with the base station via bluetooth, which can be received by a standard computer with a bluetooth receiver.
\\

XML (Computer Science) - eXtensible Markup Language - A popular tree-tree based format for encoding data
\startchapter{Publications from this Research}
\label{chapter:publications}

Presented here is a list of all the publications that come from the
research presented in this thesis:
\\

Steven R. Ness, Daniel Peter Biro and George Tzanetakis
Computer-assisted cantillation and chant research using content-aware
web visualization tools, Multimedia Tools and Applications, Accepted
for publication
\\

S. R. Ness, A. Theocharis, G. Tzanetakis, L. G. Martins Improving
Automatic Music Tag Annotation Using Stacked Generalization Of
Probabilistic SVM Outputs, ACM Multimedia 2009
\\

Steven R. Ness, George Tzanetakis, Audioscapes: exploring surface
interfaces for music exploration - ICMC 2009
\\

Steven R. Ness, George Tzanetakis, SOMba : Multiuser music creation
using Self-Organizing Maps and motion tracking - ICMC 2009
\\

Steven R. Ness, Daniel Peter Biro, George Tzanetakis : Content-aware
web browsing and visualization tools for cantillation and chant
research, 7th International Workshop on Content-Based Multimedia
Indexing
\\

George Tzanetakis, Manjinder Singh Benning, Steven R. Ness, Darren
Minifie, Nigel Livingston : Assistive Music Browsing using
Self-Organizing Maps - PETRA 2009 : 2nd International Conference on
PErvasive Technologies Related to Assistive Environments
\\

Steven R. Ness, Matthew Wright, Luis Gustavo Martins, George
Tzanetakis: Chants and Orcas: semi-automatic tools for audio
annotation and analysis in niche domains. ACM Multimedia 2008: 9-16
\\

Daniel Peter Biro, Steven Ness, Matthew Wright, W. Andrew Schloss and
George Tzanetakis Decoding the Song: Histogram-Based Paradigmatic and
Syntagmatic Analysis of Melodic Formulae in Hungarian Laments, Jewish
Torah Trope, Tenth Century Plainchant and Koran Recitation EMUS
Expressivity in MUsic and Speech : IRCAM - Institut de Recherche et de
Coordination Acoustique/Musique - Paris, France
\\
\documentclass[12pt,oneside]{book}
\include{macros/style}
\include{macros/use_packages}
\usepackage{url} 

\begin{document}

% Front Matter
\input frontmatter/fm

\newpage

\include{chapter_introduction/chapter_introduction}
\include{chapter_related_work/chapter_related_work}
\include{chapter_chants/chapter_chants}
\include{chapter_music/chapter_music}
\include{chapter_orchive/chapter_orchive}
\include{chapter_conclusions/chapter_conclusions}
\include{glossary}
\include{weblinks}
\include{publications}


\TOCadd{Bibliography}
\bibliographystyle{alpha}
\bibliography{thesis}




\end{document}
\startchapter{Web Links}
\label{chapter:weblinks}

The following are web links to the various web-based tools developed
in the course of this research:
\\

Cantillion : \url{http://cantillion.sness.net}.
\\

Orchive : \url{http://orchive.cs.uvic.ca}
\\

Audioscapes : \url{http://audioscapes.sness.net}
\\
% -----------------------------------------------
% Template for NIME 2009
%     nime09.sty -> style file
% By Roger B. Dannenberg (rbd@cs.cmu.edu)
% Based on ISMIR 2006 style by 
%     Kjell Lemstrˆm (klemstro@cs.helsinki.fi)
% -----------------------------------------------

\documentclass[letterpaper,10pt]{article}
% I added the following to get letter-size paper using
% pdflatex on OS X. I'm not sure where this pdflatex
% came from and don't see any configuration files, but
% this worked.
\pdfpageheight \paperheight
\pdfpagewidth \paperwidth
\usepackage{nime09}
% A previous version of this template had [dvips] which
% did not work with pdflatex. On the other hand, I 
% expect that [pdftex] does not work with normal latex.
\usepackage[pdftex]{graphicx}


% Title.
% ------
\title{\LaTeX\ Paper/Poster Template for NIME 2009}

% Single address
% To use with only one author or several withz the same address
% ---------------
%\oneauthor
%  {Author} {School \\ Department}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {1st author}{ 1st author's affiliation \\ 1st line of address \\ 2nd line of address}{1st author@univ.edu}
  {2nd author}{2nd author's affiliation \\ 1st line of address \\ 2nd line of address}{2nd author@univ.edu}
  {3rd author}{3rd author's affiliation \\ 1st line of address \\ 2nd line of address}{3rd author@univ.edu} 
\begin{document}
%
\maketitle
%
\begin{abstract}   %do not leave a blank line after this
This is the \LaTeX\ template.  It tells you how to format your NIME
2009 paper.  Use this template in association with the {\tt nime09.sty}
style file.  A real abstract should be 150-200 words. 
\end{abstract}

\begin{keywords}
NIME, \LaTeX, formatting.
\end{keywords}

\section{Introduction}\label{sec:introduction}

This template includes all the information about formatting manuscripts
for the New Interfaces for Musical Expression Conference
(NIME 2009).  Please follow these guidelines to give the final
proceedings a uniform look.  If you have any questions, please contact
the Conference Management. 

This template can be downloaded from the NIME 2009 web site
(http://www.nime2009.org). 

It is recommended that you print your paper before submission.  Check
that all the figures are high quality and easily viewed.  Check the
spelling and grammar before submission.  Ensure that you have not
exceeded the maximum page limits (4 for short papers, 6 for long papers,
2 for posters and demos). 

Each paper should be written uniformly in English.  Furthermore, your
paper may be improved by having it proofread by someone other than the
main author, especially if the main author is not a native English
speaker. 

The proceedings are the records of the conference.  NIME hopes to give
these conference by-products a single, high-quality appearance.  To do
this, we ask that authors follow some simple guidelines.  In essence, we
ask you to make your paper look exactly like this document.  The easiest
way to do this is simply to download this template and replace the
content with your own material. 

\section{Using this template}\label{sec:using_template}

This template and the associated style file take care of the formatting
for \LaTeX\ documents.

As you have noticed by now, paragraphs are generated in \LaTeX\ by
leaving a blank line in between two paragraphs.

\section{Format Guidelines}\label{sec:format}

Again, this template is designed to achieve consistent formatting.  The
intended formatting is described in this section. 

\subsection{Page Size}\label{subsec:page_size}

The proceedings will be printed on letter-size (paper ($21.6 \times
27.9$ cm or $8.5" \times 11"$).  All material on each page should fit
within a rectangle of $17.78 \times 22.86$ cm ($7" \times 9"$), centered
on the page, beginning 2.54 cm (1") from the top of the page and ending
2.54 cm (1") from the bottom.  The right and left margins should be 1.9
cm (.75").  The text should be in two 8.4 cm (3.3") columns with a 1 cm
(.4") gutter.  When you print your document, {\it be sure to select
Letter size paper for the printer.}

\subsection{Normal or Body Text}\label{subsec:body}

This template uses a 10-point Times New Roman font. 
Use sans-serif or non-proportional fonts only for special purposes,
such as distinguishing source code text. Paragraphs should be flush left.
Right margins should be justified, not ragged. The first text paragraph
after a heading is {\it not} indented.

\subsection{Title and Authors}\label{subsec:title}

The title (Times New Roman, 15-point bold, style name ``Title''), authors'
names (Times New Roman, 12-point, style name ``Author''), affiliations
(Times New Roman, 10-point, style name ``Affiliation'') and email
addresses (Courier New, 10-point, style name ``Email'') run across the
full width of the page.  Authors' names are centered.  The lead author's
name is listed first (left-most), and the co-authors' names after.  If
the addresses for all authors are the same, include the address only
once, centered.  If the authors have different addresses, put the
addresses, evenly spaced, under each author's name.  If several authors
have similar email addresses, they may be listed together, e.g.: \\
%
{\tt john.smith,peter.jones@uni.edu.}

\subsection{Abstract and Keywords}\label{subsec:abstract}

The abstract is created using the \LaTeX\ Abstract environment.  The 150-200 word
abstract is optionally followed
by keywords, using the \LaTeX\ Keywords environment.

\subsection{First Page Copyright Notice}\label{subsec:copyright}

Please insert the copyright notice as shown on the first page.  It
should be typeset in 8-pont Times Roman (or similar font), in a box the
width of the column at the bottom of the column. 

\subsection{Subsequent pages}\label{subsec:subsequent_pages}

For pages other than the first page, start at the top of the page, and
continue in double-column format.  The two col\-umns on the last page
should be as close to equal length as possible. 

\begin{table}
\caption{Table captions should be placed above the table.}
\label{tab:example}
\vspace*{0.5em} % In this case, a little separation between the
                % caption and the table looks nicer than none.
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
{\bf Graphics} & {\bf Top} & {\bf In-between} & {\bf Bottom} \\
\hline
Tables & End & Last & First \\
\hline
Figures & Good & Similar & Very well \\
\hline
\end{tabular}
\end{center}
% I can't explain why table ends with so much white space
% but the following line gets rid of it
\vspace{-1.5em}
\end{table}

\subsection{Citations and References}\label{subsec:references}

All bibliographical references should be listed at the end, inside an
unnumbered section named ``References'' (heading style is ``References
Heading'').  References themselves are numbered in the order of their
first appearance in the text and fill the entire width of the column. 
All references listed should be cited in the text.  References should be
formatted in the IEEE style.  Examples can be found in the References
section of this document, and also on the web, for example here:

http://www.lib.unimelb.edu.au/cite/ieee/

When refering to a document, type the number in square brackets 
\cite{Roads:01}. Better yet, use Latex's cross-reference 
facility to number references automatically (for an example, see 
the template source code for this paragraph).

\subsection{Page Numbering, Headers and Footers}\label{subsec:numbering}

Do not include headers, footers, or page numbers in your submission. 
These will be added when the publications are assembled. 

\section{Section Headings}\label{sec:section_headings}

Main section headings are paragraphs of style ``Heading 1.'' The heading
of a section should be in Times New Roman 12-point bold, with initial
letters capitalized, flush left with 6 points of white space above the
section head and 4 points below.  Sections and subsequent sub-sections 
should be numbered
and flush left.  (Note: In headings, a word like ``the'' or ``a'' is not
capitalized unless it is the first word.)

\subsection{Subsections}\label{subsec:subsections}

The subsection headings should be in Times New Roman 10-point bold with only 
the initial leters capitalized, flush left, with 6 points of white space above
and 3 points below. For a section and subsection head together (as in 
Section 5.1) using this template, you should increase the separation by
inserting the command {\tt $\backslash$vspace*\{0.4em\}} (see the template source code).

By the way, the previous paragraph has a reference to another section. Note that ``Section'' in ``Section 5.1'' is capitalized.

\subsubsection{Subsubsections}\label{subsubsec:subsubsections}

The heading for subsubsections should be in Times New Roman 10-point italic with initial letters capitalized and 6 points of white space above the subsubsection head and 3 points below. Do not use deeper nesting (subsubsubsections)! Are you writing a paper or an outline?

\section{More Numbered Text}\label{sec:numbered_text}
\vspace*{0.4em}
\subsection{Footnotes}\label{subsec:footnotes}

Indicate footnotes with a number\footnote{{\small This a footnote}} 
in the text.
Use 9-point type (the ``small'' font size) for footnotes.  
Place the footnotes at the bottom of
the page on which they appear.  Precede the footnote with a 0.5pt
horizontal rule. 

\subsection{Figures/Captions}\label{subsec:figures/captions}

Place Tables/Figures/Images in text close to the reference (see 
Figure~\ref{fig:example}).
It may extend across both columns to a maximum width of 7 inches (17.78cm).

Captions should be Times New Roman 9-\-point bold and numbered, 
e.g. ``Table 1'' or ``Figure 2.'' Please note that the words for Table and
 Figure are spelled out. Figure captions should be centered beneath the 
image or picture, and Table captions should be centered above the table
body (note how this is done in the examples). 

\begin{figure}
\centerline{\framebox{
	\includegraphics[width=3.1in]{logo.pdf}}}
% note: framebox takes additional space, so do not set
% graphics width to \columnwidth.
\caption{Figure captions should be placed below the figure.}
\label{fig:example}
% like the table, the figure is followed by quite a bit of
% white space. The following line tightens the layout.
\vspace{-1em}
\end{figure}

\subsection{Equations}\label{subsec:equations}

Equations should be placed on separated lines and numbered.
Using the following form you achieve numbered equations automatically.

\begin{equation}
E=mc^{2}
\end{equation}

\section{Final Words}\label{sec:final}

Embed all fonts, especially any unusual ones, when converting to PDF. 
Make sure you select Letter size paper as the output format with scaling
turned off or set to 100\%. Print your PDF file and check it. 

Check grammar and spelling throughout the document.  It may help to have
a native English speaker proofread the document before submission. 

Finally, be sure that the reader can grasp the importance of your work
to the field. 

\section{Acknowledgments}\label{sec:acknowledgments}

This template is based on designs used by the ACM and in particular ISMIR 2006. 

% this command forces a column break, which makes the columns on the last
% page approximately the same length. You should move this command to
% the optimal location in your document. You may need to manually
% edit an automatically generated references section to achieve the
% desired layout
\newpage

\begin{thebibliography}{citations}

\bibitem {Roads:01} C. Roads, {\it Microsound}, Cambridge, Massachusetts: MIT Press, 2001.

\bibitem {Rovan:00} 	J. Rovan and V. Hayward. ``Typology of Tactile Sounds and their Synthesis in Gesture-Driven Computer Music Performance,'' in {\it Trends in Gestural Control of Music}, M. M. Wanderley and M. Battier, Eds. Paris: IRCAM, 2000, pp. 389-405.

\bibitem {Essl:00} G. Essl and P. R. Cook, ``Measurements and efficient simulations of bowed bars,'' {\it J. Acoust. Soc. Am.}, vol. 108, no. 1, pp. 379-388, 2000.

\bibitem {Dannenberg:05} R. Dannenberg, B. Brown, G. Zeglin, and R. Lupish. ``McBlare: A Robotic Bagpipe Player,'' in {\it Proc. of the Conf. on New Instruments for Musical Expression (NIME)}, 2005, pp. 80-84.

\bibitem {NIME:05} ``The 2005 International Conference on New Interfaces for Musical Expression,'' [Web site] 2005, [2008 Sep 20], Available: http://www.nime.org/2005/

\end{thebibliography}

%Again, the columns here should be of equal length on the last page.
%Use the \newpage command rather than the \pagebreak command to achieve
%this result.

\end{document}
% -----------------------------------------------
% Template for NIME 2009
%     nime09.sty -> style file
% By Roger B. Dannenberg (rbd@cs.cmu.edu)
% Based on ISMIR 2006 style by 
%     Kjell Lemstrˆm (klemstro@cs.helsinki.fi)
% Modified by Steven Ness (sness@sness.net)
% -----------------------------------------------

\documentclass[letterpaper,10pt]{article}
\pdfpageheight \paperheight
\pdfpagewidth \paperwidth
\usepackage{nime09}
\usepackage[pdftex]{graphicx}

%
% Title
%
\title{SOMba - Co-creation of music by tracking dancers in real time on a Self-Organizing Map using the Wii remote, Open Sound Control and Marsyas}

%
% Authors
%
\twoauthors
  {Steven R. Ness}{ University of Victoria \\ Department of Computer Science \\ University of Victoria \\ Victoria, BC, Canada}{sness@sness.net}
  {George Tzanetakis}{ University of Victoria \\ Department of Computer Science \\ University of Victoria \\ Victoria, BC, Canada}{sness@sness.net}
\begin{document}

%
\maketitle
%

%
% Abstract
%
\begin{abstract}
SOMba is a system where multiple users create new rhythms and music by
moving around a physical space and are tracked in real time using the
Infrared sensor on the Wii remote control.  The physical space they
move around in is mapped to a 2D Self-Organizing map.  This SOM is
created by Marsyas from a collection of aligned 1-bar Samba rhythms of
a variety of Brazilian musical instruments.  A user is mapped to a
unique point in the SOM, and this point contains a single rhythm.  As
the user moves around the 2D space, different rhythms are played.
Multiple users can move around a space, and each would make Marsyas
play a different rhythm.
\end{abstract}

\begin{keywords}
Self-organzing maps, Position Tracking, Collaborative music creation
\end{keywords}

\section{Introduction}\label{sec:introduction}

Since the early dawn of music in pre-historic tribal societies,
music has often been accompanied by dance.  In some traditions, like
in native american tribes, the dancers themselves would participate
in the music making experience by wearing noise making artifacts.
However, in most situations, the music making and dancing activities
have been relatively separate.  When Samba bands perform in Brazil,
musicians are joined by large numbers of dancers, who only
indirectly participate in the music making experience.

Self-organizing maps \cite{kohonen95a} are a multidimensional
scaling technique where a high dimension dataset is mapped to a
lower, typically 2-dimensional surface. SOMs and other such
dimensionality reduction tools allow users to visualize data
relationships within complex datasets.  There have been many
applications of SOMs in Music Information Retrieval including work
by automatically analyzing and organizing music archives
\cite{rauber01a} \cite{rauber98b}, visualizing music genres
\cite{pampalk03}\cite{pampalk03} and music recommendation
\cite{vembu05a}.

A less explored frontier of SOMs and music is the production of new
music.  Until recent times, the production of music has been solely
by highly trained musicians.  Recently, commericial systems like
Rock Band and Guitar Hero along with many academic research projects
have attempted to put the control of the musical experience in the
hands of less experienced users.  In the past, systems like Dance
Dance Revolution and other academic projects have begun to bridge
the gap between dancing and music creation.

Our current research project proposes to let dancers create their
own music, and generate music collaboratively with other dancers.
We achieve this by placing sensors on each of the dancers that
determine their positions either in two dimensions on a theatre
stage.  We can then translate these stage positions into positions
in a 2D SOM.

\begin{figure}[here]
\includegraphics[width=50mm]{samba-instruments.pdf}
\caption{Some traditional Samba instruments, including the large
  surdo, a snare drum, the small round tambourim, shakers and agogo
  bells.\label{fig:samba-instruments}}
\end{figure}

The SOM that we build contains aligned rhythmic phrases of different
drum patterns from Samba music.  Samba music is built from many
overlayed drum patterns, each one playing on different beats of a
typically 2/4 metrical pattern.  It is a highly syncopated music
style with each instrument, from the large surdo drum, to the small
hand held clave, playing on different beats of the bar.  Although
each pattern in a typical Samba song is simple, the patterns that
result from all the instruments playing together can be quite
complex and interesting.

\section{Related Work}\label{sec:final}

Self organizing maps have been used extensively in the visualization
of data for audio based music information retrieval \cite{cooper06}.
They have been used to analyze and organize music archives
\cite{rauber01a} \cite{rauber98a} \cite{rauber98b} \cite{rauber02a},
and to visualize the resulting music collections \cite{rauber03a}
\cite{pampalk04} \cite{rauber02}.  A particularily relvant study was
that of Palmalk in his paper Islands of Music \cite{pampalk03}.
SOMs have also been used for audio retrieval, browing and
constructivist learning in several papers \cite{cano02}
\cite{fruehwirth01} \cite{honkela00}.  While the previous mentioned
studies concentrated on organizing whole classes of music, SOMs have
also been applied to smaller audio segments, including timbre
\cite{toirvainen97}, energy-spectrum \cite{masugi04}, and musical
time series analysis \cite{capinteiro98}.  

There has been considerable research on the automatic generation of
music, including the generation of background music
\cite{nakamura94} \cite{rui07} \cite{yoo04}, the creation of
rhythmic patterns \cite{labordus83} \cite{labordus83b} \cite{iee77}
\cite{brown05} \cite{ligong05} \cite{kasahara07} and more general
automated music generation systems \cite{erb84} \cite{unemi01}.  An
excellent study was conducted back in 1970 by Howe \cite{howe70}
about compositional considerations when creating electronic
music. 

A particularily relevant study was recently conducted
\cite{kasahara07}, in this paper the authors describe a
self-organizing map system that allows users to create rhythms
co-creatively and interactively.  Also closely related was the
\cite{tzimeas07} SENEgal project, which used genetic algorithms to
create rhythms from western Africa.

Often the previous described systems have their user interaction
paradigm centered on the computer system.  In this study we are
interested in bringing the creation of music into the physical
environment.  The creation of new methods of interacting with the
computer has seen much activity, including projects using the
Radiodrum \cite{benning07} \cite{murdoch06} , SmartSkin
\cite{rekimoto02} and Cyber composer \cite{ip04}.  A particularily
relevant paper involved the use of large numbers of giveaway sensors
in a large rave dance setting \cite{feldmeier07}.  

\section{System Design}\label{sec:system}

The SOMba system is divided into a number of independent components.
These components are linked together in a specific way in SOMba, but
can easily be recombined in other ways, thus facilitating
experimentation and novel applications.  We first generate audio files
from the score and audio samples and create the Self-Organizing Map.
We then track the dancers in the performance space using the Wii
remote and send out OSC messages.  These OSC messages are then
interpreted by the SOMba executable which displays the positions of
the dancers and plays the audio files that correspond to the positions
of the dancers on the Self-Organizing Map.

\subsection{Audio file creation and Self-Organizing Map generation}

Brazilian samba music is made with a large number of performers
playing a variety of percussion instruments.  These instruments
include drums like the surdo and conga, agogo bells, tambourims,
shakers, whistles and other instruments.  Each of these instruments
plays a simple syncopated rhythm, and it is by the combination of
many of these rhythms that complex musics can be produced.

In the SOMba system, we begin by first obtaining authentic Samba
musical scores and transcribing them to text files.  Letters and
spaces in these files correspond to 16th notes, so a string of 16 of
these letters corresponds to one full bar of music.  We take each
instrumental part and transcribe it to a single line of text.

\begin{verbatim}
  "L      LLL     L" bell           
  "   HH      HH   " bell           
  "L     LL      L " bell 2         
  "  H H    H HH   " bell 2         
  "L     LL      L " agogo          
  "   HH    H H    " agogo          
  "    LL     LL L " tambourim      
  "H H    H H      " tambourim      
  "X X  X X X X XX " tambourim 2    
  "X       X       " surdo 1 (low)  
  "    O       O   " surdo 1 (low)  
  "    X       X   " surdo 2 (med)  
  "O       O       " surdo 2 (med)  
  "    O O     OO O" surdo 3 (high) 
  "L   L   L   L   " snare          
  "   H  H   H    H" snare          
  " XX  X X X X XX " snare          
\end{verbatim}

To generate the audio file, we then take a program written using the
Marsyas framework and place appropriate drum samples at the positions
of each letter in the transcribed text file.  In our preliminary
investigations, we found that if we placed the note directly on the
beat, the music that was produced sounded rather mechanical and lacked
feeling.  We therefore added code to allow a small amount of jitter,
or random temporal noise, be introduced in the placement of each
note.  This allowed us to produce multiple audio files for each
transcribed part, each of which had subtle timing variations.

We then took these resulting audio files and used the Marsyas
framework to extract a variety of features from each of these, such as
MFCC coefficients, Zero Crossings, Spectral Centroid and Spectral
Rolloff.

We then took these extracted features and used them to train a
Self-Organizing Map using functionality present in Marsyas.  This SOM
was initialized to random values.  This trained Self-Organizing Map
seperated the audio tracks from different musical instruments onto
different areas of the Self-Organizing Map.  For example, the tracks
with surdo would be placed together and the tracks with tambourim
would be placed in a different part of the map.  Figure
\ref{fig:margrid-som} is an example of a Self-Organizing Map generated
with MarGrid.

\begin{figure}[here]
\includegraphics[width=50mm]{margrid-som.pdf}
\caption{A Self-Organizing Map generated from the Marysas program
  MarGrid.  The darkness of a square correponds to how many audio
  files were mapped to that grid point.\label{fig:margrid-som}}
\end{figure}



\subsection{Position tracking with the Wii remote}

The Wii remote, or wiimote, is a multimodal interface device developed
by Nintendo for use with the Wii game system.  The wiimote has
traditional buttons and rumble functionality, but also contains a
speaker, an 3-dimensional accelerometer, and an Infrared (IR) sensor.
This IR sensor has the ability to track up to 4 independent sources of
infrared light, and reports back the positions and intensities of the
detected points.  All data from the wiimote is sent back to the
computer via Bluetooth.

In this project, we utilize the wiiosc program which is built on the
cwiid library.  wiiosc receives data from the wiimote and sends out
OSC messages for this data.  The OSC messages are sent to a single
port with different OSC addresses corresponding to the type of input
events detected.

We have written a Python OSC translator program (wiiosc-to-somba.py)
that takes the raw OSC messages from wiiosc and post-processes these
messages to a form more suitable for use by the main SOMba program.
This translator also allows for multiple wii-motes to be used at once,
each one returning the positions of four dancers at once.  In the
canonical SOMba system, we use two wii-motes, allowing up to 8 dancers
to be tracked, but this number could easily be expanded.  It is
important to align the sensors of the wiimotes accurately with the
performance space and with each other, so that an individual dancer is
only tracked by one wiimote at once.


\subsection{SOMba}

SOMba is a program written using the Marsyas framework with a
graphical interface provided using the Qt library.  The SOMba
executable takes the generated Self-Organizing Map and displays it on
the screen.  It then takes as input OSC messages from
wiiosc-to-somba.py that correspond to grid locations on the SOM and
places differently colored squares on the grid for each detected IR
source.  When a grid location contains a colored square, SOMba plays
the audio track associated with that grid location.  This interface is
shown in Figure \ref{fig:somba-interface}

\begin{figure}[here]
\includegraphics[width=50mm]{somba-interface.pdf}
\caption{The SOMba interface.  Multiple colored markers indicate the
  positions on the Self-Organizing Map that are currently selected
  from the wiimote OSC data\label{fig:somba-interface}}
\end{figure}


What results is a multitrack audio output of different instrumental
tracks of the audio files.  As each dancer moves around the
performance space, they either show or hide their IR source, when it
is visible to the IR sensors, the track corresponding to their
position in space is played.


\section{Final Words}\label{sec:final}

Dancing and music have long been close companions, and recent advances
in technology can allow us to further blend these together into a new
form of musical and physical articulation.  By allowing dancers to
interact with and create their own music, we hope to create new
exciting opportunities for creative expression.  We are also
interested in the spatial representation of music, and by transferring
Self-Organizing maps from the computer screen into physical space, we
anticipate to discover new ways of interacting with and visualizing
data.  Another part of this research is to use self-organizing maps
not just to organize different songs, but different rhythmic patterns.
Such a project might prove interesting for creation of music using
different interfaces.

\section{Acknowledgments}\label{sec:acknowledgments}

We would like to thank Gabrielle Odowichuk and Manj Benning for
helpful discussions and assistance with the project.

\bibliographystyle{abbrv}
\bibliography{refs}


\end{document}
% Template for NIME 2011
%
% Modified by Alexander Refsum Jensenius on 27 October 2010
%
% Based on "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with "nime2011.cls"
%

\documentclass{nime-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{NIME'11,}{30 May--1 June 2011, Oslo, Norway.}

\title{Metheny and Mistic vs The Robots: coping strategies for misbehaving robots}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven R. Ness\titlenote{}\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Victoria Canada}\\
       \email{sness@sness.net}
% 2nd. author
\alignauthor
Shawn Trail\titlenote{}\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Victoria Canada}\\
       \email{shawn@intrinsicaudiovisual.com}
% 3rd. author
\alignauthor Scott Miller\titlenote{}\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Victoria Canada}\\
       \email{smiller@ece.uvic.ca}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Peter Driessen\\
       \affaddr{Department of Electrical and Computer Engineering}\\
       \affaddr{University of Victoria}\\
       \affaddr{Victoria Canada}\\
       \email{peter@ece.uvic.ca}
% 5th. author
\alignauthor George Tzanetakis\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{Victoria Canada}\\
       \email{gtzan@cs.uvic.ca}
% 6th. author
\alignauthor W. Andrew Schloss\\
       \affaddr{School of Music}\\
       \affaddr{University of Victoria}\\
       \affaddr{Victoria Canada}\\
       \email{aschloss@uvic.ca}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{31 January 2011}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Abstract
\end{abstract}

\keywords{Robot musicians, MISTIC, self-aware robots}

\section{Introduction}

At the University of Victoria we have developed a multi-disciplinary
collaboratory between the departments of Computer Science, Music and
Electrical Engineering called the Music Intelligence and Sound
Technology Interdisciplinary Colliquium (MISTIC).  In this group, we
have been developing a wide range of technologies to understand music,
to create music and to develop robots that can play acoustic instruments.

Recently, the MISTIC group performed two instantiations of the fifth
MISTIC concert, a concert that featured Dr. Andrew Schloss' new piece
``sonicpair'', a duet between Dr. Schloss and Joanna Hood, a
classically trained violinist.  Also performing at this concert was
Trimpin, the father of musical robots, along with Ajay Kapur, Arne
Eigenfeld, Darren Miller and Steven Ness.  In this paper we will
primarily focus on the piece by Dr. Schloss as it is the most
musically mature and has the most applicable musical context to the
current work.

Also recently joining the MISTIC group is a developer and maintainer
of the robotic musicians on Pat Metheny's widely publicized and well
attended Orchestrion tour.  This tour represented one of the first
times that a well known popular musician featured music robots
prominently in their show.

In this paper we will first discuss previous work related to this,
then we provide a broad overview of the system description of both Pat
Metheny and MISTIC concerts, followed by a description of the show,
integrating all types of performance techniques.  We will then compare
and contrast these two similar but quite contrasting examples of the
uses of robotics in music.  The MISTIC group works heavily in the
field of Music Information Retrieval \cite{orio06} and applies these
technologies to the production of music.

A major direction in the MISTIC group is that of self-aware musical
robots.  In performance contexts, calibrating the velocity curves that
control robots to the sonic output of the drums that the robots stike
has always been a challenging endeavour.  One of the primary problems
is that due to the physics of the drum beaters, small changes in the
positioning of the beater and solenoid can produce large changes in
the sound that is produced.  Calibrating an array of multiple
solenoids on different drums is difficult, and this problem is
compounded by the fact that sometimes the beaters or the objects that
are being struck move during a performance.

The system we propose performs a timbre classification of the incoming
audio and in real-time automatically maps solenoids correctly to the
note messages sent to the musically desired drum.  We have implemented
this in the Marsyas \cite{Marsyas} dataflow architecture.  For a
performance, there would be a number of different drums and a series
of modular beaters.  The system would perform timbre analyis of the
audio from a centrally located microphone (or a series of direct
optical pickups) and would be able to calculate which beater to send a
message to make the desired musical sound.  By also listening when
messages were sent to the beater but there was silence, one could
detect broken beaters.  By doing this, a performer would not have to
map or calibrate the solenoids, the system would do it automatically.

In the video on the website http://mistic.uvic.ca/nime2011 you
can see a video of a robot playing a basic repeating rhythm on a set
of frame drums.  After a short period, an assistant switches the two
drums.  The audio and machine learning system then hears this change
in timbre and the system adapts to play the correct drum for the
musical intent.  In this video is shown how the robot re-maps the
controls dynamically to play the desired musical rhythm correctly.

\cite{tindale05}
\cite{tindale04}
\cite{traube03}







- Andy's excerpt on Japanese humanoid robots

- MISTIC Mission statement
	- Non-humanoid
	- DIY
	- Self-aware music robots

- What motivates Andy as a composer and performer
	- Spatial reach that it allows
	- 20 foot arms
	- Extend what humans can do
	- Do things that no human can do
	- Control interfaces - Radiodrum

Buzzwords to include:
	- control interfacing
	- creative
	- original 
	- custom
	- compositionally embedded logic

- continuity as percussive lineage

- radiodrum and other alternate control interfaces
	- control interfaces are vital

- Andy's composition and performance as a constant (?)

- Robots listening to themselves


Although, as we demonstrate, that it is possible to run this system on
a single microphone based system using Audio analysis and Machine
Learning techniques, in some particularly loud surroundings for
example, a music concert, that it may become difficult to correctly
map solenoid activations to sounds.  For this reason we also are
working on a system of optical pickups \cite{overholt05} that would
directly monitor the sonic output of that drum, with no distortion
occuring because of noise in the room.  With this system it would be
possible to very accurately map and calibrate drums.  These optical
pickups would be built into the robotic arm as a single unit.

Less important for the mapping and calibrating, but a very important
concept musically to the MISTIC group is the idea of Localized Sound
Reinforcement (LSR) \cite{eargle04}.  Because many drums, strings and
idophones make sounds too quiet to be heard on stage, it is important
to project their sound through speakers.  Typically, the amplified
sounds from all the drums is projected through two speakers in a
stereo configuration, or more in a multichannel audio system.
However, the spatial non-proximity of the sound and the object
producing the sound invokes a mild form of cognitive dissonance
\cite{festinger57} where the mind tries to reconcile two different
pieces of information.  In LSR, there is a speaker attached to each
drum, and the amplified sound appears to come directly from instrument
itself.  This is still inferior to the very complex sound generated by
the fact that the instrument is a physical object and often can give
off sound off of the entirety of its surface.  However, by placing the
amplification at the visual and primary auditory source of the drum...


- MIR applications to this (Marsyas)

- Acoustic instruments
	
Andy 
	- human traditions
	- extended techniques

- avant garde - no reference to lineage (?)

Just to expand on my previous choice of title. Coping strategies
generalizes nicely to describe various approaches: ignore (if it fails
live with it), mask/hide (trigger samples), redundancy, etc (you
probably can think of some others).

We can then motivate self-awareness/embodiment/proprioception
as an unexplored coping strategy and maybe provide a proof of concept
experiment.	

\section{Related Work}

- Music robots

\cite{kapur05}

- Previous work by others

- Previous work by us

\section{Description}

\subsection{System Description}


- Pat Metheny setup
- MISTIC concert setup

In this section we will discuss just the briefest overview of Pat
Metheny's setup and the MISTIC setup.  The Pat Metheny setup was
extremely large and complicated, with over 100 bots and 100 inputs on
board.  

Metheny setup

- 100 bots
- 100 inputs on board
- 2 hour concert
- 8-10 pieces
- different control interfaces (parameters, triggers)
- 3000-4000 people at each concret
- just pat and the orchestrion
- solenoids, motors and pneumatics
- drums, zylophones, wind, string, idiophones
- Eric Singer from LEMUR built robots
- Rag West
- Jazz guitarist
- Pat's pedals
- Moog
- Control scenarios
- Generalize sound situation


	
%% - Use of NotomotoN (Kapur reference)
%% 	- Single drum with a number of beaters on the head to allow for
%% 		 rolls, flams and other such techniques
%% 	- Multiple kinds of solenoids for doing different kinds of
%% 		things.  Some are fast, some can hit hard
%% 	- Tuning the performance to the type of solenoid
%% 	- Drum has a USB plug and appears as a device in Max
%% 	- We found this sonically and artistically limiting so we removed
%% 		the solenoids and put them on mic stands and hit frame drums

%%- contrast with MISTIC concert

In the MISTIC concert, we were performing the fifth in a series of
concerts that our group puts on to perform for the public a series of
artistic compositions that have been created by members of the group.
It features Dr. Andrew Schloss, a pioneer of performing using the
radiodrum \cite{schloss89} as a controller.  This year he performed
``sonicpair'' a work for radiodrum, viola, robots and microphone
amplification systems.  For this work, he created a piece in which,
using the radiodrum as controller and doth controlled robotically
activated frame drums, and also manipulated audio from the microphone
of the viola player, Fiona Hood.  On the viola, a variety of extended
techniques were used, including rubbing the bow gently on the strings
to different plucking and scratching techniques.  The robots were
controlled via a Max/MSP \cite{puckette02} patch that took input from
the radiodrum and an octave of foot pedals and sent OSC \cite{osc}
messages to the drum controllers that were controlled by the new
NotomotoN system developed by Dr. Ajay Kapur at CalArts \cite{ajay}.
These drums had a series of solenoids attached to them, with four
solenoids of differing design attached each head.  For the piece that
Andy composed, he needed to have a wider sonic pallette and removed
several of the solenoids and attached them onto microphone stands
which he then placed near to frame drums.  Calibrating this setup was
time consuming and difficult for both of the performances, but more so
for the first performance in Vancouver.  In this setup, the drums were
not amplified...  (more...)


- How to mic the setup
	- Differences and similarities

%% sness : Hmm, I have no idea about this, please add text here about how
%% the Pat Metheny show was miced and how our setup was miced.

- DIY aspects

- control scenarios
- sound situation
	- what worked, what didn't




From Metheny video
- player piano at grandparents house
- future and ancient
- reference Conlon Noncarrow and Georges Antheil
- play guitar with feet solenoids
- 17 time grammy winning jazz guitarist 
	- commercial aspect on pat's side
- nightly setup and teardown
	- rigourous of a travelling roadshow
- instruments get instructions from a variety of sources
- one to one mapping
	- what notes he plays get played directly on any instrument
- created very specific universe for very specific musical pieces
- reference video on youtube
- composite details on the fly
- overdub with yourself - fingerprint with fingerprint a perfect match 
- illusion of movement in time and space
	- a kind of swing specific to his taste.

%% sness : It would be great to turn this into a story.  Please do.


\subsection{Performance Description}

- Performance 
	- Metheny - Art and music are primary
	- MISTIC - Art and music, but also academic electro-acoustic music
	  with a long lineage.  
	- side note : Some performances were informed by
		 the MIR research we are doing.
	
- Website with videos and music - Talk about the importance of videos
and audio to really see what is going on, sonically, artistically,
visually and technically

- Concert
- What went down
- level of maturity of piece (highlighting Andy's piece)
	- applicable musical context
	- longest original control interface for robots (radiodrum)
	- Trimpin - we will not treat this part of the performance in this
	   paper, quite different from the main thrust and philosophy of
	   MISTIC
	- Also we will not go into great detail into the other
       performances (Ajay, Darren, Arne, Steven)

- Lots of pictures
	- pedals
	- pats setup
	- radiodrum
	- notomoton
	- andy with radiodrum


Presentation
- Pat's more theatrical
- outline and explain in depth (some depth) pats control situations
- how the two control situations (MISTIC vs Pat) were quite different
	- radiodrum

- Compare and contrast sound systems
- What worked, what didn't

- Solenoids
	- why we need to look other places for robotic acoustic sound
		 generation
	- ajay's old bots - make a click, this is part of the aesthetic
	- velocity range
	- motors
	- percussive
	- types of robots
		- calimba model
		- string model
	- direct signal that both offer (optical pickups)
	- mic as an acoustic transient (?)


Most of the robots in both the Metheny and MISTIC concerts use
solenoids to produce sound through striking an object.  Although
diverse and useful\cite{kapur07}, solenoids have a variety of
advantages but also have some drawbacks as well.  Many larger or older
solenoids produce a click when activated.  Some performers use this to
great artistic effect \cite{kapur06}, however it is still a fact that
must be attended to.



- how to move forward
- optical pickups
- what pitch is being played by string instruments, self aware string
	playing robots
- MIR applied to this
	- Marsyas as a realtime system for developing self-aware music
	 robots
- in pat's setup
	- lowest and highest points on a string
	- can't ever really get it in tune
	- it should interpolate the sounds itself and tune itself
- listen to each note as it is playing and dynamically adapt itself
- string
- distance of solenoid in drums
	- problems that this caused in performance, both for pat's setup
	 and for our setup
	- occurred in practice in both concerts/contexts
- marimba bars would bend
	- preset velocity
	- wouldn't execute the notes
	- preprogrammed velocity curve no longer relevant
	- bar in different relation to mallet


	
	



\section{Comparison}

- How did Pat do it, how did we do it	
	- big budget / smaller budget
	- lots of time for development / not much time for development
	- thousands of people at concert / 100-200 people for concert	
	- large concert halls / small art gallery and academic recital
	  	hall
	- robot drums were amplified / robot drums were not amplified
	- robot drums all on stage / robot drums in the audience as well
		as on stage
	- theatrical aspects / primarily sonic aspects
	

\section{Experimental}

For our experiments, we built a system that did an utomatic mapping
based on timbre classification. This was straightforward to do using
existing Marsyas feature extraction and classification modules.
These modules allow complex features to be calculated based on input
audio, some of these include Fast Fourier Transform (FFT),
Mel-Frequency Cepstral Coefficients (MFCC), Correlogram based
approaches \cite{slaney93}

Basically you
have some number of different drums and a set of modular beaters. The
goal is for the system to know which beater is beating which drum
(including a silent drum for broken beaters) without having to
explicitly specify the mapping.

2) Automatic velocity calibration. Given a desired velocity acoustic
response that can be obtained either by the actual robot or by
a human performer re-adjust the current velocity response to match it.
This could be done with a simple regression from energy or loudness
or could incorporate multiple features to include timbral effects.

CRITICAL: For the experimental setup we will need a minimum of
two beaters and two drums for the first experiment. For the second
ideally we would like something that has a good dynamic range.

	



\section{Conclusions}

\section{Future Work}

\section{Acknowledgments}














%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{nime2011gtzan}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%% \appendix
%% %Appendix A
%% \section{Headings in Appendices}
%% The rules about hierarchical headings discussed above for
%% the body of the article are different in the appendices.
%% In the \textbf{appendix} environment, the command
%% \textbf{section} is used to
%% indicate the start of each Appendix, with alphabetic order
%% designation (i.e. the first is A, the second B, etc.) and
%% a title (if you include one).  So, if you need
%% hierarchical structure
%% \textit{within} an Appendix, start with \textbf{subsection} as the
%% highest level. Here is an outline of the body of this
%% document in Appendix-appropriate form:
%% \subsection{Introduction}
%% \subsection{The Body of the Paper}
%% \subsubsection{Type Changes and  Special Characters}
%% \subsubsection{Math Equations}
%% \paragraph{Inline (In-text) Equations}
%% \paragraph{Display Equations}
%% \subsubsection{Citations}
%% \subsubsection{Tables}
%% \subsubsection{Figures}
%% \subsubsection{Theorem-like Constructs}
%% \subsubsection*{A Caveat for the \TeX\ Expert}
%% \subsection{Conclusions}
%% \subsection{Acknowledgments}
%% \subsection{Additional Authors}
%% This section is inserted by \LaTeX; you do not insert it.
%% You just add the names and information in the
%% \texttt{{\char'134}additionalauthors} command at the start
%% of the document.
%% \subsection{References}
%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.

%% % This next section command marks the start of
%% % Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.

%% %%% Place this command where you want to balance the columns on the last page. 
%% %\balancecolumns 

% That's all folks!
\end{document}
% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage{url} 
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ACM International Conference on Multimedia}{2009 Beijing, China}
%\CopyrightYear{2007} % Allows default copyright year (200X) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Symphony of the Seasons - Multichannel Audilization of Phenological Data}

%% \subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Steven R. Ness \\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \affaddr{PO Box 3055, STN CSC}\\
       \affaddr{Victoria, BC, CANADA}\\
       \email{sness@sness.net}
% % 2nd. author
% \alignauthor
% Anthony Theocharis \\
%        \affaddr{Department of Computer Science}\\
%        \affaddr{University of Victoria}\\
%        \affaddr{PO Box 3055, STN CSC}\\
%        \affaddr{Victoria, BC, CANADA}\\
%        \email{sness@sness.net}
% % 3rd. author
% \and
% \alignauthor
% L. Gustavo Martins\\
%        \affaddr{CITAR (Research Center for Science And Technology in Art)}\\
%        \affaddr{Rua Diogo Botelho, no 1327}\\
%        \affaddr{Porto, Portugal}\\
%        \email{lgustavomartins@gmail.com}
% 2nd. author
\alignauthor
George Tzanetakis \\
       \affaddr{University of Victoria}\\
       \affaddr{PO Box 3055, STN CSC}\\
       \affaddr{Victoria, BC, CANADA}\\
       \email{gtzan@cs.uvic.ca}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{27 Apr 2009}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}


\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.1}{Information Storage and Retrieval}{Content Analysis
  and Indexing Methods}

\terms{Algorithms, Theory, Experimentation}

\keywords{sound analysis, music information retrieval, tags,
  folksonomies, music recommendation}

\section{Introduction}
\cite{TC02b}

\section{Conclusions} 


\bibliographystyle{abbrv}
\bibliography{osphen2009sness}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
% THIS IS SIGPROC-SP.TEX - VERSION 3.0
% WORKS WITH V3.1SP OF ACM_PROC_ARTICLE-SP.CLS
% JUNE 2007

%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}
\usepackage{subfigure}
\usepackage{url} 

\begin{document}

\title{Assistive Music Browsing using Self-Organizing Maps}
\permission{Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
PETRA'09, June 09-13, 2009, Corfu, Greece.}
\copyrightetc{Copyright 2009 ACM ISBN 978-1-60558-409-6...\$5.00}

\numberofauthors{4}\author{
\alignauthor
George Tzanetakis\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{gtzan@cs.uvic.ca}
\alignauthor
Manjinder Singh Benning \\
       \affaddr{CANASSIST}\\
       \affaddr{University of Victoria}\\
       \email{manjb@uvic.ca}
\and
\alignauthor
Steven R. Ness \\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{sness@sness.net}
\alignauthor
Darren Minifie \\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Victoria}\\
       \email{minofifa@uvic.ca}
\alignauthor
Nigel Livingston \\
       \affaddr{CANASSIST}\\
       \affaddr{University of Victoria}\\
       \email{njl@uvic.ca}
}

\date{10 January 2009}

\maketitle
\begin{abstract}
  Music listening is an important activity for many people. Advances
  in technology have made possible the creation of music collections
  with thousands of songs in portable music players. Navigating these
  large music collections is challenging especially for users with
  vision and/or motion disabilities. In this paper we describe our
  current efforts to build effective music browsing interfaces for
  people with disabilities. The foundation of our approach is the
  automatic extraction of features for describing musical content and
  the use of self-organizing maps to create two-dimensional
  representations of music collections. The ultimate goal is effective
  browsing without using any meta-data. We also describe different
  control interfaces to the system: a regular desktop application, an
  iPhone implementation, an eye tracker, and a smart room interface
  based on Wii-mote tracking.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{J.3}{Computer Applications}{Health}

\terms{Human Factors}

\keywords{sound analysis, music information retrieval, content-aware
  user interfaces, assistive technologies, eye tracking}

\section{Introduction}

Listening to music is a common everyday activity for many
people. Recently the widespread availability of portable digital
music players and digital music distribution have made possible 
to carry around increasingly large personal music collections
(currently 5-10 thousand tracks is a common size). As these
collections keep getting larger navigating them becomes increasingly
complex. Portable players and music track management software
typically allow users to select artists, genres or individual tracks 
essentially by browsing long lists of text. This mode of interaction
although adequate for small music collections becomes increasingly
problematic as the collections become larger. The emerging area of Music
Information Retrieval (MIR) \cite{orio06} deals with all aspects of managing,
analyzing and organizing music in digital formats. In the past ten
years several MIR algorithms and user interfaces have been proposed
that can assist with the browsing and navigation of large music
collections. 

Music browsing can be especially challenging for people with special
needs. For example, finding a particular artist out of a long 
list of text using a scroll-wheel can be very difficult or impossible for a
user with motor disabilities. Similarly reading text on a screen is
not directly possible for a blind user. Various types of assistive
technologies such as single-switch access, voice recognition and text
synthesis which can be used for general computer usage, can also be
applied in the case of music browsing. At the same time we believe
that music browsing and listening are activities with unique and distinct
characteristics that motivate the creation of domain specific software
and control interfaces. Such tools have the potential to provide a much
more effective and enjoyable experience for people with special
needs. In this paper we report on our efforts to design such software
and control interfaces for assistive music browsing.

The foundation of the system is a content-aware grid surface 
representation of a music collection that is calculated using
Self-Organized Maps \cite{kohonen95a} over automatically extracted features that
describe musical content. We have experimented with several control
interfaces ranging from simple button navigation to location sensors
based on the Wii-mote and eye tracking. This flexibility is crucial as
there is a broad range of visual and motor disabilities that could
potentially benefit from the proposed system. Each individual user
with special needs has unique accessibility constraints so using the
different control interfaces allows us to personalize the basic
system. This personalization as well as the time availability of
users makes it harder to conduct detailed quantitative user
evaluations common in human-computer interaction. Instead we follow a
more qualitative approach based on questionnaires and participatory
design for guidance. 

The context of this work has been {\it CanAssist}
\footnote{\url{http://www.canassist.ca}} which is a non-profit
organization at the University of Victoria that specializes in the
research and development of assistive technology as well as the
development of community based initiatives to make life easier and
more rewarding for those with disabilities. The technical group
consists of professors, full-time engineers, graduate students on both
the Masters and Phd level, and short-term student internships from a
broad range of disciplines including electrical and mechanical
engineering, computer science, music and fine arts, psychology,
kinesiology, and neuroscience and medicine.



\section{Related work} 

The two main research areas that have informed our work are assistive
technologies and music information retrieval especially content-aware
user interfaces for music browsing. There is a growing interest in
human-centric computing which is a field where computers and devices
are designed to adapt to the user need and preferences. Assistive
technologies hold the potential to make life easier and more rewarding
for people with special needs. Representative examples that have
influenced our work include: smart environments for health care
monitoring applications \cite{doukas, bostelman}, visual tracking of
body features for computer access for people with severe disabilities
\cite{betke}, and non-visual interfaces for blind users
\cite{savidis}.


Initial work in Music Information Retrieval (MIR) concentrated on
algorithmic devopment rather than interactive systems. However in
recent years, there has been a steady increase in the number of
interfaces available for MIR. Self-organizing maps have been used in
Islands of Music \cite{RPM02} as well as the Databionic visualization
\cite{MUN05}. Another interesting interface is Musicream \cite{GG05}
which is a new music playback interface for streaming, sticking,
sorting and recalling musical pieces. The idea of personalization is
explored in \cite{VP05} where a music retrieval system based on
user-driven similarity is described. A tangible interface for browsing
music collection using a table metaphor is described in
\cite{stavness05}. Another tangible interface is {\it
  MusicBottles}, designed by Hiroshi Ishii and his team at the MIT
Media Lab. In this work, bottles can be opened and closed to explore a
music database of classical, jazz, and techno music \cite{ishii97}.
Music collection browsing on mobile devices has also been explored
\cite{Wattenhofer}. An earlier version of the proposed SOM-based music
browsing system with focus on DJ interaction using a Radio Drum
control interface can be found in Murdoch \cite{murdoch06}. The system
described in this paper draws ideas from these systems but is
differentiated by the diversity of control interfaces and the focus on
accessibility.



\section{System Overview}

The foundation of the proposed assistive music browsing system 
is a content-aware visual representation of a music collection. 
Automatically extracted audio features based on state-of-the-art 
audio signal processing techniques are used to represented musical
content. A Self-Organizing Map (SOM) is used to map these
high-dimensional audio feature vectors to 2D coordinates in a discrete
rectangular grid. More details are provided in the following
subsections. 


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{featureExtraction}
  \caption{\it Feature extraction and texture window}
  \label{fig:featExtract}
\end{figure}




\subsection{Music Content Feature Extraction} 

The goal of this stage is to represent each song in a music collection
as a single vector of features that characterize musical
content. Using suitable features, songs that ``sound'' similar should
have vectors that are ``close'' in the high dimensional feature space.
The features used are the Spectral Centroid, Rolloff, Flux and the
Mel-Frequency Cepstral Coefficients (MFCC). To capture the feature
dynamics we compute a running mean and standard deviation over the
past M frames: 

\begin{eqnarray}
  m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
  s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)]
\end{eqnarray}

\noindent 
where $\Phi(t)$ is the original feature vector. Notice that the
dynamics features are computed at the same rate as the original
feature vector but depend on the past M frames (40 in our case
corresponding to approximately a so called ``texture window'' of 1
second).  This results in a feature vector of 32 dimensions at the
same rate as the original 16-dimensional feature vector. This process
is illustrated in Figure ~\ref{fig:featExtract}. The sequence of
feature vectors is collapsed into a single feature vector representing
the entire audio clip by taking again the mean and standard deviation
across the 30 seconds (of the sequence of dynamics features) resulting
in the final 64-dimensional feature vector per audio clip. A more
detailed description of the features and their motivation can be found
in Tzanetakis and Cook \cite{TC02b}. For the calculation of the
self-organizing map described in the next section all features are
normalized so that the minimum of each feature across the music
collection is 0 and the maximum value is 1. This feature set has
shown state-of-the-art performance in audio retrieval and
classification tasks. \footnote{\url{http://www.music-ir.org/mirex/2008}} 


\begin{figure*}[t]
\centering
\caption{Topological mapping of musical content by the Self-Organizing
  Map}
\subfigure[Classical]
{
    \label{fig:sub:classical}
    \includegraphics[width=3cm]{classical.pdf}
}
\hspace{1cm}
\subfigure[Metal] 
{
    \label{fig:sub:metal}
    \includegraphics[width=3cm]{metal.pdf}
}
\hspace{1cm}
\subfigure[Hiphop]
{
    \label{fig:sub:hiphop}
    \includegraphics[width=3cm]{hiphop.pdf}
}
\hspace{1cm}
\subfigure[Rock]
{
    \label{fig:sub:rock}
    \includegraphics[width=3cm]{rock.pdf}
}

\label{fig:sub}
\end{figure*}


\begin{figure*}[t]
\centering
\subfigure[Bob Marley]
{
    \label{fig:sub1:marley}
    \includegraphics[width=3cm]{marley.pdf}
}
\hspace{1cm}
\subfigure[Radiohead] 
{
    \label{fig:sub1:radiohead}
    \includegraphics[width=3cm]{radiohead.pdf}
}
\hspace{1cm}
\subfigure[Led Zeppelin]
{
    \label{fig:sub1:zeppelin}
    \includegraphics[width=3cm]{zeppelin.pdf}
}
\hspace{1cm}
\subfigure[Dexter Gordon]
{
    \label{fig:sub1:dexter}
    \includegraphics[width=3cm]{dexter.pdf}
}
\label{fig:sub1}
\end{figure*}


\subsection{Self-Organizing Maps} 

The self-organizing map (SOM) is a type of neural network used to map
a high dimensional input feature space to a lower dimensional
representation while preserving the topology of the high dimensional
feature space. This facilitates both similarity quantization and
visualization simultaneously. The SOM was first documented in 1982 by
T. Kohonen, and since then, it has been applied to a wide variety of
diverse clustering tasks \cite{kohonen95a}. In our system the SOM is
used to map the audio features (64-dimensions) to two discrete
coordinates on a rectangular grid.

The traditional SOM consists of a 2D grid of neural nodes each
containing an $n$-dimensional vector, $ {\bf x(t)} $ of data. The goal
of learning in the SOM is to cause different neighbouring parts of the
network to respond similarly to certain input patterns. This is partly
motivated by how visual, auditory and other sensory information is
handled in separate parts of the cerebral cortex in the human brain.

The network must be fed a large number of example vectors that
represent, as closely as possible, the kinds of vectors expected
during mapping. The examples are usually applied several times. The
data associated with each node is initialized to small random values
before training. During training, a series of $n$-dimensional vectors
of sample data are added to the map.  The ``winning'' node of the map
known as the {\it best matching unit} (BMU) is found by computing the
distance between the added training vector and each of the nodes in
the SOM. This distance is calculated according to some pre-defined
distance metric which in our case is the standard Euclidean distance
on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding
nodes reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. 


The update formula for a neuron with representative vector N(t) can be
written as follows:
\begin{equation}
    {\bf N}(t + 1) = {\bf N}(t) + \Theta(v,t) \alpha(t) ({\bf x}(t) -
    {\bf N}(t))
\end{equation}
where $\alpha(t)$ is a monotonically decreasing learning coefficient
and $x(t)$ is the input vector. The neighborhood function
$\Theta(v,t)$ depends on the lattice distance between the BMU and
neuron v. We utilize a Gaussian neighborhood function that shrinks
over time. The time-varying learning rate and neighborhood function
allow the SOM to gradually converge and form clusters at different
granularities. In our implementation, $\alpha(t)$ is a linearly-decaying
function with $t$. Once a SOM has been trained, data may be added to
the map simply by locating the node whose data is most similar to that
of the presented sample, ie. the winner.  The reorganization phase is
omitted when the SOM is not in the training mode. Another interesting
property of SOMs for our application is that they can be personalized
by user initialization rather than random initialization.

Figure ~\ref{fig:sub1} illustrates the ability of
the extracted musical content-features and the SOM to represent
musical content. The top subfigures (a), (b), (c) and (d) show how
different musical genres are mapped to different regions of the SOM
grid (the black squares are the ones containing one or more songs from
each specific genre). As can be seen Classical, Heavy Metal and HipHop
are well-localized and distinct whereas Rock is more spread out
reflecting its wide diversity. The SOM is trained on a collection of
1000 songs spanning 10 genres. The bottom subfigures (e), (f), (g),
(h) show how different artists are mapped to different regions of the
SOM grid. The SOM in this case is trained on a diverse personal
collection of 3000 songs spanning many artists and genres. It is
important to note that in all these cases the only information used is
the automatically analyzed actual audio signal and the locations of the genres/artists are
emergent properties of the SOM.  
If greater grid granuality is desired by a user, this system can also
be used in parallel with an automatic genre classification system to
reduce the total number of songs.



\begin{figure}[htb]
\includegraphics[width=80mm]{system-overview}
\caption{An overview of the system}
\label{fig:system-overview} 
\end{figure} 


\section{Control Interfaces} 

The goal of our system is to provide flexible, personalized ways of
music browsing for users with special needs. To this effect we have
explored several different control interfaces to the same underlying
content-aware representation for music collections. Figure
~\ref{fig:system-overview} shows the overall architecture of our
approach. The Marsyas \footnote{\url{http://marsyas.sourceforge.net}}
audio processing software framework has been used for the audio
feature extraction, calculation of the SOM, the graphical user
interface and handling of controller data \cite{Marsyas}. The basic
system configuration utilizes the standard display/keyboard/mouse
interaction familiar to most computer users. Although familiar this
desktop metaphor is not suitable in many situations especially for
people with visual or physical impairments. In the following
subsections we describe alternative control interfaces that can be
used for that purpose.


\begin{figure}[htb]
\includegraphics[width=60mm]{iphone}
\caption{iPhone control interface}
\label{fig:iPhone} 
\end{figure} 


\subsection{Mobile phone touch screen} 

New generations of mobile devices from Apple, Blackberry, Nokia and
HTC provide a touch-based user interface instead of the traditional
physical keyboard interface. While still in its infancy, touch technology
has already refined the way users interact with a mobile device. Apple
has patented multi-touch technology, whereby the user may use multiple
fingers to control a device with complex gestures. Yet to be addressed
is the decoupling of touch input from visual onscreen objects. As an
example, the iPhone presents audio playback controls to the user as a
series of icons. The user can then touch the icon to perform the
associated action. This approach is inhibitive to the visually
disabled because they may not be able to see the graphical controls in
the first place. A better approach is to rely on spatial touch, where
the user can feel areas of the touch display and associate then with
the various controls. 

Many portable devices include accelerometers that can sense movement
of the device in three dimensions. Many games on the iPhone take
advantage of this by allowing the user to control the game through
tilting, rotating and shaking the device. Similar user actions can be
defined to control various aspects of music navigation and playback. An
important advantage of using such mobile devices compared to
custom-made controllers is that they can integrate seamlessly into the
users day to day routine. 

We have developed a music browsing prototype using the iPhone mobile
device. The main window for the music browser is a grid that covers
most of the screen. Each square on the grid represents a SOM node that
holds a group of songs. When the user touches a square on the grid, a
random song from that node begins to play. As the user moves her finger
across the various squares, songs from each corresponding node
cross-fade with each other to help her navigate the music collection by
hearing how the songs in each grid location are changing. By laying
out a music collection in this spatial fashion, navigation of the
collection with only the knowledge of a few reference points is
needed. For example, if it is known that Rock music is in the upper
left corner, and Jazz music is in the lower left corner, by dragging
from top to bottom along the left edge of the grid, Rock music will
slowly transition into Jazz music. The use of multi-touch (two or more
fingers) may also be used to control playback. Swiping the surface
with two fingers in the right direction skips to another song in the
same node while swiping left plays the previous song in that node.

We believe that user input via touch, rather than a traditional mouse
or keypad, raises the level of usability and interaction between the
user and their music collection. The large surface allows us to
provide a tactile and spacial sensation, more appropriate for
accessibility than a visual based, point-and-click interface. Figure 
~\ref{fig:iPhone} shows the interface running on the iPhone. 


\subsection{Location tracking using the Wii-mote} 

Recently Nintendo released a new game console with a novel controller
called the Wii remote control, or Wii-mote.  The Wii-mote has buttons
like a standard game controller, but also has a 3D accelerometer and
an infrared sensor.  The combination of these two new input modalities
has allowed game developers to create engaging new gaming experiences.

For this project we are using the Wii-mote in two different
modalities.  In the first of these, we use the infrared sensor on the
Wii-mote to track a single infrared source above the display device.
We then map the location where the user is pointing the Wii-mote to a
location on the Self-Organizing Map containing the music collection.
We also map the accelerometer of the Wii-mote to speed up and slow
down the music and to apply different effects to the music.  Buttons
on the Wii-mote allow the user to pause playback and change the volume
of the music track.

In the second modality, we again use the infrared sensor on the
Wii-mote, but instead of having the Wii-mote move while the infrared
source is stationary, we keep the Wii-mote in one position and move
the infrared light sources.  Specifically, we mount the Wii-mote on
the ceiling and attach infrared light sources, in this case simple
incandescent bulbs, to wheel chairs.  Up to four infrared sources can
be independently tracked by one Wii-mote, which means up to four
motion impaired persons can be tracked as they move around a space. 
the infrared light sources.  A simple example of this is shown in
Figure ~\ref{fig:somba} where the infrared sources are tea lights
that can be moved on a surface at which the Wii-mote is pointing.  The positions
of the lights are tracked in real time and their data is sent to the
self-organizing map interface.  Although there are more advanced
position tracking systems available, the Wii-mote has the advantage
that it is a ubquitous, low-cost device, and allows for quick
prototyping of interaction scenarios.

In this case, instead of full songs being mapped in each grid
coordinate, individual instruments playing different parts of the same
song are mapped to each grid coordinate.  For example in our canonical
test case of Brazilian Samba music, one grid coordinate would contain
the rhythm of the surdo drum, and other would have agogo bells,
tambourim, shaker and cowbell.  As the participants move around the
map, each one activates a different rhythm.  Together they can
co-creatively generate music.  

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{somba}
  \caption{\it Wii-mote interface}
  \label{fig:somba}
\end{figure}

This system is particularly well suited for participants without fine
motor control skills.  The only requirement for this system is that
they can locomote around in a room.  We envision that this system
could be used by a wide variety of users, including handicapped
children and adults, and could also be employed in a Senior Citizens
home as a way for multiple participants to create music together.



\begin{figure}[t]
  \includegraphics[width=\columnwidth]{eye}
  \caption{\it Eye snapshot with gaze detection}
  \label{fig:eye}
\end{figure}



\subsection{Eye tracker} 

The CanAssist eye tracking system performs non-intrusive, accurate
point of gaze (POG) estimation independent of head movements.  The
system uses the bright-eye, corneal reflection, image differencing
approach with infrared sources to accurately track the POG of a user
\cite{Ebisawa}. To achieve the level of accuracy required for the application
of music browsing, a calibration is required. The user is instructed
to follow a dot that moves and waits at four locations on the computer
screen. The dot will not advance to the next location unless the
user's sampled gaze points are within a certain standard
deviation. This is to ensure that a satisfactory calibration is
performed.

Figure ~\ref{fig:eye} shows the pupil of the eye being tracked with a
cross marking the centre. The plus sign marks the centre of the
reflected white corneal glint. The square around the pupil show the
region of interest that is searched to find the corneal glint. A
position vector defined between cross and the plus sign is used as
input to the mapping algortihm to estimate the point of gaze.


\section{Qualitative User Evaluation} 

Evaluation of browsing interfaces is challenging and becomes even more
so in the case of assistive music browsing. The fact that each person
with special needs has distinct constraints and abilities makes it
impossible to calculate statistical results over multiple
users. Therefore rather than following a quantitative evaluation
methodology we have conducted a qualitative questionnaire-based
evaluation and included disabled users in the design process. We have
been fortunate to receive useful feedback from two users with special
needs: one with cerebral palsy who used his lips to press keyboard
keys for navigation and one with limited vision who relied mostly on
touch and hearing. In addition an able-bodied user was also asked for
feedback.

There were 5 control interface configurations although not all of them
were tested by all users: desktop computer/mouse, desktop
computer/keyboard, eye tracker, Wii-mote and iPhone. The users were
asked to perform two sets of tasks repeated over two phases followed
by a questionnaire. For all the tasks a collection of 1000 songs was
used. Task 1 requested users to find a song from a specific genre
(Classical, Heavy Metal, Hip-Hop, and Jazz).  Task 2 asked users to
find a song in the collection that was similar to one that was
played. The played test songs were sampled from the same collection
that was mapped to the browser. The user was asked to repeat tasks 1
and 2 over two phases to better understand how learning of the search
space may affect browsing.

In task 1, the tester decided when the correct genre had been found,
whereas in task 2 the user decided when they had found a similar
song. This was done since it is easy to make broad distinctions
between our relatively separate chosen genre classes, using the
filenames in our data set as a baseline, and much more difficult to
agree upon intra-genre similarity between songs. In task 1, the user
was instructed to stop if they had found a song from the correct
genre. If a song in the correct genre had not been found, the user was
asked to continue searching the space. In task 2 the user decided when
a similar song had been found regardless of any baseline metric.

The following questions were asked after the two phases of tasks.
\begin{enumerate}
\item{How easy was it to navigate the music collection looking for a specific genre?} 
\item{How easy was it to navigate the music collection looking for a similar song?} 
\item{Comment on the difference in your performance between the 2 phases of the tasks}
\item{Would you want to use this system on a regular basis? }
\item{Other comments}
\end{enumerate}

To better present the outcomes of our informal user study we
structured by specific user/ability. Video demonstrations of some of
the control interfaces have been also recorded: keyboard
\footnote{\url{http://www.youtube.com/watch?v=DT_mI9Kgtvg } }, eye
gaze \footnote{\url{http://www.youtube.com/watch?v=8eqJ9ZDY5NY}},
iPhone \footnote{{\url{http://www.youtube.com/watch?v=ZzrvAt1OtiY}}}
and Wii-mote
\footnote{\url{http://www.youtube.com/watch?v=fhm9QhrYW1U}}.






\subsection{iPhone and Mouse for a visually impaired user} 

The first case study was a user with limited vision. He utilized the
iPhone as well as the desktop/mouse control interface. In both cases
he relied on tactile and audio feedback rather than visual
feedback. For the mouse configuration tasks were attempted by 
navigated to the corners and then used the auditory feedback to guide
his search. The search became more efficient as he learned the mapping
of genres to the space in the second phase. He found it quite simple
to complete the genre task and relatively simple to find similar songs
although it did take a longer time. Since he is very familiar with the
accessibility features of iTunes on his laptop he would prefer to
continue using that. However on a unfamiliar system he thought that
the proposed interface would be useful due to its intuitive usage and
thought that it would complement iTunes nicely. 

Using the iPhone it was easier to orient to the map by using the
physical edges of the touch screen. This resulted in faster task
completion times on average. He also felt strongly that the proposed
system would be more effective for users with limited vision on a
personal mobile device with a touch screen.

\subsection{Keyboard control on desktop for a motion impaired user
  with cerebral palsy} 

In this case the user has cerebral palsy which precludes using his
hands for computer usage. He has been trained to use his lips to press
keyboard keys. He is fully cognitively and visually abled. In order to
navigate the grid that was displayed on a regular desktop monitor the
{\it q,w,a,s} keys were used to go to the top left, top right, bottom
left, bottom right corners of the grid respectively. The arrow keys
were used for movement and the {\it x} key was used for
centering. Similarly to the previous visually impaired user this
participant also initiated each task by going to the corners of the
space to orient. Subsequently he used the arrow keys and visual and
audio feedback to complete the task. He found the genre tasks very
easy to complete, however did have some problems mostly due to lack of
knowledge of a particular genre. He easily completed the similarity
tasks in one case even finding the exact same song that was being
searched for. After learning how the map was organized, the second
phase was easier to complete. He commented that he would like this to
be integrated into his mobile powered wheel-chair head controls and
would also like to have it as part of his personal computer.

\subsection{Eye tracker for an able-bodied user} 

Unfortunately we were not able to recruit a special needs participants
for testing the eye tracker and Wii-mote location tracking. To
simulate a ``locked in'' participant for the eye tracking experiment
the user was only allowed to move his eyes. Contrary to the other
control interface the user did not use the corners for orientation but 
rather rapidly moved around the space using eye gaze. The user quickly
learned the organization of genres upon the space enabling him to
efficiently complete the genre and similarity tasks. However during
phase II he began to feel eye fatigue since he was not familiar with
using eye gaze as a control method. He made the interesting
observation that he found it easier to focus his gaze on a particular
region when the music was more soft and soothing. Given that he is
able to interact in other ways it was hard for him to come to definite
conclusions but he did feel that it would be an effective way of
interacting for a ``locked in'' person. 



\section{Conclusions and Future Work}

Browsing of digital music collections is an important activity that is
part of daily life for many users. By providing a content-aware
representation based on Self-Organizing Maps coupled with personalized
control interfaces it is possible to enhance the experience of music
browsing and listening for users with special needs such as visual and
motor disabilities. We have described the implementation of the systems and
initial feedback we received from a small qualitative user study. 

There is much future work to be done. We hope to conduct a user study
possibly involving multiple concurrent users of the location browsing
interface using the Wii-mote control. More work needs to be done in
the initialization of the SOM for personalizing the process to
particular music collections. We are also planning on implementing the
system on other mobile devices such as BlackBerries and the Google
Android platform. Experiments with other clustering and dimensionality
reduction methods could be performed.  A critical part of the
development process will be to incorporate user feedback from users
with a range of disabilities.  The nature and range of ability of
disabled users varies widely, and it will be important to take into
account the different needs and abilities of users when planning
future developments to this project.  Regular usage by users with
special needs in their everyday environment will help us get more
reliable information for how to improve the system. Some of the
suggestions we have received and are working on include: integration
with existing music browsing software like iTunes, incorporation of
speech recognition and text synthesis for specifying metadata and
automatic creation of playlists.




\section{Acknowledgments}

We would like to thank CanAssist for providing the motivation and
environment for a lot of this work. Telus and the National
Science and Research Council of Canada (NSERC) funded parts of this work. 
Allan Kumka, Jennifer Murdoch and Stephen Hitchner worked on different
aspects of the system implementation.

\bibliographystyle{abbrv}
\bibliography{petrae2009}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
% Created 2011-04-28 Thu 10:10
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}


\title{Ref.:  Ms. No. JMUI-D-11-00010}
\author{Steven Ness}
\date{28 April 2011}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents


Sonophenology : A multimodal tangible interface for the
sonification of phenological data at multiple time-scales

Journal on Multimodal User Interfaces

Dear Mr Ness,

Reviewers have now commented on your paper. You will see that they
are advising that you revise your manuscript. If you are prepared
to undertake the work required, we would be pleased to reconsider
our decision.

For your guidance, reviewers' comments are appended below.

If you decide to revise the work, please submit a list of changes
or a rebuttal against each point which is being raised when you
submit the revised manuscript.

Your revision is due by 08-05-2011.

To submit a revision, go to \href{http://jmui.edmgr.com/}{http://jmui.edmgr.com/} and log in as an
Author. You will see a menu item call Submission Needing
Revision. You will find your submission record there.

There will be a second round of reviewing from the team of referees.

The editors reserve the right to reject papers at the second round
if the paper does not meet the satifaction of the refreeing and
editorial team.

Yours sincerely
Roberto Bresin, Thomas Hermann \& Andy Hunt 
Guest Editors
Journal on Multimodal User Interfaces

\section{Reviewers' comments:}

\subsection{Reviewer \#1}
Title: Sonophenology : A multimodal tangible interface
for the sonification of phenological data at multiple time-scales

Review: This paper discusses the development of a multimodal
interfaces that sonifies the flowering dates of Japanese purple
lilac's collected over a period of 13 years. The user interface
developed is a map of Japan with a camera positioned above; upon
the map fiducial markers are positioned by the user. The camera
system is fed into a computer running software to recognize the
position and orientation of the markers, which is then used to
select which data is sonified.

The work presented is an extension of a previous paper submitted to
ICAD 2010, this has been extended by the addition of a tactile
interface based upon the resistance of graphite, two lines of
graphite are drawn on a separate piece of paper and then electrodes
are placed on them by the user. The position of the electrodes is
used to control the date range of the sonification.

The paper is written clearly, demonstrates appropriate use of
English and is technically sound.

Comments:

\subsubsection{}
\textit{
Related Work -The work described is more closely related to
Geographical Information Systems that include sonification; such as
(Harding et al, 2002) ,(Harding \& Souleyrette, 2010) and (Zhao et
al, 2008). ``The Climate Symphony'' is primarily an artistic
presentation that has no user interaction; user interaction is a
key feature of this research. The related work section should
reflect this.
}

I have added these references and a description of them
to the related work section.

\subsubsection{}
\textit{ 
Interactive sonification - This work is very clearly an example of
interactive sonification, however the paper makes no reference to
this. I feel that this paper should acknowledge previous work in
this area.
}

Response : We have added a section to the Related Work section of
the paper that references and acknowledges previous work about
interactive sonification

Graphite based user interface - I do believe that this is a novel
aspect that would have great teaching applications in the
classroom - students could actively be involved in making their own
user interface.

References

Chris Harding, Ioannis A. Kakadiaris, John F. Casey, R.Bowen Loftin
``A multi-sensory system for the investigation of geoscientific
data'' Computers \& Graphics 26 (2002) 259-269

Chris Harding \& Reginald R. Souleyrette ``Investigating the Use of
3D Graphics, Haptics (Touch), and Sound for Highway Location
Planning'' Computer-Aided Civil and Infrastructure Engineering 25
(2010) 20-38

Zhao H., Plaisant C., Shneiderman B., ``Data sonification for users
with visual impairment: A case study with georeferenced data'' ACM
Transcations on Computer-Human Interaction, Vol 15, No 1, Article 4
2008



\section{Reviewer \#2}

REVIEW  for Multimedia user interfaces of
Sonophenology: A multimodal tangible interface for the sonification
of phonological data at multiple time-scales
(Manuscript number: JMUI-D-11-00010)

General comments

This paper looks at the processes of phenology in the general
context of its application to addressing climate change and
ecological processes. In particular, it introduces tangible
interfaces for interactive sonification of data. In terms of its
subject matter the paper fits within the remit of the
journal. However, in terms of justifying the approach in the
context of climate change and ecological processes, there are no
concrete examples of how it might be used effectively for their
study in practice, only indications as to how it could be used. The
sonification itself creates sounds that are broken up and not
particularly pleasant to listen to (videos on the website) and this
is of concern since no mention is made of the sound quality itself
either in the paper or on the website.

Please read the whole paper and check for repeated sections of text.

Specific comments

\subsection{}

INTRODUCTION: There are topics and ideas mentioned here that should
have references.  Please read through and add references
appropriately, but particularly for ``phenology'', data collection
from the flooding of the Nile, tangible interfaces, interaction
with physical objects in the real world, and merging real and
virtual world.

\subsubsection{Response}

References for the following subjects have been researched and have been added throughout the paper:

Phenology

lieth74

Data collection from flooding of the Nile

bell70

Tangible interfaces

reactabletei07

Interaction with physical objects in the real world

ishii97

Merging real and virtual world.

bimber05


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
INTRODUCTION, last paragraph: Please say how this paper extends the
work of Ness at al and remove the conference title as this is in
the reference itself.

\subsubsection{Response}
Response : The conference title has been removed, and we now
describe how the paper extends the work of Ness at al.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 2, line 12: Remove ``in this paper'' and ``we have designed''.

\begin{quote}
We describe in this paper a system we have designed that sonifies phenolog-
\end{quote}

\subsubsection{Response} : These have been removed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 2, line 16: Change ``field phenology'' to ``field of
phenology''. Please add some examples of the ``several aspects'' as
they are not necessarily obvious (if they were it would not be
worth mentioning).

\begin{quote}
just now becoming available. There are several aspects of the field phenology
\end{quote}

\subsubsection{Response} : These have been removed
Response - ``field phenology'' has been fixed and some examples of it
have been added.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 2, line 31: Please could you add some justification for
extending the work of Ness et al - why is it useful for the display
to be more multi-modal etc. in terms of the purpose of the
interface and what it might improve?

\begin{quote}
This paper extends the work presented [11] at the International Conference
\end{quote}

\subsubsection{Response} : These have been removed
Response - We have added a reference to a book on multimodal
interfaces that describes the advantages of multimodal intefaces in
general.  We have also added text to describe how multimodal
interfaces could be beneficial for our specific project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 2, line 46: The statement ``demonstrates that this type of data
is amenable to sonification'' is not unique to these data. As I
understand it, ANY data can be sonified. Engaging the public is one
thing but is the engagement achieving anything useful - I would
suggest that being ``interesting and pleasant'' is not a
justification - where is the relevance?

\begin{quote}
demonstrates that this type of data is amenable to sonification. Another im-
\end{quote}

\subsubsection{Response}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 3, line 15: Please add references to cover how your design''
has been informed by several different research topics''.

\begin{quote}
interactive, exploratory experience. Our design has been informed by several
\end{quote}

\subsubsection{Response}
Response : We have added references for each of the three different
research topics we discuss, for phenology, tangible interfaces and
sonification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 3, line 27: This is a repeat of what has already been stated.

\begin{quote}
linked to the environment in which the organisms exist, and one of the most
\end{quote}

\subsubsection{Response}
As the reviewer points out, the paragraph this line was in was already
covered in the introduction and has been removed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 3, line 36: What does ``exquisitely'' add to ``precise'' in this
context?

\begin{quote}
of phenological processes, and that these processes are exquisitely precise mea-
\end{quote}

\subsubsection{Response}
The use of word ``exquisitely'' is excessive in describing the way that
phenological processes measure climate and has been removed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 3, lines 46-49: This is a repeat of what has already been
stated.

\begin{quote}
Tangible Computing Interfaces link the virtual and real worlds using a variety
\end{quote}

\subsubsection{Response}
The information in the paragraph this line was in was already covered
in the introduction and has been removed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 4, lines 17-21: Please clarify the electrical circuit here and
add a circuit diagram. ``The resistance of the circuit is converted
into digital form'' needs some detail so it can be properly
understood.

\begin{quote}
imperfect conductor of electricity, a circuit containing a longer path of graphite
\end{quote}

\subsubsection{Response}
A clarification of the electrical circuit used has been added to the
text, and a figure with a circuit diagram has been added to help
explain the circuit.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 4, line 29: Sonification is audio and not a graph - so why is
there a graphical representation included in the video?

\subsubsection{Response}
That is an excellent point as we are trying to sonify data.  We found
for this particular educational application that the addition of a
graphical display of the score helped us to explain to participants
the concept of sonification.  Hopefully as the concept of sonification
of data becomes more accepted in the mainstream, these kind of
informational crutches will not be required.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 4, line 41: Presumably temperature is only associated with
``increasing pitch'' when it is a rising temperature? Perhaps
``changing pitch'' would be better?

\begin{quote}
temperature to pitch example, we must consider how quickly will the
pitch increase, and whether the relationship will be linear or
non-linear [15], that is
\end{quote}

\subsubsection{Response}
The phrase ``changing pitch'' more accurately describes this concept,
and we have changed ``increasing pitch'' to ``changing pitch''.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 4, line 44: Having considered how quickly the pitch increases
what would you do with the result? I do not think any consideration
is required concerning linearity since pitch should be logarithmic
to match our hearing system.

\begin{quote}
to say, one wants to preserve the ratios, not the differences in
frequency. The
\end{quote}

\subsubsection{Response}
The mapping of data ranges to frequency ranges is indeed a critical
part of the process of sonifiying data.  If the pitch increases too
quickly, a linear mapping of data to pitch values can often be very
effective, and as the reviewer states, our hearing system perceives
pitches logarithmically.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 4, line 47: Why are the aesthetics important and in what
sense? It should be made clear whether this is for art or
scientific research (see also line 49 in relation to being
``pleasing to listen to'' - it is not clear to me that this is at all
important and something displeasing could be a key way to find
discontinuities for example.) What does ``logically'' and clearly''
mean in this context?

\begin{quote}
aesthetics of sonification are also an important consideration. The
goal is to
\end{quote}

\subsubsection{Response}
In general in the field of sonification, aesthetic concerns are
secondary to the primary concern of helping people to understand data.
The comment of the reviewer that discontinuities could be a key way to
find discontinuities is an excellent point and has been added to the
paper.  In this particular application of presenting phenology data to
the public in an educational and engaging manner, aesthetics is of
importance in order to enhance engagement of the public.  We have
rewritten the section that formerly contained ``logically'' and
``clearly'' to clarify our point.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 5, line 45: Please explain ``sanitize'' as it suggests that the
data are being modified which is not appropriate in any scientific
system without clear appropriate justification.

\begin{quote}
in our interface. We first sanitize these data sources and read them into our
\end{quote}


\subsubsection{Response}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 7, lines 47-49: This is a repeat of what has already been
stated.

\subsubsection{Response}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 8, line 43: How many different lines of data at once - you
should look into the musical literature to find how many individual
lines within a piece of music can be followed by a professional
musician - you will find it is not many!

\subsubsection{Response}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 9, line 38: Please explain the ``organisation of pitches'' and
how a listener can distinguish events . Do they need musical
training? Do they recognise particular patterns? Please state
clearly how this system is used and how scientific research is
carried out with it.

\subsubsection{Response}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
Page 9, line 45: I agree that this is essential and I want to read
how it can be done and is being done (see also comment about page
9, line 38 above). I believe you could do what you are suggesting
quite adequately with a graph (cf Fig 5) so I am having difficulty
understanding how sonification can aid the process (your website
shows a graphical representation that is quite adequate - in fact I
would argue that the sound does not help given its broken up
quality.

\subsubsection{Response}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{}
REFERENCES: Page numbers should be included for references 5, 9, 11,

\subsubsection{Response}



\end{document}
%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Sonophenology  %\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}

\subtitle{A multimodal tangible interface for the sonification of
  phenological data at multiple time-scales}

%\titlerunning{Short form of title}        % if too long for running head

\author{
	%% Steven R. Ness \and
    %% W. Andrew Schloss \and
	%% George Tzanetakis
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{
	Steven R. Ness \at
    University of Victoria \\
    Department of Computer Science \\
    Tel.: (250) 472-5700\\
    Fax: (250) 472-5708\\
    \email{sness@sness.net} \\
    and
	W. Andrew Schloss \at
    University of Victoria \\
    School of Music \\
    \email{aschloss@uvic.ca} \\
    and
	George Tzanetakis \at
    University of Victoria \\
    Department of Computer Science \\
    \email{gtzan@cs.uvic.ca} \\
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

The study of periodic biological processes, such as when plants flower
and birds arrive in the spring is known as Phenology.  In recent years
this field has gained interest from the scientific community because
of the applicability of this data to the study of climate change and
other ecological processes.  In this paper we propose the use of
tangible interfaces for interactive sonification with a specific
example of a multimodal tangible interface consisting of a physical
paper map and tracking of fiducial markers combined with a novel
drawing interface. The designed interface enables one or more users to
specify point queries with the map interface and to specify time
queries with the drawing interface.  This allows the user to explore
both time and space while receiving immediate sonic feedback of their
actions.  This system can be used to study and explore the effects of
climate change, both as tool to be used by scientists, and as a way to
educate and involve members of the general public in a dynamic way in
this research.

\keywords{Sonification \and Tangible Interfaces \and Geo-Spatial Data \and Phenology}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}

The ancient science of phenology is the study of the annual timing of
biological processes such as when a particular species of tree first
flowers in the year, when birds return from their migrations, or when
frogs first emerge after winter.  The word phenology comes from the
two Greek words for ``to show or appear'' (phaino) and ``reasoning, or
rational thought'' (logos).

Phenological data has been collected and used since the dawn of
agriculture and civilization in humans.  From the timing of the
flooding of the Nile \cite{bell70} to the best times to harvest maize, people have
collected this data and have used it to determine timing strategies
for planting as well as to keep track of medium to long term climatic
changes.  In the example of farming, not only the timing of biological
processes but also the dependence of timing on specific geographic
locations are of prime importance.

%% In just the last few years, internet enabled collaborative websites
%% have transformed the collection of phenology data from a discipline
%% where the individual scientist or farmer records their data onto paper
%% for primarily their own research or use, into one where large numbers
%% of amateurs from the general public can all collect and enter their
%% own phenological observations.  Sites such as the National Phenology
%% Network \footnote{http://usanpn.org} and Nature's
%% Calendar \footnote{http://www.naturescalendar.org.uk/} allow citizen
%% scientists to record their own observations about when certain natural
%% phenomenon happen.  This type of approach is commonly referred to as
%% Crowdsourcing.


A new type of interface metaphor in the field of Human-Computer
Interactions (HCI) is that of tangible interfaces.  In these
interfaces, users not only interact with the screen and keyboard as
usual, but also interact with physical objects in the real world.
Types of devices commonly used for tangible computing include cameras,
sensors, motors, actuators and displays.  These interfaces help to
merge the real and virtual worlds into a single, unified user
interface \cite{bimber05}.

We describe a system that sonifies phenological data and allows users
to explore these datasets using a tangible interface.  The system we
propose could be used with both historical phenological data and also
with the large quantities of crowdsourced phenological data that is
just now becoming available.  There are several aspects of the field
of phenology that make it a particularly interesting candidate for
control through a tangible interface and sonification.  One of these
is that phenology studies changes over time and music can be thought
of as sounds organized in time \cite{landy07}, the interrelation and
mappings between data and time in the problem domain and the music
domain.

 Ideally a system for
exploring phenology data should allow the specification of both
spatial and time range queries in addition to simple point queries
(e.g. render the data from Tokyo and Osaka between 1985-1990).  We
describe a multimodal tangible interface with fiducial markers on a
map that are used to specify point queries and a tangible drawing
interface to specify time queries.  Of particular interest is the
relative timing of different events such as the flowering that happens
earlier in the South than the North.  Synchronicity and relative
timing are clearly conveyed in our sonification.

This paper extends the work presented \cite{ness10} by Ness, et al. by
making the display to be more multi-modal, with both a map-based
fiducial interface as well as a drawable resistive sensitive
interface.  Multi-modal interfaces have a number of advantages over
unimodal interfaces, including a flexible use of input modes to best
suit the task at hand, allowing diverse user groups with different
abilities to use an interface, and that they have the flexibility to
adapt to the continuously changing conditions of mobile use
\cite{oviatt02}.  In our particular case one important advantage of
using a multimodal interface is that our device is designed to be used
in teaching setting or as a public art installation, and by having a
combination of the fiducial based interface with the graphite
interface, participants would be more engaged with the topic of
climate change.


\section{Related work}

A related paper is ``The Climate Symphony'', \cite{quinn01} in which a
system is described that is a combination of sonification and a
narrative structure.  In this artistic presentation, 200,000 years of
ice core data is sonified by taking eight sets of time series data of
the relative concentrations of a number of ions and reducing them to
three dimensions using Principal Component Analysis.  These time
series data were sonified with simple sine waves which were then
amplitude modulated by the amount of the ice sheet's coverage.  This
design exploits the natural ability of humans to hear periodic
structure in audio signals, and demonstrates that this type of data is
amenable to sonification.  Another important contribution of this
paper that is of direct relevance to our work is that it attempts to
create a system that will engage members of the general public by
providing an interesting and pleasant way to explore climate data.

Also related is the paper ``Sonification of Daily Weather Records''
\cite{flowers01}, in which the authors describe a system that sonifies
the weather data from Lincoln, Nebraska.  In this paper, the authors
choose three different parameters to sonify, temperature, rainfall and
snowfall.  For the temperature, they take daily high and low
temperature measurements and convert these to MIDI notes.  Because of
the sizable difference between the high and low notes, this produces a
sonification with two independent melodic lines, which humans are able
to independently track as separate streams, as previous research by
Bregman has shown \cite{bregman90}.  

In the majority of existing system for sonifying scientific data the
result of the sonification process is a monolithic audio signal and
the amount of influence users have in the sonification process is
minimal or non-existent. In contrast in our system we have tried to
make the sonification process an interactive, exploratory
experience. Our design has been informed by several different research
topics: phenology\cite{lieth74}, tangible interfaces\cite{ishii97},
and sonification\cite{zhao06}. In the following section we describe
these different topics and show how they relate to our work. The
resulting system which we call Sonophenology integrates these
different influences in a coherent whole.

A multi-sensory system for the investigation of geoscientific data
\cite{harding02}

Investigating the Use of 3D Graphics, Haptics (Touch), and Sound for
Highway Location Planning \cite{harding10}

Interactive sonification of abstract data: framework, design space,
evaluation, and user tool \cite{zhao06}

Interactive sonification - acknowledge previous work in this area


\section{Background and Motivation}

\subsection{Phenology}

%% Phenology is the study of the timing of biological processes as they
%% occur during various times of the year.  The timing of biological
%% processes are intimately linked to the environment in which the
%% organisms exist, and one of the most important determiners of the
%% timing of seasonal changes is the average local temperature.  For
%% example, during a warm year, cherry blossoms will flower earlier than
%% they would during a year with a colder spring. 

Recently, phenological data has been used in a number of research
projects in climate change \cite{post99} \cite{penuelas02}
\cite{walther02} \cite{menzel06}.  In these studies, a general
conclusion has been reached that changes in local and global
temperature affect the timings of phenological processes, and that
these processes are precise measures of climate change.  Currently
these results are typically compared using using either statistical
measures, such as the ANOVA (Analysis of Variance) tests such as in
Doi \cite{doi08} or using visual representations of this data such as
graphs that show histograms of the timing of various events across
years.

%% Data about winter temperatures have been recorded for the last 2000
%% years in China \cite{ge03}, and this data has been used to study
%% climate variations.  Another set of phenological data that has been
%% used to study climate change is that of Burgundy grapes in France
%% \cite{chuine04}.  In this study, spring and summer temperatures from
%% 1370 to 2003 were studied, and using the data from the ripening of
%% this species of grape, it was possible to look at variations in
%% temperature over this time span.  These types of studies show that
%% phenology data can be used as a source of proxy data for studying the
%% climate.  Karl Linnaeus, the founder of modern taxonomy, studied
%% phenology extensively, and by making observations of the flowering of
%% 18 different plant species across Sweden.  In his research, he came to
%% the conclusion that flowering plants are exquisitely sensitive weather
%% instruments.

%% \subsection{Crowdsourcing}

%% Crowdsourcing is a relatively new phenomenon that has been enabled by
%% the pervasive spread of the internet in society, and allows members of
%% the general public to help scientists collect or analyze data.  It is
%% a new type of collaboration where non-specialists help expert
%% scientists \cite{howe08_crowdsourcing} and has been used to great
%% advantage in a number of research programs
%% \cite{surowiecki05_crowdsourcing} \cite{bradham08_crowdsourcing}
%% \cite{travis08_crowdsourcing}.  Hong \cite{hong04_crowdsourcing}
%% presents results that show that a group of problem solvers with a
%% diverse background can outperform smaller groups of experts.

%% Whereas it used to be the case that obtaining phenological datasets
%% used to be a difficult and time consuming process, the advent of these
%% websites will mean that there will soon be huge archives of
%% phenological data.  One of these sites that has already started to
%% distribute data is the Nature
%% Watch \footnote{http://www.naturewatch.ca} website in Canada.  This
%% website has subprojects including IceWatch, PlantWatch, FrogWatch and
%% WormWatch that monitor the timing of various physical and biological
%% processes, including when ice is present, when plants emerge and bloom
%% and when worms and frogs emerge from hibernation.

%% With the advent of these new crowdsourced sites for the collection of
%% phenological data, the concept of phenology is becoming more well
%% known in the general community.  These websites have thousands of
%% observers located in many geographical regions, and with this data
%% becoming available, it can be anticipated that these citizen
%% scientists will want to observe the results of their observations.
%% Currently, results are usually presented in the form of a map with an
%% associated timeline which allows the user to go back and forth in time
%% to observe which plants are flowering at which places over time.

\subsection{Tangible Interfaces}

%% Tangible Computing Interfaces link the virtual and real worlds using a
%% variety of different technologies, including cameras, microphones,
%% accelerometers, the sensing of resistance amongst many others.  In our
%% paper we use two of these technologies, fiducial markers and a
%% resistive drawable interface.

A fiducial marker detector uses a camera system and a set of fixed
tags that could contain a variety of different iconographies that
typically look like a grid of dots.  The colour, geometry and topology
\cite{regionadjancency_vvg03} of the placement of these dots are
designed to be uniquely identifiable \cite{fiala10}.  There are many
advantages to the use of fiducial tracking systems, including a
direct-manipulation modality, multiple dimensions per fiducial marker
and the ability to quickly and cheaply print new markers using an
ordinary printer. An excellent collaborative benefit of this system is
that markers can be positioned by a separate person, or by teams of
people.

Another type of tangible interface that has gained some popularity
amongst artists are drawable interfaces that exploit the fact that
graphite is a partially conductive material.  Graphite is a common
medium used by artists, both in the form of raw charcoal and
compressed into pencils.  Because graphite is an imperfect conductor
of electricity, a circuit containing a longer path of graphite will
have more resistance than a shorter one.  The resulting circuit is
about as simple as can be imagined, consisting only of a variable
resistor, which is the graphite strip, an Analog to Digital (A/D)
converter, a power source, and the wires that connect these items.
This circuit is shown in Figure \ref{fig:circuit}.  In this type of
drawable interface, an electrode is attached to a section of graphite
drawn on paper, which is then attached to an Analog/Digital converter.
To complete the circuit, another electrode is placed on a piece of
graphite, and the resistance of the circuit is converted into digital
form.

\begin{figure}[htb]
\begin{center}
	\includegraphics[width=30mm]{circuit}
\end{center}
\caption{A circuit diagram of the graphite resistive sensor.}
\label{fig:circuit} 
\end{figure} 


%% In our system, we print a map of the area desired to be sonified, and
%% add a small fiducial marker on the top left side of the map, this
%% allows the system to then do a mapping of real-world coordinates to
%% the image detected by the camera.  Users then place fiducial markers
%% on the map, which are mapped to locations on the map in question.  By
%% moving these markers to different points on the map, the data
%% associated with that map area is sonified.

%% This fiducial marker-based interface is combined with a tangible,
%% drawing based interface in our current work.  In small ethnographic
%% based research with students and other users we found that the
%% addition of this second interface to our multimodal system enhanced
%% the collaboration of users together to solve problems.  With this
%% enhanced collaborativity, this type of interface allows for the
%% solution of problems in new and interesting directions, for example,
%% one user might have a better facility for navigating around 2D maps,
%% where another user may find it more intuitive to use the conductivity
%% based interface for navigating date ranges.

%% We extend the concept of a tangible, fiducial marker-based interface
%% used to create an aesthetically pleasing, and usable environment for
%% exploring phenological data.

\subsection{Sonification}

Sonification can be described as the use of audio to convey
information.  In other words, scientific data is represented not as a
visualization, like a graph, but instead as a collection of sounds
that are played at different times.

The manner in which a given set of data is mapped to audio is a
challenging problem, there are an infinite number of ways to transform
data into a sonification\cite{fitch94}.  Many aspects of any sound can
be modified: we can perceive changes in amplitude, pitch, timbre,
directional, and temporal information.  Any of these auditory aspects,
or audio parameters, can be modified by a data set. The best choice
when selecting audio varies, depending on the content of a given set
of data.  The direction, or polarity, of the datasets that are being
compared can also affect the perception of a sonification.  For
example, temperature is often described aurally as a tone with
changing pitch \cite{walker07}. The scale of the relationship between
a one-dimensional data set and the audio parameter modified by that
data must also be considered.  If we consider the temperature to pitch
example, we must consider how quickly will the pitch increase, and
whether the relationship will be linear or non-linear \cite{walker00},
that is to say, one wants to preserve the ratios, not the differences
in frequency.  The presence of interesting trends, for example
discontinuities, can often be clearly identified in sonifications of
data.  The aesthetics of sonification are also an important
consideration.  The goal is to create a collection of sounds that
represents a dataset accurately, logically and clearly and is also
pleasing to listen to.

\section{System Description}

\subsection{Overview}

\begin{figure}[htb]
\begin{center}
	\includegraphics[width=80mm]{flowChart}
\end{center}
\caption{A flowchart of the system organization of our system.  This
  system has at its core a Controller module that communicates with
  the phenology and GIS databases as well as the video camera and
  sonification engine.  It generates sonifications by tracking the
  positions of fiducials on a printed paper map.}
\label{fig:flowChart} 
\end{figure} 

Our system consists of a number of separate sub-components that
interact together to provide a tangible interface for the exploration
of geo-spatial phenological data.  The overall organization of this
system can be seen in Figure \ref{fig:flowChart}.

The phenological data sources that we obtained for this paper are
quite diverse, and contain various types of information that could be
used for sonification.  In this application, we constrained our
analysis to include only the species name, the latitude and longitude
of the observation and the date when this observation was taken.
Other data that we are not using for this paper includes the type of
observation, for example, was the observation of the first bloom of
the lilacs or when they were in full bloom.  Many of the observations
also include comments from the observers.  These additional sources of
data could be used in the future to enhance the sonification and
visualization in our interface.  We first sanitize these data sources
and read them into our GIS-enabled database system.

The second section of our system is the fiducial tracking interface.
This uses a consumer grade webcam and tracks pre-printed fiducial
markers on a surface.  We also use fiducial markers to determine the
position and orientation of the physical map underneath the fiducial
markers.  We then create a mapping from the set of coordinates of the
fiducial markers to the physical latitude and longitude on the map.
When the user places fiducial markers on the map, this system then
takes the latitude and longitude of these points and queries the
database to obtain corresponding phenological data points.

The third part of our system is a drawing based interface that uses
the conductivity of graphite to allow users to select a specific date
range.  In our system we use two different regions of graphite that
has been applied to a heavy stock type of paper, one region for the
beginning date and one region for the end date.  The conductivity
information is digitized by the Make Controller single board system
and are sent to the controller program using the Open Sound Control
(OSC) protocol.

The final step in this system then involves taking these phenological
data points, which include latitude, longitude, species and
observation date, and sonifying them.  

\subsection{Phenology - Japan lilac}


In order to describe the system we focus on a set of of observations
of the flowering of the common purple lilac \textit{Syringa vulgaris}
in Japan \cite{funakoshi00}.  Observations on the flowering of this
species were collected from 1996 until 2009.  Because of the large
difference in latitudes between the south and north of Japan, flowers
bloom earlier in regions in the south of Japan before they do in the
north of Japan.  These types of geographical differences are one
source in the variation of flowering times.  Another difference that
may be possible to observe is the effects of climate change on the
flowering times of these lilacs, however to truly see effects of
climate change, one must of course examine temperature records over
longer time spans, on time spans of centuries to millenia.  If average
temperatures increase over a period of years, one would expect that
the phenological processes that respond to temperature would tend to
move to earlier times in the year.

\subsection{Tangible interface}

%% - map of japan (or the place of interest)
%% - fiducial markers

\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm]{apparatus}
\end{center}
\caption{Shown above is a picture of the fiducial tracking interface.
Above the computer monitor is a small consumer grade video camera,
which is pointed downwards in order to view the fiducial markers which
are placed on a printed paper map.}
\label{fig:apparatus} 
\end{figure} 

While it would be possible to develop a simple desktop or web-based
interface to explore a sonification of this data, a much more
intuitive and engaging interface could be a tangible interface, where
users interact with a physical interface.  We have chosen a fiducial
based tag tracking system previously used in the reacTable
\cite{reactable_tei07}.  A picture of this system is shown in Figure
\ref{fig:apparatus} and a detailed picture of the map is shown in
Figure \ref{fig:fiducialDiagram}.

This interface is inexpensive and easy to deploy, requiring only a
consumer-grade webcam, physical printed map and printed fiducial tags,
and could be easily deployed within a classroom setting.  With such a
system in a classroom, a teacher could teach students not just about
phenology, climate change and maps, but also about new systems for
physical interaction with computers.  By moving markers across the
map, the students experience a direct correlation with the location of
the marker on the map and the associated phenological data.  Because
of its physicality and interactivity, this is a system that even young
children could use to interactively engage in climate study, something
that otherwise might be too abstract.

In the previously described version of this interface \cite{ness10},
we used the rotation of the fiducial markers to specify a date range
to be sonified.  We found that this interface was quite non-intuitive
for most users.  In our current work, we have replaced this method for
selecting date ranges with a drawing interface based on the
conductivity of graphite pencils.  In this interface, a piece of paper
is used, and using graphite pencils, two regions are drawn on this
piece of paper.  One set of electrodes are attached to one end of the
solid drawn region, and another is attached to the ends of two
graphite pencils.  Because graphite is a conductor of electricity,
electrical current can run through this circuit, and because it is not
a perfect conductor of electricity, a longer path will give rise to a
higher resistance.  We then take these two circuits and connect them to a
device capable of converting this analog signal into a digital signal,
in our case we use the popular Make Controller, which is a small,
general purpose controller device.  The Make Controller has the added
advantage that it natively outputs data using the Open Sound Control
(OSC) protocol.  We map one of these regions to the beginning date to
be sonified and the other region to the end date to be sonified.  We
have found the entire interface works best with two or more users, one
to select the date range, and one or more to select the range of dates
to be sonified.  We have found this type of multi-user interface works
well when applied in educational contexts, and encourages groups of
students to work together to explore and interact with this data.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm]{tangible}
\end{center}
\caption{Our tangible drawing interface is shown in this figure.  On
  the sheet of paper two regions of graphite have been drawn.  These
  are hooked up to the analog input ports on the Make Controller shown
here.  Hooked to the positive voltage terminal are two artists
charcoals which complete the resistive sensing circuit.  }
\label{fig:tangible} 
\end{figure} 


\begin{figure}[htb]
\begin{center}
\includegraphics[width=70mm]{fiducialDiagram}
\end{center}
\caption{ Moving a fiducial will sonify data that is represented by
  map locations under that particular fiducial marker.  These fiducial
  markers and the map itself are printed out on heavy stock paper in
  an ordinary printer.}
\label{fig:fiducialDiagram} 
\end{figure} 

%% In order to generate the query of the GIS database that contains the
%% phenology data, we use two different user-interface metaphors.  The
%% first, simple method, is to simply use the center of the fiducial as
%% the latitude/longitude search point and to return all data points that
%% lie underneath that fiducial.  A more complex setup that we have also
%% implemented allows users to select a region of the map and a time
%% range for each region.  In this scheme, regions are created by placing
%% two fiducials on the map.  The first fiducial specifies the top left
%% corner of a bounding box and the second fiducial specifies the bottom
%% right corner.  To change the time range that is sonified, the system
%% calculates the relative rotation angle between the two fiducials and
%% maps this to a value of years.  This setup is demonstrated in Figure \ref{fig:fiducialDiagram}.

%% - step sequencer 
%%  - each fiducial is a different timbre (instrument)
%%  - each year is a different pitch
%%  - each date (from april 10th-july22nd) is a step in the step sequencer



\subsection{Sonifications}

There are a number of advantages to sonifying these phenological data
over using statistical tools and visual graphs.  One advantage is that
by using different timbres to represent the different sections of the
map that we are sonifying, we take advantage of the fact that humans
can distinguish different melodic streams that are rendered in
parallel by different timbres.  This could potentially allow a user to
follow many different lines of data at once.  This technique becomes
even more powerful because of the distributed geographical and
temporal nature of the phenological data, where flowers in the south
bloom earlier than flowers in the north.  These different melodic
lines start and swell at different times, and the combination of
different timbres with different start times of these timbres make it
even simpler for users to follow the progression of phenological
events.

Our primary sonification metaphor is that of a step sequencer, which
uses a fixed two-dimensional grid consisting of quantized steps, with
the horizontal axis representing time and different steps on the
vertical axis being different instruments, or different pitches of one
instrument.  In our system, the vertical, or pitch axis, corresponds
to different years, and the horizontal, or time axis, corresponds to
the timing of the phenological event in days since the start of the
year.  This system allows us to easily hear and compare changes in the
timings of different events over years by listening to the
organization of pitches.  If a phenological event occurs on the same
date each year, one would hear a chord of all the notes at the same
time.  If on the other hand, the date of a phenological event becomes
earlier each year, one would hear a descending arpeggio of notes. A graphical
view of three observation locations over a time period of 10 years is
shown in Figure \ref{fig:sonificationChart}.


The comparison of phenomena over various years is an essential part of
this system, as one a primary motivation of this project is to provide
a way for people to not just see but also hear and explore the effects
of climate change.  These different modalities of experience might
prove effective in educating people about phenology and climate
change.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=80mm]{sonificationChart}
\end{center}
\caption{A graphical representation of 10 years of flowering data for
  the common lilac in three locations in Japan.  The three different
  locations are depicted by different shapes, a circle, a triangle and
  a star.  From this diagram, one can see that there are certain years
  (2002-2004) in which flowering occurred earlier than in other years.}
\label{fig:sonificationChart} 
\end{figure} 

%% One mapping that we have found to be useful is a step sequencer.  In
%% our system, one axis of the sequencer has pitches that correspond to
%% different years, where earlier years have lower pitches, and later
%% years have higher pitches.  On the other axis of the step sequencer we
%% have the day of the year.  We also then map each fiducial marker to a
%% different musical instrument or timbre.  With this mapping, if the
%% flowers in a specific region all flowered at the same time, then one
%% would hear the notes from all the years sounding at once.  On the
%% other hand, if the flowering dates occur at later times each year, one
%% would hear an arpeggio of notes with increasing pitches.

Another mapping that we are exploring is to instead represent each
phenological observation as a distinct sonic event.  This type of
sonification produces a radically different soundscape which is more
textural and ambient.  One can imagine what this sonification sounds
like by thinking of the timing of blooming of plants in the spring.
One will often see one or two different plants of a species flower,
then as time goes on more plants will flower in almost an exponential
fashion until all the plants of the species have flowered.  If one
were to sonify each of these events as an impulse sound, then the
sonification of this data would sound something like the popping of
popcorn.  What is interesting in this method is that it allows us to
perceive the ``stochastic'' nature of the natural process, where each
event is not significant unto itself, but the aggregate events outline
a process that can be reflected in an auditory soundscape that reveals
subtle differences in the rate of change of a physical system.  Our
ears are very sensitive to subtle differences in stochastic signals
like colored (or filtered) noise changing its spectral characteristics
over time.

%% When converting data into audio, there are a number of different
%% mappings that can be used.  The simplest would use a sinusoidal
%% oscillator and would linearly map input data into the frequency of
%% this oscillator.  One disadvantage of this mapping is that in the
%% human auditory system, the frequency to pitch ratio is not linear but
%% rather is logarithmic.  Because the human ear hears frequencies
%% logarithmically, a logarithmic mapping of data to frequency would more
%% accurately preserve the ratios of data points to each other.  There
%% are a potentially infinite number of mappings of data to pitch values,
%% the one that we chose for this application was to map data values onto
%% the equal tempered scale, as seen on the piano keyboard or MIDI note
%% values.  However, in our system, we anticipate that several values
%% could occur at one point in time, and if we were to simply map data
%% values to MIDI note values, it would be common to encounter
%% dissonances in simultaneously played notes.  To overcome this, one can
%% use different scales or chords instead of the chromatic scale.  In our
%% system, we mapped the 10 different year values to the pentatonic
%% scale.  We are also developing mappings using chords, for example the
%% notes of a C-sharp major 7th chord, or any other chord, could be used
%% to map each year to a pitch component of the chord. Then the
%% chronological order would determine the position of the year within
%% the chord - in our example chronological order follows pitch height.
%% One could use the same chord for all instruments with or without the
%% same keynote, however, one could also use different chords for
%% different instruments, which might have the advantage that it would
%% further improve distinguishability for people by different melodic
%% lines following the chords.

%% For most of our work in this paper we have used sampled sounds from
%% the RWC dataset \cite{goto03}.  However, we have also implemented a
%% synthetic instrument model in order to provide more and different
%% sonification parameters.  In doing this, we have implemented simple
%% sine sources, plucked strings as well as more advanced synthetic
%% models of physical instruments.  The advantage to using these types of
%% synthesized sounds is that it is possible to control different
%% parameters of the sound, for example, the brightness of a clarinet
%% sound, or attack speed of a trumpet, these parameters can then be
%% mapped to the data that is being sonified.  Using synthetic instrument
%% models, one could also generate timbres that are intermediate between
%% two instruments, for example, one could make a sound that was half-way
%% between a clarinet and saxophone.  This type of fine-grained control
%% is difficult to implement using pre-generated samples.  The main
%% disadvantage to using synthetic instruments is that the models are
%% often quite elaborate and are computationally expensive, which limits
%% the amount of simultaneously playing instruments.

%% Plucked string model 

%% - sine source
%% - synthetic instrument
%% - using different aspects that you can control for instruments map
%%   these to different data
%% - timbre changing over time - difference between first bloom and full
%%   bloom
%% - difficult to do this with samples
%% - chords - use chord structure for these
%% - each year has own note within the chord
%% - disharmonicity
%% - intermediate timbres
%% - most models are quite elaborate - maybe too much for the computer
%% - different instruments can play different chords


\section{Conclusions}

In this paper we have presented a system that takes geo-spatial
phenology data and allows users to interact with it using a tangible
interaction metaphor.  The dataset of the flowering dates of Japanese
lilacs was a useful dataset to explore with this system as it
contained data points of flowering dates that occurred at different
times and in different locations from the northernmost to the
southernmost areas in Japan.  

We have explored this dataset with our system.  We have observed a
number of interesting properties of the data and of the system.  One
interesting observation about this data is that in certain years the
flowering of trees occurs earlier, and in some years they occur later.
This is clearly heard in the sonification of this dataset because in
these years, the note that is played for the different instruments is
the same, and is repeated earlier in the cycle than those notes from
other years.  Another observation is that for the data points that
occur earlier in later years, a descending arpeggio is indeed heard.

With the inclusion of the tangible interaction interface, this system
is quite approachable for members of the general public, and in the
few number of interactions that these individuals have had with our
system, they find it both interesting and easy to use.  We are
currently considering doing user studies with this system, with the
goal of building a system to help educate students and the public
about climate change with an engaging interface.

%% In future work, we would like to develop a similar system to the one
%% described in this paper but for mobile devices, such as the iPhone.
%% This interface would allow people to interact with a computer
%% generated map of a region, for example, a map of Japan and would allow
%% people to explore the timing of various phenological events on their
%% own personal mobile device.  In conjunction with this, we are building
%% a web-enabled version of this app using a combination of Flash and
%% HTML5 technologies.  The advantage of these web based and iPhone based
%% applications is that they could have much wider penetration into the
%% general community, at the cost of a more limited interaction metaphor.

This system can also be used for other phenology datasets, and as
websites such as the National Phenology Network and Nature's Calendar
start releasing their crowdsourced data, we anticipate that there will
be a huge amount of phenological data that would be interesting to
sonify.  In addition, this system could also be used with other
geo-spatial datasets, for example, one could develop an interface to
allow scientists to sonify the amount and type of ground cover as
determined by satellite images.

We have made a website\footnote{http://sonophenology.sness.net} that
presents visualizations and sonifications of the data used in this
paper, along with videos showing the system in action.

\section{Acknowledgments}

We would like to thank the National Phenology Website and
Dr. S. Funakoshi for making the Japanese lilac dataset available to
the scientific community.  Steven Ness was funded in part by a NSERC
Ph.D. scholarship from the Government of Canada.

\bibliographystyle{spmpsci}      % basic style, author-year citations
\bibliography{sono2011gtzan}

\end{document}
% end of file template.tex

\documentclass[ChapterTOCs,krantz1]{krantz}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{epsfig}
\usepackage{makeidx}
%\usepackage{showidx}
\usepackage{multicol}
\frenchspacing
\tolerance=5000

\include{Chapters/chapter1/preamble}

\makeatletter


\makeatother

\makeindex

\begin{document}

\title{Book title goes here}

\author{Author Name}

\maketitle

\frontmatter
\include{frontmatter/foreword}
\include{frontmatter/preface}

\listoffigures
\listoftables
\tableofcontents

\mainmatter

\include{Chapters/symbollist}

\setcounter{page}{1}
\part{This is a Part X}

\include{Chapters/chapter2/ch2}

\bibliographystyle{plain}
\bibliography{biblio}

\clearpage
\printindex

\end{document}
\documentclass[acmtomcca]{acmtrans2m}
%% \acmVolume{2}
%% \acmNumber{3}
%% \acmYear{01}
%% \acmMonth{09}

\markboth{Steven Ness et al.}{Content aware music discovery using self-organizing tag clouds}

\title{Content aware music discovery using self-organizing tag clouds}
\author{
  STEVEN R. NESS\\
  University of Victoria\\
  MELISSA NESS\\
  Vision Critical
\and
  GEORGE TZANETAKIS\\
  University of Victoria
}



%% \documentclass[tomcca]{acmtrans2m}
%% \usepackage{times}
\usepackage{url}
\usepackage{graphicx}
%% \usepackage{color}
%% %\usepackage[pdftex]{hyperref}
\usepackage{subfigure}
%% %% \hypersetup{%
%% %% pdftitle={Your Title},
%% %% pdfauthor={Your Authors},
%% %% pdfkeywords={your keywords},
%% %% bookmarksnumbered,
%% %% pdfstartview={FitH},
%% %% colorlinks,
%% %% citecolor=black,
%% %% filecolor=black,
%% %% linkcolor=black,
%% %% urlcolor=black,
%% %% breaklinks=true,
%% %% }
%% \newcommand{\comment}[1]{}
%% \definecolor{Orange}{rgb}{1,0.5,0}
%% \newcommand{\todo}[1]{\textsf{\textbf{\textcolor{Orange}{[[#1]]}}}}

%% \pagenumbering{arabic}  % Arabic page numbers for submission.  Remove this line to eliminate page numbers for the camera ready copy


%% % to make various LaTeX processors do the right thing with page size
%% \special{papersize=8.5in,11in}
%% \setlength{\paperheight}{11in}
%% \setlength{\paperwidth}{8.5in}
%% \setlength{\pdfpageheight}{\paperheight}
%% \setlength{\pdfpagewidth}{\paperwidth}

% use this command to override the default ACM copyright statement 
% (e.g. for preprints). Remove for camera ready copy.
%\toappear{Submitted for review to CHI 2010.}


%% \author{
%%   \alignauthor Steven R. Ness\\
%%   \affaddr{Department of Computer Science}\\
%%   \affaddr{University of Victoria}\\
%%   \email{sness@sness.net}
%%   \alignauthor Melissa Ness\\
%%   \affaddr{Vision Critical}\\
%%   \email{melissaness@gmail.com}
%%   \alignauthor George Tzanetakis\\
%%   \affaddr{Department of Computer Science}\\
%%   \affaddr{University of Victoria}\\
%%   \email{gtzan@cs.uvic.ca}
%% }

%% \numberofauthors{3}
%% \author{
%%   \alignauthor Author A\\
%%     \affaddr{Department of Computer Science}\\
%%     \affaddr{University of Somewhere}\\
%%     \email{a@somewhere.edu}
%%   \alignauthor Author B\\
%%     \affaddr{Department of Computer Science}\\
%%     \affaddr{University of Somewhere}\\
%%     \email{b@somewhere.edu}
%%   \alignauthor Author C\\
%%     \affaddr{Department of Computer Science}\\
%%     \affaddr{University of Somewhere}\\
%%     \email{c@somewhere.edu}
%% }

\begin{abstract}
Personal digital music collections are continuously growing and
frequently feature thousands of tracks. Browsing and navigating these
large collections is challenging. The most common way of interaction
is using textual meta-data such as artist name or genre. More recently
tag folksonomies have also been utilized. A variety of visualizations
based on automatically analyzed musical content have also been
proposed.  Tag clouds are a two-dimensional stylized visual
representation of a list of words with different visual design
characteristics for each word. Tag clouds are commonly ordered either
alphabetically or randomly, and in this paper we examine the utility
and engageability of placing similar tags next to each other.  In
order to determine similarity of tags we use a self-organizing map
based on acoustical features derived from songs rather than using the
more common tag co-occurrence to measure similarity. To evaluate the
proposed approach a subset of the Magnatune database tagged using the
Targeting game-with-a-purpose was utilized. Experimental results
showing that using a self-organizing tag cloud is both faster and more
fun are presented.
\end{abstract} 

\category{H.5.2}{Information Interfaces and Presentation}{User Interfaces, Miscellaneous}

\keywords{multimedia annotation, multimedia analysis, audio feature extraction, semi-automatic annotation, machine learning} 

\begin{document}

\begin{bottomstuff}
Author's address: Department of Computer Science, University of
Victoria, British Columbia, Canada, V8W 3P6.  Vision Critical: Suite
700, 858 Beatty Street Vancouver, BC, Canada, V6B 1C1.
\end{bottomstuff}

\maketitle

\section{Introduction}
The size of personal digital audio collections has been steadily
increasing due to a combination of factors including digital music
distribution, increases in storage capacity, advances in audio
compression and the wide popularity of portable digital music players
and phones with music playing capabilities. Effective interaction with
these large audio collections poses significant challenges to
traditional user interfaces. Portable players and music management
software typically allow users to select artist, genres or individual
tracks by essentially browsing long sortable lists of text. This mode
of interaction, although adequate for small music collections, becomes
increasingly problematic as collections become larger. A variety of
alternative ways of browsing music collections have been proposed in
the emerging area of Music Information Retrieval (MIR) which deals
with all aspects of managing, analyzing, and organizing music in
digital formats. They typically rely on a combination of audio signal
analysis to automatically extract features that describe the musical
content followed by visualization techniques to map the
high-dimensional feature space to a 2D or 3D representation that can
be used for browsing and navigation.

Tagging-based systems rely on users for categorizing objects by means
of tags (freely chosen words). Tags are aggregated from many users
forming ``folksonomies'' which, although not as accurate as
well-designed ontologies, have the advantage of reflecting how users
perceive the data and how their vocabulary and perception evolve over
time. Tagging is simple and does not require a lot of thinking. Tags
form an essential part of personalized internet radio and music
community websites such as
Last.fm \footnote{\url{http://www.last.fm}}. Tag clouds are the most
common way of visualizing tags. They are two-dimensional stylized
visual representations of a list of words where the more prominent
words are typically assigned a larger font. They are useful for
quickly giving users the gist of a set of words. Tag clouds are in
common usage on a number of different social networks, including
Flickr \footnote{\url{http://www.flickr.com}} ,
del.icio.us \footnote{\url{http://dilicius.com}} and
wordle \footnote{\url{http://www.wordle.net}}, but trace their origins
back at least 90 years to Soviet Constructivist art
\cite{viegas08}. The first true example may be that of a psychological
experiment where participants were asked to create a collective mental
map of landmarks in Paris \cite{milgram76}.  Later, tag clouds were
featured prominently in the book \textit{Microserfs}\cite{coupland95},
and first were introduced onto the web by Jim Flanagan in the program
``Search Referral Zeitgeist''.  However, it was the use of tag clouds
on the popular photo sharing site Flickr that made their use
ubiquitous on Web 2.0 sites \cite{brusilovsky96}.  Today, many social
websites use tag clouds as a way to make large quantities of data more
accessible and as a friendly interface for users.

Interacting with large music collections like most information
retrieval tasks involves both querying (or direct search) in which the
user has a well-defined search goal in mind as well as browsing (or
indirect search) in which the goal is to explore, with some degree of
serendipity, an information space. Summarization is the ability to
extract the gist of a collection without going into
details. Interfaces based on long sortable lists of text are effective
for querying but provide little support for browsing and especially
for finding music by artists that are not known to the user. In
contrast, content-aware visualization-based interfaces can be quite
effective for browsing, and music discovery but have weak support for
direct searching. Tag clouds provide both an overview of the
information space as well as direct search support. In order to
satisfy all these possibly conflicting user information needs a
straightforward solution would be to provide all these three different
ways of interacting with a music collection as separate
views/interface components that are coupled. The disadvantage of such
a design is that the user interface becomes unnecessary complicated
and confusing.

In this paper, we present content-aware self-organizing tag clouds a
technique that attempts to support querying, browsing, and
summarization using the familiar information model of a tag cloud. Tag
clouds are commonly ordered either alphabetically or randomly. In some
cases, tag clouds are ordered based on clustering using some kind of
tag similarity metric such as tag co-occurance. In other applications,
like wordle, users position each word in a tag cloud by hand.  In this
paper, we examine the utility and engagedness of placing similar
(based on content not co-occurance) tags next to each other using the
Self-Organizing Map (SOM) \cite{kohonen95a} algorithm.  Specifically,
we use techniques from Music Information Retrieval (MIR) to extract
high-dimensional feature vectors characterizing each song, and then
use the SOM algorithm to map these high dimensional feature vectors
onto coordinates on a discrete 2D grid. The tags are then placed by
using the centroid of the 2D grid coordinates of each set of songs
associated with a particular tag. A final post-processing step using
force-directed placement is utilized for better visual appearance and
overlap removal.

A proof-of-concept implementation in the music collection browsing
domain is described. Most existing music browsing interfaces proposed
in the literature are prototype systems that have not been evaluated
with user studies. This can be partly attributed to their publication
in other fields in which user evaluation is not as important. However,
it is also caused by the challenge of evaluating such interfaces due
to the highly subjective nature of music similarity. We tried to
address some of these challenges in the design of our user study and
we hope that the insights gained will be of value for future
research. Evaluation of the proposed interface shows that
self-organizing tag clouds can result in more effective browsing
especially for the case of music by artists that are unfamiliar to the
user. This is supported by reporting both quantitative results as well
as discussing qualitative reactions to the interface. To close, we
discuss lessons learned and directions for future work.

\section{Background}

\subsection{Tagging and Tag Clouds}

Tagging systems allow users to add keywords, or tags, to resources
without relying on a controlled vocabulary \cite{john06} and have
become ubiquitous in web-based systems.  It has been observed that
they have the potential to enhance many types of online interaction
and because of their free-form nature, they take advantage of the
underlying and pre-existing social organization of web communities
\cite{marlow06}.  While controlled ontologies and taxonomies hold the
promise of providing a regular and well-defined structure for
organizing knowledge, in practice this taxonomic rigidity becomes too
heavyweight and can stifle input and collaboration from user
communities. Tag clouds are one of the most common methods of
visualizing tag information.

There has been considerable research in recent years into the design,
use and effectiveness of tag clouds.  The Dogear system
\cite{millen06}, uses tags to organize social bookmarks for large
enterprise organizations.  A system that uses tags in an eCommerce
application is described in Ganesan et al. \cite{ganesan08}, and has
the feature that it automatically mines tags from feedback and
presents the results in a visually appealing manner.  In Halvey and
Keane \cite{halvey07}, a variety of tag presentation techniques are
evaluated, and show that the use of different techniques can affect
the ease with which users can find tags.  A historical look at tag
clouds is presented in Viegas and Wattenburg \cite{viegas08}, which
looks at the development of tag clouds since their inception a decade
ago, and speculates about their development in the future.  A novel
way of determining the size of tags in a tag cloud by examining the
entropy of the tag, which is then related to the emotional impact of
the tag, is presented in Eda et al. \cite{eda09}.  Another study
examined tag clouds derived from Automatic Speech Recognition (ASR) as
surrogates for tag clouds generated by human listeners and found that
the ASR determined tag clouds perform equally well \cite{tsagkias08}.
In the paper ``Seeing things in clouds'' \cite{bateman08}, an
extensive evaluation of different types of visual features in tag
clouds, including font size, font weight, intensity, number of
characters and area were investigated, and while font size and font
weight had the largest impact, when multiple variables were changed at
once, no one property stood out amongst the others. Tag navigation in
general has been examined in detail with particular focus on
``last.fm'', an online social community for music \cite{mesnage09}.
The Qtag \cite{lee07} system investigates collaborative tagging in the
domains of Information Filtering and Information Retrieval and
presents a tag visualization model \cite{lee07}.  A context aware
browser for mobile devices that uses tag clouds is presented in
Mizzaro et al. \cite{mizzaro09}.  A recent paper investigates semantic
tag clouds \cite{schrammel09} and comes to the conclusion that around
half the users prefer semantically constructed tag clouds.  Another
article by Lohmann et al. \cite{lohmann09} presents results of a
comparative study that looked at a variety of tag layouts, and came
to the conclusion that different applications need different tag
clouds, and that information designers should carefully consider which
is the best tag cloud for their particular problem.

Music databases are typically very large, containing thousands to
millions of songs, and the number of users interested in listening to
and browsing music is similarly large.  The effectiveness of browsing
large scale social annotations has been examined using the Effective
Large Scale Annotation Browser (ELSABer) algorithm \cite{li07}.
Another paper describes the Topiography system, a visualization for
large scale tag cloud \cite{fujimura08}.

More artistic applications of tagging and tag clouds have also been
explored, one of these is ArsMeteo \cite{acotto09}, a Web 2.0 portal
that allows users to collect and share digital artworks, including
videos, pictures and music. Tag-based retrieval of video content has
been explored using a variety of tag sources including social tags,
professional metadata and automatically generated metadata
\cite{melenhorst08}. The paper ``Your place or mine?'' \cite{danis08}
explores the Many Eyes website, a place that allows for collaborative
visualization of datasets, and examines the themes that reoccur across
various scenarios of the use of the data in the visualizations.

There are a large number of papers that focus on the application of
various mathematical techniques to tagging in general, of which we
have chosen only two as a very small representative sample.  The
Folksoviz \cite{lee08} project uses a statistical method for deriving
subsumption relationships based on the frequency of tags in Wikipedia
texts and also uses the Tag Sense Disambiguation (TSD) method for
mapping each tag to a Wikipedia article.

An exploration of information seeking in the socio-semantic web shows
how items can be viewed semiotically depending on tags, topics and
points of view \cite{cahier07}.

\subsection{Music Collection Browsing Interfaces}


Currently the most common interfaces for browsing music collections
such as iTunes by Apple are based on long sortable lists of
text. Although effective for direct searching they provide limited
support for music discovery and exploration. In the field of Music
Information Retrieval, data of high dimensionality and of considerable
complexity is generated. Various visualization interfaces have been
proposed to make this data accessible and useful to users. Frequently
these interfaces rely on automatically extracted audio features.

Islands of Music \cite{pampalk03} is an example of such a
visualization of audio information which uses Self-Organizing Maps to
generate a two-dimensional representation of a collection of music.
MusiCream \cite{goto05musicream} is an interface that allows users to
interact with a music collection using a dynamic visualization
interface.  MusicRainbow \cite{goto06musicRainbow} is a similar system
that uses web-based labelling and audio similarity to visualize music
collections.  Another relevant system is MusicSun \cite{PampalkGoto07}
which combines three different similarity measures to generate music
recommendations for users.  The Databionic/MusicMiner system
\cite{morchen05} allows users to organize large collections of music
and employs Emergent Self-Organizing Maps to generate visualizations
of the data involved.  A very large web based system for helping users
find new music is part of the Last.fm website
\url{http://playground.last.fm/iom} which provides advanced
functionality for music recommendation and visualization based on a
self-organized map calculated solely based on tag data. A simplified
2D grid representation with no text support based on audio content
analysis has been proposed for assistive music browsing
\cite{tzanetakis09}.

Conceptually, the closest work to our approach is Salonen that also
describes tags clouds using self-organizing maps \cite{salonen07}. The
two main differences from our work are 1) tag instead of content
information is used for training the SOM 2) the lack of user
evaluation. A 2006 review of visualization in audio based music
information retrieval can be found in Cooper \cite{cooper06}. Examples
of visualizations for music discovery in commercial and research
systems can be found in the Visualizing Music
blog \footnote{\url{http://visualizingmusic.com/}}.


\subsection{Motivation and Design Goals} 

Music browsing and discovery in large digital collections is a
particularly interesting domain with unique challenges and
opportunities for interaction design. It is an activity that many
computer users engage daily. As the primary goal is entertainment in
many cases the user can be satisfied with little effort. For example
most portable music players feature a shuffle button that just plays
random songs from a collection. It is highly unlikely that the user of
a text search engine would be in any way satisfied with such random
retrieval. At the same time, the notion of music similarity is highly
personal and subjective compared to relevance in other fields of
information retrieval. Music from unfamiliar styles or cultures is
typically perceived as sounding all the same by listeners and the same
pair of tracks might be considered similar by one listener and
dissimilar by another. As in most information retrieval tasks there is
a need for both querying (direct search) and browsing (indirect
search). However, in browsing, listeners frequently have only a vague
idea of what they want to hear so the ability to quickly and
effectively explore a large information space and discover new music
by unfamiliar artists is important.

The technique of content-aware self-organizing maps, proposed in this
paper, evolved over experimentation with the ideas and techniques
presented in the previous subsections and informal feedback through
participatory design activities.  It can be viewed as a fusion of
concepts from text-based visualization interfaces and more abstract
content-aware visualization interfaces. In order to motivate the
design goals we briefly mention some of the issues that users raised
when experimenting with various different interfaces for browsing
large music collections. A rough classification of existing systems
along two dimensions will be used to illustrate these issues. An
additional simplification used throughout this paper is the use of the
word tags to denote any textual data associated with a particular
music track. For example the genre of a song, the year of release, or
the artist can be viewed as tags (albeit with some constraints such as
that a track can only be associated with a single artist). Existing
systems can be characterized either as tag-based (using the more
generalized tag definition) or content-based. In addition they can be
characterized either as simple or complex. Simple interfaces have
minimum requirements in terms of screen real-estate and can be
navigated using simple mouse or even just keyboard interaction. In
contrast complex interfaces require large screen real-estate and
require complex user interactions and gestures to control. Existing
systems are combinations of these extremes.

Traditional complex tag-based systems based on long lists of sortable
text such as iTunes provide very little support for browsing,
discovery and summarization. An alternative is visualization
interfaces that are based on automatic analysis of musical content. By
mapping the music collection onto a visual 2D or 3D representation
they enable quick browsing and navigation especially in the case of
music that is not known to the user or that has not been
tagged. Simple content-based interfaces typically only provide tag
data once a particular track is selected
\cite{pampalk03,tzanetakis09,morchen05}. User quickly learn a mental
map of the representation (such as the lower corner of the display
contains mostly fast, energetic rap songs) but have trouble
understanding the display (for example a frequent question might be
what is the meaning of the x-axis or how are the tracks placed). Some
of the proposed content-based interfaces can be visually complex (for
example displaying hundreds of dots representing songs in a 3D space)
and require complex interactions such as 3D rotation and zooming
\cite{goto05musicream,goto06musicRainbow}.  Even though such
interfaces make great demos they are frustrating to use
regularly. Adding text/tags on the visualization further increases
complexity.

Tag-clouds provide a simple, familiar interface that partly overcomes
these limitations. For example they support both direct searching as
well as browsing and navigation. However they come with their own
problems. In order for a tag to assist search or browsing it is
necessary for the user to have some notion of its meaning. For example
a specialized term such as indie pop might be completely unfamiliar to
a particular listener while at the same time essential to
another. This problem becomes even more acute using the more
generalized notion of tags that includes information such as artist or
album.  As one of the goals for an effective interface of music
collection browsing is the discovery of new music by artists not known
to the listener this is an important disadvantage. Simple tag clouds
also do not provide the user with any information about the
connections and similarity relations between tags. More sophisticated
approaches rely on analyzing and visualizing tag similarity calculated
based on co-occurrence relations. A final problem with any system
based solely on tag information is that there is no way to access
music tracks that have not been tagged (the so-called cold start
problem). In contrast content-based visualizations allow any track to
be accessed and do not require familiarity with the music explored.

Based on these considerations, we identified five distinct design
goals for our music discovery interface:
\begin{itemize} 
\itemsep -0.1cm
\item{{\bf Simplicity:} Both the visual display and the user
  interaction should be simple, straightforward and familiar to
  users. The design should not hinder implementation on small
  displays, touch surfaces or general accessibility by users with
  special needs. Both direct and indirect search should be supported
  by the same user actions. }
 \item{{\bf Discovery:} The interface should support browsing,
   discovery and exploration of music not familiar to the user without
   this support affecting direct searching and retrieval. Tracks that
   have not been tagged should be integrated and accessible.}
\item{{\bf Consistency:} Frequently multiple views of a music
  collection are desired. For example a listener might want to see all
  the artists in a particular collection or playlist as well as all
  the associated tags. Using existing techniques the tag clouds
  generated for these two views (facets) would have no relation to
  each other. Although clustering approaches that rely on tag
  similarity based on co-occurance provide a more semantically
  meaningful layout they can not provide layout consistency among
  different facets.}
\item{{\bf Disambiguation:} Typically there is no imposed structure or
  consistency in tagging especially in a highly subjective fields such
  as music listening. Polysemy and synonyms are well known problems of
  tagging. For example some music tracks might be labeled with the tag
  ``female voice'' and some with the tag ``woman singing'' even though
  they essentially refer to the same type of musical content. It is
  likely users will use one or the other therefore tag similarity
  based on co-occurance will not provide any help. Furthermore
  frequently tags might be completely unfamiliar to a user. Such
  disambiguation problems can be addressed If tags are placed based on
  analyzing musical content. For example a listener unfamiliar with
  the tag ``motet'' should be able to use neighboring tags such as
  ``classical'' or ``vocal music'' to infer the meaning.  }
\end{itemize} 

Using these goals as requirements we propose a hybrid of text-based
and content-based approaches to music collection browsing that we term
content-aware self-organizing tags. In the following sections we
describe the various processing steps required to create the
visualization interface and present a proof-of-concept prototype
implementation. There is a lack of empirical data offering insights
into the design of music discovery interfaces. We present the results
of a user study evaluating our proposed design and discuss some of the
methodological issues we had to deal with.



\begin{figure*}[t]
\centering
\subfigure[Feature Extraction]
{
    \label{fig:sub1:featureExtraction}
    \includegraphics[width=40mm]{featureExtraction.pdf}
}
\hspace{1cm}
\subfigure[Self-Organizing Map] 
{
    \label{fig:sub1:som_grid}
    \includegraphics[width=45mm]{som_grid.pdf}
}
\\
\subfigure[Self-Organizing Tag Cloud before MID]
{
    \label{fig:sub1:som_tag_cloud_before_msd}
    \includegraphics[width=40mm]{som_tag_cloud_before_msd.pdf}
}
\hspace{1cm}
\subfigure[SOTC After MSD]
{
    \label{fig:sub1:som_tag_cloud}
    \includegraphics[width=40mm]{som_tag_cloud.pdf}
}
\label{fig:fig1}
\caption{Processing stages for creating a content-aware self-organizing tag cloud} 
\end{figure*}





\section{System Description}

We describe a new method for organizing music tag clouds that makes a
persistent map that takes into account the musical similarity between
songs. Figure 1 shows the various stages of the process of creating a
content-aware self-organizing tag cloud. The first step
(Fig. ~\ref{fig:sub1:featureExtraction}) uses techniques from the
field of Music Information Retrieval (MIR) to calculate a
high-dimensional feature vector representation for each track in the
music collection. Once all the feature vectors are calculated each
track is mapped onto a discrete position on a 2D grid using a
Self-Organizing Map (Fig. ~\ref{fig:sub1:som_grid}). Each generalized
tag is associated with a set of tracks that have been annotated with
it. As the tracks have been mapped to feature vectors and subsequently
to 2D grid coordinates each tag can be associated with a set of 2D
grid coordinates. The self-organized map process ensures that
neighboring points (tracks) will have similar high-dimensional audio
features and therefore similar musical content. In the third step the
tags are placed on the centroids of their corresponding set of 2D grid
coordinates. Their placement will reflect the underlying musical
content but results in visual overlap between them
(Fig. ~\ref{fig:sub1:som_tag_cloud_before_msd}). The final step
(Fig. ~\ref{fig:sub1:som_tag_cloud}) is applying a force-based layout
drawing algorithm to reduce overlap and result in a more aesthetically
pleasing tag cloud.  The font size for each tag was determined by
counting the number of instances of that tag.  Some tags had very few
instances, so a minimum font size of 10 points was used to make these
tags visible.


\subsection{Music Feature Extraction}


The goal of audio feature extraction is to represent each song in a
music collection as a single vector of features that characterize
musical content. Using suitable features, songs that ``sound'' similar
should have vectors that are ``close'' in the high dimensional feature
space.  First low-level features such as the Spectral Centroid,
Rolloff, Flux and the Mel-Frequency Cepstral Coefficients (MFCC) that
summarize information about the sound are computed approximately every
20 milliseconds. To capture the feature dynamics we compute a running
mean and standard deviation over the past M frames (the so-called
``texture window'' typically around 1 second). Figure
~\ref{fig:sub1:featureExtraction} shows this process of audio feature
extraction.

% \begin{eqnarray}
%   m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
%   s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)]
% \end{eqnarray}

% \noindent 

% where $\Phi(t)$ is the original feature vector. 

The results is a feature vector of 32 dimensions at the same rate as
the original 16D feature vector. The sequence of feature vectors is
collapsed into a single feature vector representing the entire audio
clip by taking again the mean and standard deviation across the 30
seconds (of the sequence of dynamics features) resulting in the final
64D feature vector per audio clip. A more detailed description of the
features and their motivation can be found in Tzanetakis and Cook
\cite{TC02b}. For the calculation of the self-organizing map described
in the next section all features are normalized so that the minimum of
each feature across the music collection is 0 and the maximum value is
1. This feature set has shown state-of-the-art performance in audio
retrieval and classification tasks for example in the Music
Information Retrieval Evaluation Exchange (MIREX) 2008 and was
computed using the free Marsyas audio processing
framework \footnote{\url{http://marsyas.sness.net}}. Most audio
feature sets proposed exhibit similar performance so we expect that
any audio feature front end can be used.



\subsection{Self-Organizing Maps}

For creating the visualization layout we utilized the self-organizing
map (SOM) which is a type of neural network used to map a high
dimensional input feature space to a lower dimensional representation
while preserving the topology of the high dimensional feature
space. This facilitates both similarity quantization and visualization
simultaneously. The SOM was first documented in 1982 by T. Kohonen,
and since then, it has been applied to a wide variety of diverse
clustering tasks \cite{kohonen95a}. In our system the SOM is used to
map the audio features (64-dimensions) to two discrete coordinates on
a rectangular grid.

The traditional SOM consists of a 2D grid of neural nodes each
containing an $n$-dimensional vector, $ {\bf x(t)} $ of data. The goal
of learning in the SOM is to cause different neighbouring parts of the
network to respond similarly to certain input patterns. This is partly
motivated by how visual, auditory and other sensory information is
handled in separate parts of the cerebral cortex in the human
brain. The network must be fed a large number of example vectors that
represent, as closely as possible, the kinds of vectors expected
during mapping. The data associated with each node is initialized to
small random values before training. During training, a series of
$n$-dimensional vectors of sample data are added to the map.  The
``winning'' node of the map known as the {\it best matching unit}
(BMU) is found by computing the distance between the added training
vector and each of the nodes in the SOM. This distance is calculated
according to some pre-defined distance metric which in our case is the
standard Euclidean distance on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding nodes
reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. The time-varying
learning rate and neighborhood function allow the SOM to gradually
converge and form clusters at different granularities. Once a SOM has
been trained, data may be added to the map simply by locating the node
whose data is most similar to that of the presented sample,
\textit{i.e.} the winner.  The reorganization phase is omitted when
the SOM is not in the training mode.


% The update formula for a neuron with representative vector N(t) can be
% written as follows:
% \begin{equation}
%     {\bf N}(t + 1) = {\bf N}(t) + \Theta(v,t) \alpha(t) ({\bf x}(t) -
%     {\bf N}(t))
% \end{equation}

% where $\alpha(t)$ is a monotonically decreasing learning coefficient and $x(t)$ is the input vector. 


% The neighborhood function $\Theta(v,t)$ depends on the lattice distance between the BMU and neuron v. 



% In our implementation, $\alpha(t)$ is a linearly-decaying function with $t$. 


\subsection{Self-organizing Tag Clouds}

Once the self-organized map of tracks is created each track is mapped
to a set of 2D coordinates $(x,y)$. The centroid of this set of 2D
coordinates is used as the position of the corresponding tag. To
generate the content-aware self-organized tag cloud we iterate over
each of the song in the collection and place it using the
centroid. Figure ~\ref{fig:sub1:som_grid} shows how the
self-organizing map can move tags using an underlying content
space. The top of the figure shows a random distribution of songs for
the tags ``Flute'' and ``Electronic''. As can be seen after the SOM
processing the songs corresponding to each tag are clustered although
there is some overlap (for example an electronic piece might contain
flute) and the corresponding tag positions are moved.

This initial layout contains many overlapping words, so the position
of each tag is repositioned using a mass, spring and damper
force-based algorithm for drawing \cite{ellson01}.  In our
implementation each tag is anchored to its original position using a
spring and an electrostatic-like force is applied between every pair
of tags that is proportional to the inverse of their squared distance.
Therefore tags that are close and overlapping will be pushed away
while still trying to remain close to their original location.  An
additional wall force term was added to keep all tags within the
designated window. Figures
~\ref{fig:sub1:som_tag_cloud_before_msd},\ref{fig:sub1:som_tag_cloud}
shows the self-organizing tag cloud before and after applying the
Mass-Spring damper algorithm.

\begin{figure*}[htb]
\centering
\includegraphics[width=120mm]{playtagnow_interface}
\caption{Play Tag Now! Interface} 
\label{fig:playtagnow_interface} 
\end{figure*} 

In order to present this data to users, we designed an interactive
web-based interface utilized multiple facets and synchronized tags
clouds \cite{hernandez08}. Three views/panes that are coupled are
provided: 1) a track view 2) an artist view and 3) a tag view. The
interface is shown in Figure \ref{fig:playtagnow_interface}.  On the
right is the tag pane, and in this figure the tags are organized
according to the Self-Organizing Map.  In the center is the artist
pane, which shows the names of the artists that created the tracks,
and the songs are shown in the left hand pane.  Because there are many
more tags than can be shown in either the tag pane or the song pane,
only a small subset of the tags are displayed at any one time, and
above each pane is a ``Shake'' button which selects a different random
subset of tags to show the user. The display area is partitioned into
a 5x5 grid and tags in each subgrid are rotated during each ``Shake''.
In Figure \ref{fig:playtagnow_interface}, the user has clicked on the
``Flute'' tag in the tag window, which then displays all the songs
that have the tag ``Flute'' associated with them in orange.  To give
the user a feeling of the overall song organization, a subset of
tracks is shown in black in the track pane.

\section{Evaluation}

\begin{figure*}[t]
\centering
\subfigure[Random Tag Cloud]
{
    \label{fig:sub:random_tag_cloud}
    \includegraphics[width=45mm]{random_tag_cloud.pdf}
}
\hspace{1cm}
\subfigure[alphabetical tag cloud]
{
    \label{fig:sub:alphabetical_tag_cloud}
    \includegraphics[width=45mm]{alphabetical_tag_cloud.pdf}
}
\hspace{1cm}
\subfigure[SOM After MSD]
{
    \label{fig:sub:som_tag_cloudrock}
    \includegraphics[width=45mm]{som_tag_cloud.pdf}
}
\caption{Tag-cloud configurations}
\label{fig:configurations}
\end{figure*}


The goal of the evaluation was to compare the effectiveness of three
different ways of tag clouds visualization: random placement,
alphabetical layout, and the self-organized tag clouds based on music
content similarity proposed in this paper. Two other configurations we
considered evaluating were: sortable lists of text (similar to the
interface of iTunes by Apple) and self-organized tag clouds based on
tag co-occurance.  We decided to not include sortable lists of text in
the evaluation as it would be unclear whether any observed difference
are caused by the use of self-organization or by simple preference for
lists over tag clouds. Given the popularity of tag clouds as a
representation for text meta-data we believe they are a viable
alternative or compliment to the existing sortable lists of text
interfaces.  The problem with tag co-occurance as a measure of tag
similarity is that it only works for actual free-form tags but not for
artists and song titles (the generalized notion of tags). One of the
main advantage of the proposed self-organizing tag clouds is that the
various views (song, artist, tag) are all related topologically as
they share the same underlying structure of the self-organizing
map. In addition the proposed approach still works for ``new'' tags
for which the co-occurance value can be low.  Figure
~\ref{fig:configurations} shows these three configurations.

The evaluation of music browsing and discovery interfaces is
challenging. Frequently the effectiveness of the interface is
evaluated indirectly through specific tasks which tend to be more
related to directed search. Examples of such tasks include find a song
of a particular genre or artist or using the interface to create a
playlist. One methodological problem with such tasks is that the
stopping condition from the task is dictated by the experiment
designer and does not take into account individual patterns of
usage. Although the artificial nature of such tasks for evaluation is
to some extent unavoidable they can be designed to be more open and
flexible. Throughout our participatory design process and pilot study
we observed that users tend to have very different behaviors when
interacting with large collections. Some users are easily satisfied
with a quick match that approximately corresponds to what they are
looking for while others spend considerable time refining and
narrowing their search in order to find a much more tighter match.

In order to accommodate this variety in usage patterns we decided to
let the users specify the stopping criterion rather than the
experimenter. For all tasks the users were asked to indicate when they
locate a music track that was similar to the provided query according
to them. In contrast other user studies frequently ask the user to
keep looking for tracks until the ``correct'' answer is found and
measure the time to complete such task. In our opinion our approach
provides a more valid assessment of browsing effectiveness across
different usage patterns but has the unfortunate side effect of
largely varying task completion times among users. Therefore in order
to compare the three configuration across different users we
normalized the task completion times. For each task the ``slowest''
configuration was used to normalize the task completion times for a
particular user. As an example a user with task completion times $(A:
60, B: 14, C:30)$ in seconds would have normalized task completion
times of $(A:1, B:0.23, C:05) $ which can be interpreted that the time
to complete the task using configuration B was 0.23 faster than using
configuration A (or 14 seconds for configuration B compared to 60
seconds for configuration A).

When examining the results of the user study, we found that the
average time to complete the tasks varied widely between different
users.  The time to complete all the tasks averaged across all the
participants was 436 seconds with a standard deviation of 224.35
seconds.  The shortest time to complete all the tasks was 202 seconds,
and the longest time was 1162 seconds.  The times did not follow a
normal distribution curve.

Another issue we had to wrestle with was whether the configuration
used was known to the users. The problem was that unless the users
were aware of the underlying content-based mapping they would not
expect similar tags to be located near each other. Therefore we
decided that the users would see which configuration was used each
time. Given that the differences between each configuration are
obvious anyway we don't think that this choice affected our
results. We collected both quantitative data such as task completion
times as well as qualitative data. The following sections describe the
experimental setup and results.


\subsection{Experimental Setup}


Fourteen participants were recruited from graduate Computer Science
students.  Three were female and 11 were male. All subjects had normal
or corrected-to-normal vision, enjoyed listening to music and were
experienced computer users. None of the participants had previous
knowledge of the Magnatune dataset.


The Magnatagatune dataset is a new dataset for MIR applications that contains a large collection of songs that has associated tags generated by users. It contains songs from the Magnatunes record label that aims to treat both artists and customers fairly by releasing music under permissive licences.  Magnatunes made available a large number of songs to the scientific community for use in research.  The Tagatune game \cite{law09} is a new game-with-a-purpose \cite{vonahn08} in which two users are both presented with a song.  Both users are then asked to guess what tag the other user would select for this song, and if both users agree, the tag is added to the song.  This tag is then not allowed to be used for this song by subsequent pairs of players.  The Magnatagatune dataset contains over 25,000 songs and over 180 of the most common tags derived from the Tagatune game and is freely available for research. \footnote{\url{http://tagatune.org/Magnatagatune.html}} One thousand clips were randomly chosen from the Magnatunes dataset and were placed on a self-organizing map using music feature similarity.  These 1000 clips represented 278 songs by 24 artists and have 188 different tags.  

Subjects were instructed in the use of the interface and were then asked to first practice with the interface for 5-10 minutes.  Subjects then performed each task in sequence.  During all tasks, subjects were encouraged to speak aloud and their comments were recorded along with the completion times for each task. After the experiments, users filled in a 5-point System Usability Survey (SUS) and were interviewed. 



\subsection{Task 1}

In Task 1, subjects were asked the question: ``Play a classical music
track by clicking on the classical tag.''  After the corresponding tag
was clicked, a piece of classical music was played. The subjects were 
than asked to: ``Find another track that sounds
similar, according to you, using a different tag.'' and the time required 
was recorded. The mean and standard deviations of response times are detailed in
Table \ref{table:task1}.  From this table we can see that the SOM
condition had the lowest mean normalized time, of 0.48, which is
considerably better than the mean normalized times of either the
Random or Alphabetical conditions. A two-way between-subjects ANOVA was conducted. 
There was no significant effect of tag cloud configuration for this task (F(2,39)=2.657, p=0.083$>$0.05).
This task is more representative of direct searching and therefore participants did not need 
to utilize the underlying representation. For example a user could locate the ``Baroque'' tag 
visually in any configuration and know that it probably contains similar tracks. 

\begin{table}
\centering
\caption{Task 1}
\begin{tabular}{ccc} \hline
Sorting & Mean & SE \\  \hline
Random  &  0.73 & 0.33\\
SOM &  0.48 & 0.34 \\
Alphabetical  & 0.73 & 0.32 \\ \hline
\end{tabular}
\label{table:task1}
\end{table}



\subsection{Task 2}

In Task 2, subjects were asked the question: ``Play Asteria by
clicking on Asteria in the artists pane.''  After the corresponding
tag was clicked, a piece of music by the artist Asteria was
played. The subjects were asked to: ``Find another track that sounds
similar, according to you, using a different artist.'' and the
response time was recorded. The subject was then asked to ``Find
another track that sounds similar, according to you, using a tag.''

The mean and standard deviations of response times are detailed in
Table \ref{table:task2}.  From this table we can see that for Task 2a
the SOM condition had the lowest mean normalized time, of 0.4, which
is considerably better than the mean normalized times of either the
Random or Alphabetical conditions.  A two-way between-subjects ANOVA
was conducted showing that this result was statistically significant
(F(2,39)=12.489, p$<$0.001).  For Task 2b, the SOM condition also had
the lowest mean normalized time (0.42), which was considerably better
than the mean normalized times of either the Random or Alphabetical
conditions.  We also found that this result was statistically
significant (F(2,39)=3.525, p$<$0.05). This task was the most
representative of browsing unfamiliar music as the participants had no
knowledge of the artists involved. It also illustrates the importance
of visual consistency in different facets. For example as can be seen
in Figure ~\ref{fig:playtagnow_interface} the tags ``Monks'' and
``Opera'' are probably relevant for the artists ``Asteria'' and
``Ensemble Sreteniye''.

\begin{table}
\centering
\caption{Task 2}
\begin{tabular}{cccc} \hline
& Sorting & Mean & SE \\  \hline
Task 2a & &  \\ 
& Random  & 0.9  & 0.23 \\
& SOM & 0.4 & 0.28  \\
& Alphabetical & 0.53 & 0.31  \\ \hline
Task 2b & & & \\
& Random & 0.70 & 0.34 \\
& SOM & 0.42 &  0.24 \\
& Alphabetical & 0.69 & 0.34\\ \hline
\end{tabular}
\label{table:task2}
\end{table}

\subsection{Task 3}

In Task 3, subjects were asked the question: ``In the next exercise,
I'll ask you to find a song that you enjoy using the interface in any
way you like.''  The response time was intentionally not recorded.
The subjects were then asked to ``Find another song that sounds
similar to your selection, according to you in any way you want using
the interface.'' as well as ``Find a song that sounds very different
to it, according to you.''. Both response times were recorded.

The mean and standard deviations of response times are detailed in
Table \ref{table:task3}.  From this table we can see that for Task 3a
the SOM condition had the lowest mean normalized time, of 0.52, which
is considerably better than the mean normalized times of either the
Random or Alphabetical conditions. A two-way between-subjects ANOVA
was conducted showing no statistically significant difference
(F(2,39)=1.039, p=0.363$>$0.05).  For Task 2b, the SOM condition also
had the lowest mean normalized time (0.42), which was considerably
better than the mean normalized times of either the Random or
Alphabetical conditions. This result was also not statistically
significant (F(2,39)=2.703, p=0.080$>$0.05). For similar reasons to
the ones described in Task 1 this can be attributed to knowledge of
tag semantics. At the same time it shows that there is no significant
penalty in any of these tasks by using the self-organizing tag cloud.

\begin{table}
\centering
\caption{Task 3}
\begin{tabular}{cccc} \hline
& Sorting & Mean & SE \\  \hline
Task 3a & & \\ 
& Random  & 0.68  & 0.33 \\
& SOM & 0.52 & 0.33 \\
& Alphabetical & 0.68  & 0.35 \\ \hline
Task 3b & & & \\
& Random  & 0.83 & 0.26 \\
& SOM & 0.60 & 0.29 \\
& Alphabetical & 0.65 & 0.31 \\ \hline
\end{tabular}
\label{table:task3}
\end{table}

\subsection{System Usability Survey}

After the conclusion of the three timed tasks, the participants were
asked to fill out a short System Usability Survey \cite{brooke96}
consisting of 6 questions, each rated on a 5 point scale, where ``1''
was labelled ``Strongly disagree'' and ``5'' was labelled ``Strongly
agree''. The 6 questions were as follows:


\begin{enumerate}
\itemsep -0.1cm
\item I thought the application was easy to use
\item I needed to learn a lot before I could accomplish tasks
with the application
\item I think people would need technical supported to be able
to learn how to use the application
\item I think most people would learn to user the application
very quickly
\item Overall, accomplishing tasks using the self-organizing map
was easier than with other methods
\item Overall, accomplishing tasks using the self-organizing map
was more fun than with other methods

\end{enumerate}

Results from survey are detailed in Table \ref{table:sus}. On average
users rated Question 4 highest, which indicated that they thought most
other people would be able to learn the application quickly. This
question also had the lowest variance.  In Table \ref{table:sus} we
detail all the responses from the participants, and we can see that 2
participants chose the middle check box, 6 chose the next one to the
right, and 6 chose the checkbox labelled ``Strongly agree''.

In a similar vein, participants also rated questions 5 and 6 highly,
although notably, two participants rated this question as one box to
the right of ``Strongly Disagree''.  This shows that certain users
found our interface facile to use and fit in well with their
expectations of an interface to explore music collections, but for
other users it did not.  Different people enjoy different ways of
interacting with media, some are more spatially oriented, and others
prefer to have options presented in a linear form.  In subsequent
versions of this application, we would like to explore the possibility
of using different visual design strategies to make this an enclusive
environment for a wide community of users.

For Question 2, the average response was 1.85, which means that on
average, users mostly strongly disagree that they would have to learn
a lot before accomplishing tasks with this application.  It is
important to include negative examples on such a user study to ensure
that participants are not just choosing answers to questions randomly,
and this question performs this control function.

\begin{table}
\centering
\caption{System Usability Survey}
\begin{tabular}{ccccccccc} 
\hline
Question & 1 & 2 & 3 & 4 & 5 & Mean & SE \\  \hline
\\ 
1 & 0 & 1 & 3 & 8 & 2 & 3.79  & 0.8   \\
2 & 5 & 7 & 1 & 1 & 0 & 1.86  & 0.86  \\
3 & 5 & 3 & 3 & 1 & 2 & 2.43  & 1.45  \\
4 & 0 & 0 & 2 & 6 & 6 & 4.29  & 0.73  \\
5 & 0 & 2 & 1 & 4 & 7 & 4.14  & 1.1   \\
6 & 0 & 2 & 0 & 6 & 6 & 4.14  & 1.03  \\
\\ \hline
\end{tabular}
\label{table:sus}
\end{table}

\begin{figure*}[htb]
\centering
\includegraphics[width=80mm]{sus_graph}
\caption{System Usability Survey} 
\label{fig:sus_graph} 
\end{figure*} 

\subsection{Interview}

We also carried out an interview with all the participants after the
SUS survey.  The participants were first asked which of the three
conditions they felt took the least amount of time to complete, and
were then asked which of the three conditions they found most fun.  We
then asked the participants to feel free to give us feedback on the
software and algorithms.

Of the 14 users, 10 users felt the Self-Organized Map condition was
the fastest, 2 users felt the random condition was the fastest, 1
felt the alphabetical condition was the fastest, and 1 expressed no
preference.  When asked which condition was the most fun, 9 users felt
the Self-Organized Map condition was the most fun, 2 users felt the
random condition was the most fun, 1 felt the alphabetical condition
was the most fun, and 2 expressed no preference.

\section{Conclusions and Future Directions}

In this paper we describe our investigations in designing an interface
for content-aware music browsing and discovery based on synchronized
self-organizing tag clouds. The experimental results show that
self-organizing tag clouds can result in more effective retrieval
especially in the case of browsing unknown artists and relating
different facets such as artists and tags. We also discuss evaluation
issues for music browsing interfaces. The proposed interface provides
a simple, consistent interface for music discovery that can easily be
adapted to small screen real-estate and touch surfaces.

There are many directions for future work. We are planning to explore
visualizing tag-based similarities as edges between tags with
proportional thickness. The combination of tag co-occurance similarity 
and content-based similarity should also be investigated. Possibilities 
include user control of how they are combined or separate visualization 
methods (for example position based on content and edge thickness 
based on co-occurence). Another interesting direction is the use of
more complex layout algorithms that take into account the shape of
words to approximate the aesthetic seen in manually created tag
clouds. Several of the user study participants suggested using the
same interface for tag annotation. Another interesting possibility is
the use of self-organizing tag clouds for collaborative music browsing
and comparison of collections between different listeners.  

We also plan to develop a more robust prototype as a plugin for existing music management programs in order to conduct a wider ethnographic study where self-organizing tag clouds are used in personal music collections. Finally, although we focus on music browsing in this work, we hope the ideas in this paper can be applied to any application domain where the underlying objects that are tagged can be automatically analyzed based on their content.

\section{Acknowledgements}

We thank the National Science and Research Council (NSERC) for providing part of 
the funding necessary to do this research.  We would also like to thank
all the participants in this study for their time and valuable comments.

\bibliographystyle{acmtrans} 
\bibliography{tomccap2010gtzan}
\end{document}

% LocalWords:  Polysemy occurance Flickr assistive Salonen
\documentclass{article}

\usepackage{times}
\usepackage{uist}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}

\begin{document}

% --- Copyright notice ---
\conferenceinfo{UIST'09}{October 4-7, 2009, Victoria, British Columbia, Canada}
\CopyrightYear{2009}
\crdata{978-1-60558-745-5/09/10}

% Uncomment the following line to hide the copyright notice
% \toappear{}
% ------------------------

\bibliographystyle{plain}

\title{Content-aware Music Discovery using \\ Faceted Self-Organizing Tag Clouds}


%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

\author{
\parbox[t]{9cm}{\centering
	     {\em Author One}\\
	     User Interface Laboratory\\
             ABC Corporation\\
	     1234 Anywhere Road\\
	     Anytown, NY 10027 USA\\
	     +1-212-555-1212\\
	     one@abc.com}
\parbox[t]{9cm}{\centering
	     {\em Author Two}\\
	     Universit\'{e} de XYZ\\
	     5678 rue des Parapluies\\
	     99099 Cr\`{e}me de Menthe, FRANCE\\
	     +33-12-34-56-78\\
	     deux@uvw.xyz.fr}
}

\maketitle

\abstract Personal digital music collections are continuously growing
and frequently feature thousands of tracks. Browsing and navigating
these large collections is challenging. The most common way of
interaction is using textual meta-data such as artist name or
genre. More recently tag folksonomies have also been utilized. A
variety of visualizations based on automatically analyzed musical
content have also been proposed.  Tag clouds are a two-dimensional
stylized visual representation of a list of words with different
visual design characteristics for each word. Tag clouds are commonly
ordered either alphabetically or randomly, and in this paper we
examine the utility and engageability of placing similar tags next to
each other.  In order to determine similarity of tags we use a
self-organizing map based on acoustical features derived from songs
rather than using the more common tag co-occurrence to measure
similarity.  This approach enables the display of multiple consistent
faceted self-organizing clouds where each facet shows a different type
of meta-data, be it artis, songs or tags. To evaluate the proposed
approach a subset of the Magnatune database tagged using the Targeting
game-with-a-purpose was utilized. Experimental results showing that
using a self-organizing tag cloud is both faster and more fun are
presented.

\classification{H5.2 [Information interfaces and presentation]:
User Interfaces. - Graphical user interfaces.}

\terms{Design, Human Factors}

\keywords{music, tags, self-organizing maps}

\tolerance=400 
  % makes some lines with lots of white space, but 	
  % tends to prevent words from sticking out in the margin

\section{Introduction}
The size of personal digital audio collections has been steadily
increasing due to a combination of factors including digital music
distribution, increases in storage capacity, advances in audio
compression and the wide popularity of portable digital music players
and phones with music playing capabilities. Effective interaction with
these large audio collections poses significant challenges to
traditional user interfaces. Portable players and music management
software typically allow users to select artist, genres or individual
tracks by essentially browsing long sortable lists of text. This mode
of interaction, although adequate for small music collections, becomes
increasingly problematic as collections become larger. A variety of
alternative ways of browsing music collections have been proposed in
the emerging area of Music Information Retrieval (MIR) which deals
with all aspects of managing, analyzing, and organizing music in
digital formats. They typically rely on a combination of audio signal
analysis to automatically extract features that describe the musical
content followed by visualization techniques to map the
high-dimensional feature space to a 2D or 3D representation that can
be used for browsing and navigation.

Tagging-based systems rely on users for categorizing objects by means
of tags (freely chosen words). Tags are aggregated from many users
forming ``folksonomies'' which, although not as accurate as
well-designed ontologies, have the advantage of reflecting how users
perceive the data and how their vocabulary and perception evolve over
time. Tagging is simple and does not require a lot of thinking. Tags
form an essential part of personalized internet radio and music
community websites such as Last.fm
\footnote{\url{http://www.last.fm}}. Tag clouds are the most common
way of visualizing tags. They are two-dimensional stylized visual
representations of a list of words where the more prominent words are
typically assigned a larger font. They are useful for quickly giving
users the gist of a set of words. Tag clouds trace their origins back
at least 90 years to Soviet Constructivist art \cite{viegas08}.  It
was the use of tag clouds on the popular photo sharing site Flickr
that made their use ubiquitous on Web 2.0 sites \cite{brusilovsky96}.
Today, many social websites use tag clouds as a way to make large
quantities of data more accessible and as a friendly interface for
users.

Interacting with large music collections like most information
retrieval tasks involves both querying (or direct search) in which the
user has a well-defined search goal in mind as well as browsing (or
indirect search) in which the goal is to explore, with some degree of
serendipity, an information space. Summarization is the ability to
extract the gist of a collection without going into
details. Interfaces based on long sortable lists of text are effective
for querying but provide little support for browsing and especially
for finding music by artists that are not known to the user. In
contrast, content-aware visualization interfaces are effective for
browsing, and music discovery but have weak support for direct
searching. Tag clouds provide both an overview of the information
space as well as direct search support.


% In order to
% satisfy all these possibly conflicting user information needs a
% straightforward solution would be to provide all these three different
% ways of interacting with a music collection as separate
% views/interface components that are coupled. The disadvantage of such
% a design is that the user interface becomes unnecessary complicated
% and confusing.

In this paper, we present content-aware self-organizing tag clouds a
technique that attempts to support querying, browsing, and
summarization using the familiar information model of a tag cloud. Tag
clouds are commonly ordered either alphabetically or randomly. In some
cases, tag clouds are ordered based on clustering using some kind of
tag similarity metric such as tag co-occurance. In other applications,
like wordle, users position each word in a tag cloud by hand.  In this
paper, we examine the utility and engagedness of placing similar
(based on content not co-occurance) tags next to each other using the
Self-Organizing Map (SOM) \cite{kohonen95a} algorithm.  Specifically,
we use techniques from Music Information Retrieval (MIR) to extract
high-dimensional feature vectors characterizing each song, and then
use the SOM algorithm to map these high dimensional feature vectors
onto coordinates on a discrete 2D grid. The tags are then placed by
using the centroid of the 2D grid coordinates of each set of songs
associated with a particular tag. A final post-processing step using
force-directed placement is utilized for better visual appearance and
overlap removal.

A proof-of-concept implementation in the music collection browsing
domain is described. Most existing music browsing interfaces proposed
in the literature are prototype systems that have not been evaluated
with user studies. This can be partly attributed to their publication
in other fields in which user evaluation is not as important. However,
it is also caused by the challenge of evaluating such interfaces due
to the highly subjective nature of music similarity. We tried to
address some of these challenges in the design of our user study and
we hope that the insights gained will be of value for future
research. Evaluation of the proposed interface shows that
self-organizing tag clouds can result in more effective browsing
especially for the case of music by artists that are unfamiliar to the
user. This is supported by reporting both quantitative results as well
as discussing qualitative reactions to the interface. To close, we
discuss lessons learned and directions for future work.

\section{Background}

\subsection{Tagging and Tag Clouds}

Tagging systems allow users to add keywords, or tags, to resources
without relying on a controlled vocabulary and have become ubiquitous
in web-based systems. While controlled ontologies and taxonomies hold
the promise of providing a regular and well-defined structure for
organizing knowledge, in practice this taxonomic rigidity becomes too
heavyweight and can stifle input and collaboration from user
communities. Tag clouds are one of the most common methods of
visualizing tag information.

There has been considerable research in recent years into the design,
use and effectiveness of tag clouds.  The Dogear system
\cite{millen06}, uses tags to organize social bookmarks for large
enterprise organizations. A historical look at tag clouds is presented
in Viegas and Wattenburg \cite{viegas08}, which looks at the
development of tag clouds since their use on the web a decade ago, and
speculates about their development in the future.  A novel way of
determining the size of tags in a tag cloud by examining the entropy
of the tag, which is then related to the emotional impact of the tag,
is presented in Eda et al. \cite{eda09}.  In the paper ``Seeing things
in clouds'' \cite{bateman08}, an extensive evaluation of different
types of visual features in tag clouds, including font size, font
weight, intensity, number of characters and area were
investigated. Tag navigation in general has been examined in detail
with particular focus on ``last.fm'', an online social community for
music \cite{mesnage09}.  A context aware browser for mobile devices
that uses tag clouds is presented in Mizzaro et al. \cite{mizzaro09}.
Another article by Lohmann et al. \cite{lohmann09} presents results of
a comparative study that looked at a variety of tag layouts, and came
to the conclusion that different applications need different tag
clouds, and that information designers should carefully consider which
is the best tag cloud for their particular problem.

More artistic applications of tagging and tag clouds have also been
explored, one of these is ArsMeteo \cite{acotto09}, a Web 2.0 portal
that allows users to collect and share digital artworks, including
videos, pictures and music. Tag-based retrieval of video content has
been explored using a variety of tag sources including social tags,
professional metadata and automatically generated metadata
\cite{melenhorst08}. There are a large number of papers that focus on
the application of various mathematical techniques to tagging in
general, of which we have chosen only two as a very small
representative sample.  The Folksoviz \cite{lee08} project uses a
statistical method for deriving subsumption relationships based on the
frequency of tags in Wikipedia texts and also uses the Tag Sense
Disambiguation (TSD) method for mapping each tag to a Wikipedia
article.


\subsection{Music Collection Browsing Interfaces}


Currently the most common interfaces for browsing music collections
such as iTunes by Apple are based on long sortable lists of
text. Although effective for direct searching they provide limited
support for music discovery and exploration. In the field of Music
Information Retrieval, data of high dimensionality is
generated. Various visualization interfaces have been proposed to make
this data accessible and useful to users. Frequently these interfaces
rely on automatically extracted audio features.

Islands of Music \cite{pampalk03} is an example of such a
visualization of audio information which uses Self-Organizing Maps to
generate a two-dimensional representation of a collection of music.
MusiCream \cite{goto05musicream} is an interface that allows users to
interact with a music collection using a dynamic visualization
interface.  MusicRainbow \cite{goto06musicRainbow} is a similar system
that uses web-based labelling and audio similarity to visualize music
collections.  Another relevant system is MusicSun \cite{PampalkGoto07}
which combines three different similarity measures to generate music
recommendations for users.  The Databionic/MusicMiner system
\cite{morchen05} allows users to organize large collections of music
and employs Emergent Self-Organizing Maps to generate visualizations
of the data involved.  A very large web based system for helping users
find new music is part of the Last.fm website
\url{http://playground.last.fm/iom} which provides advanced
functionality for music recommendation and visualization based on a
self-organized map calculated solely based on tag data. A simplified
2D grid representation with no text support based on audio content
analysis has been proposed for assistive music browsing
\cite{tzanetakis09}.

Conceptually, the closest work to our approach is Salonen that also
describes tags clouds using self-organizing maps \cite{salonen07}. The
two main differences from our work are 1) tag instead of content
information is used for training the SOM 2) the lack of user
evaluation. A 2006 review of visualization in audio based music
information retrieval can be found in Cooper \cite{cooper06}. Examples
of visualizations for music discovery in commercial and research
systems can be found in the Visualizing Music
blog \footnote{\url{http://visualizingmusic.com/}}.


\subsection{Motivation and Design Goals} 

Music browsing and discovery in large digital collections is a
particularly interesting domain with unique challenges and
opportunities for interaction design. It is an activity that many
computer users engage daily. As the primary goal is entertainment in
many cases the user can be satisfied with little effort. For example
most portable music players feature a shuffle button that just plays
random songs from a collection. It is highly unlikely that the user of
a text search engine would be in any way satisfied with such random
retrieval. At the same time, the notion of music similarity is highly
personal and subjective compared to relevance in other fields of
information retrieval. Music from unfamiliar styles or cultures is
typically perceived as sounding all the same by listeners and the same
pair of tracks might be considered similar by one listener and
dissimilar by another. As in most information retrieval tasks there is
a need for both querying (direct search) and browsing (indirect
search). However, in browsing, listeners frequently have only a vague
idea of what they want to hear so the ability to quickly and
effectively explore a large information space and discover new music
by unfamiliar artists is important.

The technique of content-aware self-organizing tag clouds, proposed in
this paper, evolved over experimentation with the ideas and techniques
presented in the previous subsections and informal feedback through
participatory design activities.  It can be viewed as a fusion of
concepts from text-based visualization interfaces and more abstract
content-aware visualization interfaces. In order to motivate the
design goals we briefly mention some of the issues that users raised
when experimenting with various different interfaces for browsing
large music collections. A rough classification of existing systems
along two dimensions will be used to illustrate these issues. An
additional simplification used throughout this paper is the use of the
word tag to denote any textual meta-data associated with a particular
music track. For example the genre of a song, the year of release, or
the artist can be viewed as tags (albeit with some constraints such as
that a track can only be associated with a single artist). Existing
systems can be characterized either as tag-based (using the more
generalized tag definition) or content-based. In addition they can be
characterized either as simple or complex. Simple interfaces have
minimum requirements in terms of screen real-estate and can be
navigated using simple mouse or even just keyboard interaction. In
contrast complex interfaces require large screen real-estate and
require complex user interactions and gestures to control. Existing
systems are combinations of these extremes.

Traditional complex tag-based systems based on long lists of sortable
text such as iTunes provide very little support for browsing,
discovery and summarization. An alternative is visualization
interfaces that are based on automatic analysis of musical content. By
mapping the music collection onto a visual 2D or 3D representation
they enable quick browsing especially in the case of music that is not
known to the user or that has not been tagged. Simple content-based
interfaces typically only provide tag data once a particular track is
selected \cite{pampalk03,tzanetakis09,morchen05}. User quickly learn a
mental map of the representation (such as the lower corner of the
display contains mostly fast, energetic rap songs) but have trouble
understanding the display (for example a frequent question is what is
the meaning of the x-axis or how are the tracks placed). Some of the
proposed content-based interfaces can be visually complex (for example
displaying hundreds of dots representing songs in a 3D space) and
require complex interactions such as 3D rotation and zooming
\cite{goto05musicream,goto06musicRainbow}.  Even though such
interfaces make great demos they are frustrating to use regularly.

Tag-clouds provide a simple, familiar interface that partly overcomes
these limitations. For example they support both direct searching as
well as browsing and navigation. However they come with their own
problems. In order for a tag to assist search or browsing it is
necessary for the user to have some notion of its meaning. For example
a specialized term such as indie pop might be unfamiliar to a
particular listener while at the same time essential to another. This
problem becomes more acute using the more generalized notion of
tags that includes information such as artist or album.  As one of the
goals for an effective interface of music collection browsing is the
discovery of new music by artists not known to the listener this is an
important disadvantage. Simple tag clouds also do not provide the user
with any information about the connections and similarity relations
between tags. More sophisticated approaches rely on analyzing and
visualizing tag similarity calculated based on co-occurrence
relations. A final problem with any system based solely on tag
information is that there is no way to access music tracks that have
not been tagged (the so-called ``cold start'' problem). In contrast
content-based visualizations allow any track to be accessed and do not
require familiarity with the music explored.

Based on these considerations, we identified five distinct design
goals for our music discovery interface:
\begin{itemize} 
\itemsep -0.1cm
\item{{\bf Simplicity:} Both the visual display and the user
  interaction should be simple, straightforward and familiar to
  users. The design should not hinder implementation on small
  displays, touch surfaces or general accessibility by users with
  special needs. Both direct and indirect search should be supported
  by the same user actions. }
\item{{\bf Discovery:} The interface should support browsing,
    discovery and exploration of music not familiar to the user
    without this support affecting direct searching and
    retrieval. Tracks that have not been tagged should be integrated
    to the interface and accessible.}
\item{{\bf Consistency:} Frequently multiple views of a music
    collection are desired. For example a listener might want to see
    all the artists in a particular collection or playlist as well as
    all the associated tags. Using existing techniques based on tag
    co-occurance the tag clouds generated for these two views (facets)
    would have no relation to each other. Although clustering
    approaches that rely on tag similarity based on co-occurance
    provide a more semantically meaningful layout they can not provide
    layout consistency among different facets and they don't work for
    the more generalized notition of tags that includes artist and
    track name}

\item{{\bf Disambiguation:} Typically there is no imposed structure or
  consistency in tagging especially in a highly subjective fields such
  as music listening. Polysemy and synonyms are well known problems of
  tagging. For example some music tracks might be labeled with the tag
  ``female voice'' and some with the tag ``woman singing'' even though
  they essentially refer to the same type of musical content. It is
  likely users will use one or the other therefore tag similarity
  based on co-occurance will not provide any help. Furthermore
  frequently tags might be completely unfamiliar to a user. Such
  disambiguation problems can be addressed If tags are placed based on
  analyzing musical content. For example a listener unfamiliar with
  the tag ``motet'' should be able to use neighboring tags such as
  ``classical'' or ``vocal music'' to infer the meaning.  }


\item{{\bf Personalization} Music listening is a personal activity and
    therefore ideally a music browsing interface should be adapted to
    the particular music taste of a person. Therefore the way content
    and meta-data information should depend on the particular 
    characteristics of the user collection.} 

\end{itemize} 

Using these goals as requirements we propose a hybrid of text-based
and content-based approaches to music collection browsing that we term
content-aware self-organizing tags. In the following sections we
describe the various processing steps required to create the
visualization interface and present a proof-of-concept prototype
implementation. There is a lack of empirical data offering insights
into the design of music discovery interfaces. We present the results
of a user study evaluating our proposed design and discuss some of the
methodological issues we had to deal with.



\begin{figure*}[t]
\centering
\subfigure[Feature Extraction]
{
    \label{fig:sub1:featureExtraction}
    \includegraphics[width=50mm]{featureExtraction.pdf}
}
\hspace{1cm}
\subfigure[Self-Organizing Map] 
{
    \label{fig:sub1:som_grid}
    \includegraphics[width=40mm]{som_grid.pdf}
}
\\
\subfigure[Self-Organizing Tag Cloud before MID]
{
    \label{fig:sub1:som_tag_cloud_before_msd}
    \includegraphics[width=40mm]{som_tag_cloud_before_msd.pdf}
}
\hspace{1cm}
\subfigure[SOTC After MSD]
{
    \label{fig:sub1:som_tag_cloud}
    \includegraphics[width=40mm]{som_tag_cloud.pdf}
}
\label{fig:fig1}
\caption{Processing stages for creating a content-aware self-organizing tag cloud} 
\end{figure*}





\section{System Description}

We describe a new method for organizing music tag clouds that makes a
persistent map that takes into account the musical similarity between
songs. Figure 1 shows the various stages of the process of creating a
content-aware self-organizing tag cloud. The first step
(Fig. ~\ref{fig:sub1:featureExtraction}) uses techniques from the
field of Music Information Retrieval (MIR) to calculate a
high-dimensional feature vector representation for each track in the
music collection. Once all the feature vectors are calculated each
track is mapped onto a discrete position on a 2D grid using a
Self-Organizing Map (Fig. ~\ref{fig:sub1:som_grid}). Each generalized
tag is associated with a set of tracks that have been annotated with
it. As the tracks have been mapped to feature vectors and subsequently
to 2D grid coordinates each tag can be associated with a set of 2D
grid coordinates. The self-organized map process ensures that
neighboring points (tracks) will have similar high-dimensional audio
features and therefore similar musical content. In the third step the
tags are placed on the centroids of their corresponding set of 2D grid
coordinates. Their placement will reflect the underlying musical
content but results in visual overlap between them
(Fig. ~\ref{fig:sub1:som_tag_cloud_before_msd}). The final step
(Fig. ~\ref{fig:sub1:som_tag_cloud}) is applying a force-based layout
drawing algorithm to reduce overlap and result in a more aesthetically
pleasing tag cloud.  The font size for each tag was determined by
counting the number of instances of that tag.  Some tags had very few
instances, so a minimum font size of 10 points was used to make these
tags visible.

There are some interesting characteristics of the resulting
visualization that we would like to highlight. The first is that tags
that are not correlated with the acoustical content will correspond to
tracks spread across the underlying self-organizing map and therefore
their placement will be in the center. For example in Figure 2 the
tags Male Singer, Singing and Female Vocal are near the center as they
have a large variety of tracks that have been annotated with them. In
contrast more specialized tags such as Heavy Metal or Monks are more
localized.  The second important characteristic is that faceted
browsing is naturally supported. For example an artist name, that the
user might not be familiar with, located near the left corner will
correspond to the tag Monks. Finally a track for which there are no
tag annotations will still be placed on the underlying self-organizing
map and that way receive an implicit visual automatic tag annotation
addressing to some extent the cold-start problem.


\subsection{Music Feature Extraction}


The goal of audio feature extraction is to represent each song in a
music collection as a single vector of features that characterize
musical content. Using suitable features, songs that ``sound'' similar
should have vectors that are ``close'' in the high dimensional feature
space.  First low-level features such as the Spectral Centroid,
Rolloff, Flux and the Mel-Frequency Cepstral Coefficients (MFCC) that
summarize information about the sound are computed approximately every
20 milliseconds. To capture the feature dynamics we compute a running
mean and standard deviation over the past M frames (the so-called
``texture window'' typically around 1 second). Figure
~\ref{fig:sub1:featureExtraction} shows this process of audio feature
extraction.

%\begin{eqnarray}
%   m\Phi(t) = mean[\Phi(t-M+1),..,\Phi(t)] \\
%   s\Phi(t) \; = \;\;\; std[\Phi(t-M+1),..,\Phi(t)]
% \end{eqnarray}

% \noindent 

% where $\Phi(t)$ is the original feature vector. 

The results is a feature vector of 32 dimensions at the same rate as
the original 16D feature vector. The sequence of feature vectors is
collapsed into a single feature vector representing the entire audio
clip by taking again the mean and standard deviation across the 30
seconds (of the sequence of dynamics features) resulting in the final
64D feature vector per audio clip. A more detailed description of the
features and their motivation can be found in Tzanetakis and Cook
\cite{TC02b}. For the calculation of the self-organizing map described
in the next section all features are normalized so that the minimum of
each feature across the music collection is 0 and the maximum value is
1. This feature set has shown state-of-the-art performance in audio
retrieval and classification tasks for example in the Music
Information Retrieval Evaluation Exchange (MIREX) 2008 and was
computed using the free Marsyas audio processing
framework \footnote{\url{http://marsyas.sness.net}}. Most audio
feature sets proposed exhibit similar performance so we expect that
any audio feature front end can be used.



\subsection{Self-Organizing Maps}

For creating the visualization layout we utilized the self-organizing
map (SOM) which is a type of neural network used to map a high
dimensional input feature space to a lower dimensional representation
while preserving the topology of the high dimensional feature
space. This facilitates both similarity quantization and visualization
simultaneously. The SOM was first documented in 1982 by T. Kohonen,
and since then, it has been applied to a wide variety of diverse
clustering tasks \cite{kohonen95a}. In our system the SOM is used to
map the audio features (64-dimensions) to two discrete coordinates on
a rectangular grid.

The traditional SOM consists of a 2D grid of neural nodes each
containing an $n$-dimensional vector, $ {\bf x(t)} $ of data. The goal
of learning in the SOM is to cause different neighbouring parts of the
network to respond similarly to certain input patterns. This is partly
motivated by how visual, auditory and other sensory information is
handled in separate parts of the cerebral cortex in the human
brain. The network must be fed a large number of example vectors that
represent, as closely as possible, the kinds of vectors expected
during mapping. The data associated with each node is initialized to
small random values before training. During training, a series of
$n$-dimensional vectors of sample data are added to the map.  The
``winning'' node of the map known as the {\it best matching unit}
(BMU) is found by computing the distance between the added training
vector and each of the nodes in the SOM. This distance is calculated
according to some pre-defined distance metric which in our case is the
standard Euclidean distance on the normalized feature vectors.

Once the winning node has been defined, it and its surrounding nodes
reorganize their vector data to more closely resemble the added
training sample.  The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are
adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. The time-varying
learning rate and neighborhood function allow the SOM to gradually
converge and form clusters at different granularities. Once a SOM has
been trained, data may be added to the map simply by locating the node
whose data is most similar to that of the presented sample,
\textit{i.e.} the winner.  The reorganization phase is omitted when
the SOM is not in the training mode.


% The update formula for a neuron with representative vector N(t) can be
% written as follows:
% \begin{equation}
%     {\bf N}(t + 1) = {\bf N}(t) + \Theta(v,t) \alpha(t) ({\bf x}(t) -
%     {\bf N}(t))
% \end{equation}

% where $\alpha(t)$ is a monotonically decreasing learning coefficient and $x(t)$ is the input vector. 


% The neighborhood function $\Theta(v,t)$ depends on the lattice distance between the BMU and neuron v. 



% In our implementation, $\alpha(t)$ is a linearly-decaying function with $t$. 


\subsection{Self-organizing Tag Clouds}

Once the self-organized map of tracks is created each track is mapped
to a set of 2D coordinates $(x,y)$. The centroid of this set of 2D
coordinates is used as the position of the corresponding tag. To
generate the content-aware self-organized tag cloud we iterate over
each of the song in the collection and place it using the
centroid. Figure ~\ref{fig:sub1:som_grid} shows how the
self-organizing map can move tags using an underlying content
space. The top of the figure shows a random distribution of songs for
the tags ``Flute'' and ``Electronic''. As can be seen after the SOM
processing the songs corresponding to each tag are clustered although
there is some overlap (for example an electronic piece might contain
flute) and the corresponding tag positions are moved.

This initial layout contains many overlapping words, so the position
of each tag is repositioned using a mass, spring and damper
force-based algorithm for drawing \cite{ellson01}.  In our
implementation each tag is anchored to its original position using a
spring and an electrostatic-like force is applied between every pair
of tags that is proportional to the inverse of their squared distance.
Therefore tags that are close and overlapping will be pushed away
while still trying to remain close to their original location.  An
additional wall force term was added to keep all tags within the
designated window. Figures
~\ref{fig:sub1:som_tag_cloud_before_msd},\ref{fig:sub1:som_tag_cloud}
shows the self-organizing tag cloud before and after applying the
Mass-Spring damper algorithm.

\begin{figure*}[htb]
\centering
\includegraphics[width=120mm]{playtagnow_interface}
\caption{Play Tag Now! Interface} 
\label{fig:playtagnow_interface} 
\end{figure*} 

In order to present this data to users, we designed an interactive
web-based interface utilized multiple facets and synchronized tags
clouds \cite{hernandez08}. Three views/panes that are coupled are
provided: 1) a track view 2) an artist view and 3) a tag view. The
interface is shown in Figure \ref{fig:playtagnow_interface}.  On the
right is the tag pane, and in this figure the tags are organized
according to the Self-Organizing Map.  In the center is the artist
pane, which shows the names of the artists that created the tracks,
and the songs are shown in the left hand pane.  Because there are many
more tags than can be shown in either the tag pane or the song pane,
only a small subset of the tags are displayed at any one time, and
above each pane is a ``Shake'' button which selects a different random
subset of tags to show the user. The display area is partitioned into
a 5x5 grid and tags in each subgrid are randomly selected during each ``Shake''.
In Figure \ref{fig:playtagnow_interface}, the user has clicked on the
``Flute'' tag in the tag window, which then displays all the songs
that have the tag ``Flute'' associated with them in orange.  To give
the user a feeling of the overall song organization, a subset of
tracks is shown in black in the track pane.

\section{Evaluation}


Before discussing  the quantitative and qualitative evaluation of the
interface we comment on how our proposed interface addresses the
design goals outlined in Section 2.1. The interface is simple and
straightforward and can easily adapt to small displays and touch
surfaces. It supports the discovery of new music even in the cases
where an artist is uknown or a the meaning of a tag is unclear. In
addition it supports access to tracks that have not been tagged. Of
course direct search is also supported. In terms of consistency all
faceted views are based on the same underlying content-based map 
so information from one facet can be used to inform browsing of
another facet. By incorporating content-based information the
interface can assist in the disambiguation of tags and indirectly 
deals with issues such as synonyms. Finally the self-organizing tag
cloud is based on a analyzing a particular music collection. Therefore
it provides a personalized view of the tag cloud compared to
approaches based on tag co-occurence. 


\begin{figure*}[t]
\centering
\subfigure[Random Tag Cloud]
{
    \label{fig:sub:random_tag_cloud}
    \includegraphics[width=45mm]{random_tag_cloud.pdf}
}
\hspace{1cm}
\subfigure[alphabetical tag cloud]
{
    \label{fig:sub:alphabetical_tag_cloud}
    \includegraphics[width=45mm]{alphabetical_tag_cloud.pdf}
}
\hspace{1cm}
\subfigure[SOM After MSD]
{
    \label{fig:sub:som_tag_cloudrock}
    \includegraphics[width=45mm]{som_tag_cloud.pdf}
}
\caption{Tag-cloud configurations}
\label{fig:configurations}
\end{figure*}


The goal of the evaluation was to compare the effectiveness of three
different ways of tag clouds visualization: random placement,
alphabetical layout, and the self-organized tag clouds based on music
content similarity proposed in this paper. Two other configurations we
considered evaluating were: sortable lists of text (similar to the
interface of iTunes by Apple) and self-organized tag clouds based on
tag co-occurance.  We decided to not include sortable lists of text in
the evaluation as it would be unclear whether any observed difference
are caused by the use of self-organization or by simple preference for
lists over tag clouds. Given the popularity of tag clouds as a
representation for text meta-data we believe they are a viable
alternative or compliment to the existing sortable lists of text
interfaces.  The problem with tag co-occurance as a measure of tag
similarity is that it only works for actual free-form tags but not for
artists and song titles (the generalized notion of tags). One of the
main advantage of the proposed self-organizing tag clouds is that the
various views (song, artist, tag) are all related topologically as
they share the same underlying structure of the self-organizing
map. In addition the proposed approach still works for ``new'' tags
for which the co-occurance value can be low.  Finally the tag
co-occurence numbers for the tags in the Magnatagatune collection were
not particularly informative. Figure ~\ref{fig:configurations} shows
the three configurations we evaluated. 

The evaluation of music browsing and discovery interfaces is
challenging. Frequently the effectiveness of the interface is
evaluated indirectly through specific tasks which tend to be more
related to directed search. Examples of such tasks include find a song
of a particular genre or artist or using the interface to create a
playlist. One methodological problem with such tasks is that the
stopping condition from the task is dictated by the experiment
designer and does not take into account individual patterns of
usage. Although the artificial nature of such tasks for evaluation is
to some extent unavoidable they can be designed to be more open and
flexible. Throughout our participatory design process and pilot study
we observed that users tend to have very different behaviors when
interacting with large collections. Some users are easily satisfied
with a quick match that approximately corresponds to what they are
looking for while others spend considerable time refining and
narrowing their search in order to find a much more tighter match.

In order to accommodate this variety in usage patterns we decided to
let the users specify the stopping criterion rather than the
experimenter. For all tasks the users were asked to indicate when they
locate a music track that was similar to the provided query according
to them. In contrast other user studies frequently ask the user to
keep looking for tracks until the ``correct'' answer is found and
measure the time to complete such task. In our opinion our approach
provides a more valid assessment of browsing effectiveness across
different usage patterns but has the unfortunate side effect of
largely varying task completion times among users. Therefore in order
to compare the three configuration across different users we
normalized the task completion times. For each task the ``slowest''
configuration was used to normalize the task completion times for a
particular user. As an example a user with task completion times $(A:
60, B: 14, C:30)$ in seconds would have normalized task completion
times of $(A:1, B:0.23, C:05) $ which can be interpreted that the time
to complete the task using configuration B was 0.23 faster than using
configuration A (or 14 seconds for configuration B compared to 60
seconds for configuration A).

When examining the results of the user study, we found that the
average time to complete the tasks varied widely between different
users.  The time to complete all the tasks averaged across all the
participants was 436 seconds with a standard deviation of 224.35
seconds.  The shortest time to complete all the tasks was 202 seconds,
and the longest time was 1162 seconds.  The times did not follow a
normal distribution curve.

Another issue we had to wrestle with was whether the configuration
used was known to the users. The problem was that unless the users
were aware of the underlying content-based mapping they would not
expect similar tags to be located near each other. Therefore we
decided that the users would see which configuration was used each
time. Given that the differences between each configuration are
obvious anyway we don't think that this choice affected our
results. We collected both quantitative data such as task completion
times as well as qualitative data.


\subsection{Experimental Setup}


Fourteen participants were recruited from graduate Computer Science
students.  Three were female and 11 were male. All subjects had normal
or corrected-to-normal vision, enjoyed listening to music and were
experienced computer users. None of the participants had previous
knowledge of the Magnatune dataset.


The Magnatagatune dataset is a new dataset for MIR applications that
contains a large collection of songs that has associated tags
generated by users. It contains songs from the Magnatunes record label
that aims to treat both artists and customers fairly by releasing
music under permissive licences.  Magnatunes made available a large
number of songs to the scientific community for use in research.  The
Tagatune game \cite{law09} is a new game-with-a-purpose
\cite{vonahn08} in which two users are both presented with a song.
Both users are then asked to guess what tag the other user would
select for this song, and if both users agree, the tag is added to the
song.  This tag is then not allowed to be used for this song by
subsequent pairs of players.  The Magnatagatune dataset contains over
25,000 songs and over 180 of the most common tags derived from the
Tagatune game and is freely available for
research. \footnote{\url{http://tagatune.org/Magnatagatune.html}} One
thousand clips were randomly chosen from the Magnatunes dataset and
were placed on a self-organizing map using music feature similarity.
These 1000 clips represented 278 songs by 24 artists and have 188
different tags.

Subjects were instructed in the use of the interface and were then
asked to first practice with the interface for 5-10 minutes.  Subjects
then performed each task in sequence.  During all tasks, subjects were
encouraged to speak aloud and their comments were recorded along with
the completion times for each task. After the experiments, users
filled in a 5-point System Usability Survey (SUS) and were
interviewed.



\subsection{Task 1}

In Task 1, subjects were asked the question: ``Play a classical music
track by clicking on the classical tag.''  After the corresponding tag
was clicked, a piece of classical music was played. The subjects were 
than asked to: ``Find another track that sounds
similar, according to you, using a different tag.'' and the time required 
was recorded. The mean and standard deviations of response times are detailed in
Table \ref{table:task1}.  From this table we can see that the SOM
condition had the lowest mean normalized time, of 0.48, which is
considerably better than the mean normalized times of either the
Random or Alphabetical conditions. A two-way between-subjects ANOVA was conducted. 
There was no significant effect of tag cloud configuration for this task (F(2,39)=2.657, p=0.083$>$0.05).
This task is more representative of direct searching and therefore participants did not need 
to utilize the underlying representation. For example a user could locate the ``Baroque'' tag 
visually in any configuration and know that it probably contains similar tracks. 

\begin{table}
\centering
\caption{Task 1}
\begin{tabular}{ccc} \hline
Sorting & Mean & SE \\  \hline
Random  &  0.73 & 0.33\\
SOM &  0.48 & 0.34 \\
Alphabetical  & 0.73 & 0.32 \\ \hline
\end{tabular}
\label{table:task1}
\end{table}



\subsection{Task 2}

In Task 2, subjects were asked the question: ``Play Asteria by
clicking on Asteria in the artists pane.''  After the corresponding
tag was clicked, a piece of music by the artist Asteria was
played. The subjects were asked to: ``Find another track that sounds
similar, according to you, using a different artist.'' and the
response time was recorded. The subject was then asked to ``Find
another track that sounds similar, according to you, using a tag.''

The mean and standard deviations of response times are detailed in
Table \ref{table:task2}.  From this table we can see that for Task 2a
the SOM condition had the lowest mean normalized time, of 0.4, which
is considerably better than the mean normalized times of either the
Random or Alphabetical conditions.  A two-way between-subjects ANOVA
was conducted showing that this result was statistically significant
(F(2,39)=12.489, p$<$0.001).  For Task 2b, the SOM condition also had
the lowest mean normalized time (0.42), which was considerably better
than the mean normalized times of either the Random or Alphabetical
conditions.  We also found that this result was statistically
significant (F(2,39)=3.525, p$<$0.05). This task was the most
representative of browsing unfamiliar music as the participants had no
knowledge of the artists involved. It also illustrates the importance
of visual consistency in different facets. For example as can be seen
in Figure ~\ref{fig:playtagnow_interface} the tags ``Monks'' and
``Opera'' are probably relevant for the artists ``Asteria'' and
``Ensemble Sreteniye''.

\begin{table}
\centering
\caption{Task 2}
\begin{tabular}{cccc} \hline
& Sorting & Mean & SE \\  \hline
Task 2a & &  \\ 
& Random  & 0.9  & 0.23 \\
& SOM & 0.4 & 0.28  \\
& Alphabetical & 0.53 & 0.31  \\ \hline
Task 2b & & & \\
& Random & 0.70 & 0.34 \\
& SOM & 0.42 &  0.24 \\
& Alphabetical & 0.69 & 0.34\\ \hline
\end{tabular}
\label{table:task2}
\end{table}

\subsection{Task 3}

In Task 3, subjects were asked the question: ``In the next exercise,
I'll ask you to find a song that you enjoy using the interface in any
way you like.''  The response time was intentionally not recorded.
The subjects were then asked to ``Find another song that sounds
similar to your selection, according to you in any way you want using
the interface.'' as well as ``Find a song that sounds very different
to it, according to you.''. Both response times were recorded.

The mean and standard deviations of response times are detailed in
Table \ref{table:task3}.  From this table we can see that for Task 3a
the SOM condition had the lowest mean normalized time, of 0.52, which
is considerably better than the mean normalized times of either the
Random or Alphabetical conditions. A two-way between-subjects ANOVA
was conducted showing no statistically significant difference
(F(2,39)=1.039, p=0.363$>$0.05).  For Task 2b, the SOM condition also
had the lowest mean normalized time (0.42), which was considerably
better than the mean normalized times of either the Random or
Alphabetical conditions. This result was also not statistically
significant (F(2,39)=2.703, p=0.080$>$0.05). For similar reasons to
the ones described in Task 1 this can be attributed to knowledge of
tag semantics. At the same time it indicates that there is no significant
penalty in any of these tasks by using the self-organizing tag cloud.

\begin{table}
\centering
\caption{Task 3}
\begin{tabular}{cccc} \hline
& Sorting & Mean & SE \\  \hline
Task 3a & & \\ 
& Random  & 0.68  & 0.33 \\
& SOM & 0.52 & 0.33 \\
& Alphabetical & 0.68  & 0.35 \\ \hline
Task 3b & & & \\
& Random  & 0.83 & 0.26 \\
& SOM & 0.60 & 0.29 \\
& Alphabetical & 0.65 & 0.31 \\ \hline
\end{tabular}
\label{table:task3}
\end{table}

\subsection{System Usability Survey}

After the conclusion of the three timed tasks, the participants were
asked to fill out a short System Usability Survey \cite{brooke96}
consisting of 6 questions, each rated on a 5 point scale, where ``1''
was labelled ``Strongly disagree'' and ``5'' was labelled ``Strongly
agree''. The 6 questions were as follows:


\begin{enumerate}
\itemsep -0.1cm
\item I thought the application was easy to use
\item I needed to learn a lot before I could accomplish tasks
with the application
\item I think people would need technical supported to be able
to learn how to use the application
\item I think most people would learn to user the application
very quickly
\item Overall, accomplishing tasks using the self-organizing map
was easier than with other methods
\item Overall, accomplishing tasks using the self-organizing map
was more fun than with other methods

\end{enumerate}

Results from survey are detailed in Table \ref{table:sus}. On average
users rated Question 4 highest, which indicated that they thought most
other people would be able to learn the application quickly. This
question also had the lowest variance.  In Table \ref{table:sus} we
detail all the responses from the participants, and we can see that 2
participants chose the middle check box, 6 chose the next one to the
right, and 6 chose the checkbox labelled ``Strongly agree''.

In a similar vein, participants also rated questions 5 and 6 highly,
although notably, two participants rated this question as one box to
the right of ``Strongly Disagree''.  This shows that certain users
found our interface facile to use and fit in well with their
expectations of an interface to explore music collections, but for
other users it did not.  Different people enjoy different ways of
interacting with media, some are more spatially oriented, and others
prefer to have options presented in a linear form.  In subsequent
versions of this application, we would like to explore the possibility
of using different visual design strategies to make this an enclusive
environment for a wide community of users.

For Question 2, the average response was 1.85, which means that on
average, users mostly strongly disagree that they would have to learn
a lot before accomplishing tasks with this application.  It is
important to include negative examples on such a user study to ensure
that participants are not just choosing answers to questions randomly,
and this question performs this control function.

\begin{table}
\centering
\caption{System Usability Survey}
\begin{tabular}{ccccccccc} 
\hline
Question & 1 & 2 & 3 & 4 & 5 & Mean & SE \\  \hline
\\ 
1 & 0 & 1 & 3 & 8 & 2 & 3.79  & 0.8   \\
2 & 5 & 7 & 1 & 1 & 0 & 1.86  & 0.86  \\
3 & 5 & 3 & 3 & 1 & 2 & 2.43  & 1.45  \\
4 & 0 & 0 & 2 & 6 & 6 & 4.29  & 0.73  \\
5 & 0 & 2 & 1 & 4 & 7 & 4.14  & 1.1   \\
6 & 0 & 2 & 0 & 6 & 6 & 4.14  & 1.03  \\
\\ \hline
\end{tabular}
\label{table:sus}
\end{table}

\begin{figure*}[htb]
\centering
\includegraphics[width=80mm]{sus_graph}
\caption{System Usability Survey} 
\label{fig:sus_graph} 
\end{figure*} 

We also carried out an interview with all the participants after the
SUS survey.  The participants were first asked which of the three
conditions they felt took the least amount of time to complete, and
were then asked which of the three conditions they found most fun.  We
then asked the participants to feel free to give us feedback on the
software and algorithms.

Of the 14 users, 10 users felt the Self-Organized Map condition was
the fastest, 2 users felt the random condition was the fastest, 1
felt the alphabetical condition was the fastest, and 1 expressed no
preference.  When asked which condition was the most fun, 9 users felt
the Self-Organized Map condition was the most fun, 2 users felt the
random condition was the most fun, 1 felt the alphabetical condition
was the most fun, and 2 expressed no preference.

\section{Conclusions and Future Directions}

In this paper we describe our investigations in designing an interface
for content-aware music browsing and discovery based on synchronized
self-organizing tag clouds. The experimental results show that
self-organizing tag clouds can result in more effective retrieval
especially in the case of browsing unknown artists and relating
different facets such as artists and tags. We also discuss evaluation
issues for music browsing interfaces. The proposed interface provides
a simple, consistent interface for music discovery that can easily be
adapted to small screen real-estate and touch surfaces.

There are many directions for future work. We are planning to explore
visualizing tag-based similarities as edges between tags with
proportional thickness. The combination of tag co-occurance similarity 
and content-based similarity should also be investigated. Possibilities 
include user control of how they are combined or separate visualization 
methods (for example position based on content and edge thickness 
based on co-occurence). Another interesting direction is the use of
more complex layout algorithms that take into account the shape of
words to approximate the aesthetic seen in manually created tag
clouds. Several of the user study participants suggested using the
same interface for tag annotation. Another interesting possibility is
the use of self-organizing tag clouds for collaborative music browsing
and comparison of collections between different listeners.  

We also plan to develop a more robust prototype as a plugin for
existing music management programs in order to conduct a wider
ethnographic study where self-organizing tag clouds are used in
personal music collections. Finally, although we focus on music
browsing in this work, we hope the ideas in this paper can be applied
to any application domain where the underlying objects that are tagged
can be automatically analyzed based on their content.

% \section{Acknowledgements}

% We thank the National Science and Research Council (NSERC) for providing part of 
% the funding necessary to do this research.  We would also like to thank
% all the participants in this study for their time and valuable comments.

\bibliography{uist2010gtzan}

\end{document}
