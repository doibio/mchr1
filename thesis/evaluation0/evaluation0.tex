
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter - Expert Interface Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startchapter{Expert Interface Evaluation}
\label{chap:expertinterfaceevaluation}

In order to measure the effectiveness of the part of the Orchive that
was designed for experts in orca vocalizations, there are several
relevant methods that could be used.

The first is to use quantitative methods and to look at the total
number of annotations that were made, and the behaviour of the user
making the annotations.  After 5 years of the site having run, I have
collected 17,000 annotations.  These annotations were created by 12
users, these experts were recruited primarily from researchers
involved with OrcaLab who had extensive experience in listening to the
vocalizations of orcas.  The total number of annotations per user can
be seen in Table \ref{table:expert-annotations} and were collected
over a period of 5 years.  From this table we can see that 3 users are
mostly responsible for annotating the Orchive.  Six are at a level of
between 500-1500 annotations, and four of them have only a few
annotations.  Clearly, from the high use rates from these three users
shows that with persistence the interface can be engaging.  However,
the low annotation rates for most of the users shows that improvements
can be made in making the interface more engaging.

It is of interest that the annotators with the most annotations were
developers of bioacoustic algorithms.  The one with the third most
annotations, and one other researcher with 1223 annotations, were
doing active research on orca vocalizations.  The rest were doing it
purely as a volunteer activity and often put considerable time into
the activity.

\begin{table}
\begin{tabular}{|c|c|}
\hline
User ID & Number Annotations \\
\hline
12  & \textbf{7864} \\
11  & 10 \\
10  & 476 \\
9  &  616 \\
8  &  4272 \\
7  &  72 \\
6  &  51 \\
5  &  929 \\
4  &  5048 \\
3  &  1 \\
2  &  1223 \\
1  &  1552 \\
\hline
\end{tabular}
\caption{A table showing the total number of annotations that each of
  the 12 users on the Orchive V1.0 site created.  Note the wide
  variation between the user with the most annotations (7864) and the
  lowest (1).}
\label{table:expert-annotations}
\end{table}

Another approach that was taken to study the expert interface was to
email the annotators and ask them what they thought of the Orchive
interface.  The results from this field study were very interesting
and encouraging.  One of the respondents said:

\begin{quote}
`` I wish I had this when I was doing my Ph.D.  It would have made my
  research much easier.''
\end{quote}

another said:

\begin{quote}
`` This makes it possible to finally access all this data easily.''
\end{quote}

When shown the predictions from machine learning, one of the
researchers became quite excited and said:

\begin{quote}
`` Wow!  It can tell the difference between the different calls!''
\end{quote}

One respondent had concerns that she would not know if the people who
were making the annotations would really know what they were doing.
This same sentiment was shared by most of the other respondents.
Another discussion ensued about inter-observer reliability, the
quality of annotations, and knowledge about who had made which
annotation, this highlighted the importance for reputation management
systems to be implemented in this application.

Another topic that the subjects talked about was the importance for
being able to temporarily hide data that would be used in an upcoming
paper, so that other researchers would not be able to scoop them in
the race to publication in scholarly journals.  They said that:

\begin{quote}
`` I want to share data with people in my lab, but not with people in
  other labs, at least until I publish.''
\end{quote}

Another comment that was made was:

\begin{quote}
`` I want to be able to write my field notes and have them attached to
  a recording, but also be able to see all of them at once.''
\end{quote}


One researcher who has deep knowledge of the NRKW community said:

\begin{quote}
`` The Orchive is a fantastic way for researchers, such as myself, to
  easily access such a unique long term database of orca vocals. The
  uses are unlimited. The 'tour' section with the YouTube videos are
  especailly helpful and I found doing the annotations was very easy
  to do. The only thing that would make it better is if there was a
  way to search the labbook notes.''
\end{quote}

Another researcher who was able to use the site for her own research
said:

\begin{quote}
`` The Orchive is a great tool for whale researchers to utilize and
  increase information about the marine environment.  Analysis of
  killer whale vocalizations was conducted about two thirds of the
  time through the Orchive some calls were further investigated using
  other acoustic software. The acoustic call catalogue was very
  helpful in the identification of call types.  In progress changes to
  the Orchive web site did move some initial call annotations from
  their original positions.``
\end{quote}

Our collaborator from the Alberta Biodiversity Monitoring Institute
(ABMI), who has an M.Sc. in hummingbird sounds, said:

\begin{quote}
`` The two features that immediately caught my attention with Orchive
  were the ability to annotate on the sonogram and the linking of the
  sound player to digitized copies of field log notebooks.  In
  conjunction with the on-screen call libraries, this combination
  package immediately sparked my interest in having this program be
  further developed for use in the bird taxonomic work I perform as
  part of a province wide biodiversity monitoring program in
  Alberta.''
\end{quote}

Another researcher who has just joined the Orchive and is working with
it for her Ph.D. said:

\begin{quote}
	``To me what makes orchive special is that it offers hands-on
  experience for the interested parties and gives access to such an
  extensive digital archive, which is not only exciting but also
  extremely useful since it stimulates collaboration between research
  groups. ``
\end{quote}		

A researcher very experienced in NRKW vocalizations said:

\begin{quote}
``After some initial technical issues like sounds not playing or
taking long to load, the Orchive web interface worked beautifully. The
call matching game is a great idea for getting the public involved and
offers levels requiring different skills, so that it is fun to play
for beginners and experienced users alike.''
\end{quote}

A university student who also volunteered at OrcaLab, and provided
many of the annotations to the Orchive said:

\begin{quote}
`` I found the Orchive web interface to be intuitively laid out and
  easily understood. I felt that the ability to zoom in on
  spectrograms and easily scroll through large amounts of acoustic
  data without losing detail from compression in time was a great
  success, and something I wish to see implemented in other websites
  presenting spectrographic and acoustic data.  The option to have the
  spectrogram follow the audio or to pause the spectrogram while the
  audio continues (and be able to jump forward on the spectrogram to
  the time the audio has reached at the click of a button) was very
  useful during annotation sessions. Being able to create annotations
  while the audio continued became very useful when I gained
  proficiency with call identification since I could then annotate
  calls in real time while the audio track continued to play without
  having to stop and start the track. Having examples of each call
  listed under the spectrogram of the audio file I was working on,
  with the ability to click to enlarge the spectrogram and listen to
  the call, was invaluable to the process of learning the calls and
  annotating calls that I wasn't immediately familiar with or could
  not recall the alphanumeric name of.  Being able to refine my search
  in the call catalog by pod was also very helpful. Improvements to
  the interface could include the ability to search for recordings by
  annotation (e.g. to be able to look for recordings of specific
  pods), and digitized versions of the notebook pages that can be
  searched for information such as pods present, call types noted, or
  changes to hydrophone settings so that those who are learning the
  pods and call types can compare their guesses to notations made by
  the expert researchers at orcalab.''
\end{quote}

\section{Discussion}

However, as relative to the total amount of audio in the Orchive, the
amount that has been annotated is very small, about 0.1\% combined
with the fact that the project failed to attract and engage more
experts in orca vocalizations meant that orchive v1.0 was in many ways
unsuccessful.  One major challenge in this project was that in large
part it was very difficult to engage expert biologists in using this
interface.  Considerable work went into attracting more biologists,
and almost 50 scientists did sign up to use the Orchive, but very few
contributed annotations.  Upon further reflection, this was clearly
explained by Grudin \cite{grudin1988cscw} where he asks researchers to
ask themselves the question of who does the work and who gets the
benefits.  In this case, the person doing the work was the experts in
NRKW vocalizations, and the people who got
the benefit was the bioacoustic algorithms community, along with the
machine learning and Data Mining communities, and potentially other
future unknown researchers in orca communications.

This was a completely different state of affairs to what the author
had thought at the start of this project where it was thought that
biologists were of desperate need of this data.  In reality,
biologists almost always collect their own data, and often go to
considerable personal effort and risk to collect this data.  The audio
that they record is collected in a highly targeted fashion and in
almost all cases, they have a surplus of data.  The data from fixed
hydrophone arrays is often of lower quality than their targeted field
recordings, and often does not contain extensive metadata like they
would collect out in the field.  In addition, these biologists have
their own research ongoing which only coincidentally overlaps with
research into questions involving large bioacoustic archives.

However, these factors are precisely what attract developers of
bioacoustic algorithms, large amounts of noisy data that requires
advanced machine learning techniques to process.  Because of this, for
the orchive v2.0 interface, it was built with the main user being a
developer of bioacoustic algorithms using audio feature extraction and
supervised machine learning to classify large bioacoustic archives.
It assumes that the user of the system has good knowledge of computer
networking and web software, and that they are able to use simple
distributed filesystems, for example Dropbox and NFS.  This system has
been designed to be lighter weight and more flexible than Orchive
Version 1.0, at the expense of requiring more knowledge of computers.
However, developers of bioacoustic algorithms usually have these
skills almost by definition, as in order to develop and run these
algorithms, they can read audio files, calculate their features and
save them to a shared disk folder, which is all that is required to
interact with the more advanced parts of the Orchive website. 

With such a user, the orchive v2.0 system will be fully functional.
However, it has been designed more in mind for a bioacoustic
researcher who is also a developer of web based tools for large
bioacoustic or other audio archive.  Because of the diversity of
bioacoustic archives available, the size of this community of
developers might not be exceedingly small.  While the Django server
works well when used in the Orchive, many other sites do not run
Django.  For that reason, all the important functionality for each of
the tools, such as the Recording Viewer, the Data Viewer and the Game
Builder, is in the Javascript code, which can communicate with a wide
variety of servers, and uses a very simple, straightforward and well
documented REST API.  I have had interactions with the well known
xeno-canto \footnote{\url{http://www.xeno-canto.org/}} site who
expressed interest in standalone Javascript components for listening
to bird songs and interacting with spectrograms of them.

However, from purely personal experience, the author found that there
were no other tools that could enable him to work with the data in the
Orchive and do the experiments that were needed.  The tools that
worked well on small datasets had issues when working with larger
datasets.  The time to simply load a spectrogram in many of these
programs was prohibitive, and keeping track of hundreds of clips and
dozens of recordings made progress difficult.

In the system that has been constructed, new interfaces are relatively
simple to implement, and a custom interface to allow for the trimming
of the raw orchive v1.0 clips was built in a short amount of time.
This interface also allowed for the easy creation of training datasets
that were exported in the \textit{Marsyas} collection file format.  These
datasets could then be turned into crossvalidation folds using custom
Python scripts.  In the old system, all work was done using the web
interface, in order to make it approachable by biologists.  In the new
system, the assumption that the user is experienced in using computers
allows them to use their own tools on these datasets.  The orchive v2.0
then allows the researcher to view the output of their audio feature
extraction tools alongside the raw audio data in the web interface.
When these audio features have been optimized, the user can then run
machine learning algorithms on this data in order to generate
classifications.

This orchive v2.0 interface was used extensively in the creation of
datasets used in this chapter.  It was also used to view and select
appropriate parameters for the audio feature extraction algorithms,
and was then used to view the results of the machine learning
classification jobs on the recordings.  The full OBV segmentation job
described above was run on a large cluster and the results were then
able to be viewed by this web interface.

While it is difficult to evaluate the two versions of the Orchive
website, some general conclusions can be reached.  The first was that
orchive v1.0 was in some ways successful, in that it enabled the
collection of \totalAnnotations annotations by experts in orca
vocalizations.  In other ways it was unsuccessful in that it failed to
draw a large population of users and/or developers.


\section{Summary}
\label{sec:evaluation:summary}

In this chapter, I have evaluated the performance of the Orchive
system to annotate and classify a large bioacoustic archive.  There
are many parts to this system, and each one of them must function if
the entire system is to function.

The first function that the system must have is that is must allow
expert users to annotate calls in recordings.  It was shown that
version 1.0 of the Orchive accomplished this by obtaining
\totalAnnotations from \totalExperts over the course of 5 years.  With
this many high quality annotations, machine learning efforts can
proceed, which speaks to the success of this part of the system.

What is very encouraging is that the rate of participation has
recently increased, and hopefully by tailoring the development of the
new site to these new users, and to really listen to their concerns,
will help draw a larger community of experts to Orchive Version 2.0.

I also showed that after I have these annotations, I can use them
in an audio feature extraction and machine learning system to first
segment all the audio in the Orchive with the fairly high accuracy of
\classificationAccuracyOBV, and then to classify it with an accuracy
of \classificationAccuracyCALLS.  This is fairly high accuracy
already, and might be acceptable given certain scientific questions.
This means that this part of the system has been successful in meeting
the goals set out at the beginning of this thesis.

However, to get even higher accuracy, these segmented and classified
clips can be given as input to the citizen science part of the system,
and multiple users can guess on what they think the identity of the
call is.  Depending on the user, these classifications could be of
very high quality, for example if the user was an expert in orca
bioacoustics, I could place high faith in the game answers.  This
extra information could be used in the assignment of certainty to
classifications for different call types.  This citizen science part of the
system would help to increase the accuracy of the identification of
the call types in the Orchive.  This is the final part of the system, and
is also successful in that users were able to get very high accuracy
for certain classification tasks, and with training would become even
better at this task.

